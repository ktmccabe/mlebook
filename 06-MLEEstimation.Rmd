```{r, include=F}
knitr::knit_exit()
```


# MLE Estimation {#mleest}

This section will discuss the general process for deriving maximum likelihood estimators. It's all very exciting. It builds on the resources from the previous sections. In the next section, we will go through this process for a binary dependent variable. Here, we lay out the overview.

## Deriving Estimators 

Recall, we've already gone through a few steps of maximum likelihood estimation.

Initial Setup

  1. What is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered?  Based on this, describe the probability distribution for $Y_i$.
  2. Define the likelihood for a single observation
  3. Define the likelihood for all observations
  4. Find the log-likelihood
  
Now we add steps building on the log-likelihood.

  5. Maximize the function with respect to (wrt) $\theta$
      + Take the derivative wrt $\theta$. We call this the "score"
      + Set $S(\theta) = 0$ and solve for $\hat \theta$ (if possible)
      + If not possible, we use an optimization algorithm to maximize the log likelihood.
  6. Take the second derivative of the log likelihood to get the "hessian" and help estimate the uncertainty of the estimates.
  

### Score function

The first derivative of the log-likelihood is called the score function: $\frac{\delta \ell}{\delta \theta} = S(\theta)$. We set the $S(\theta) = 0$ and solve for $\hat \theta$ (if possible).

  - $\hat \theta$ are the slopes/gradient, which we use as estimates (e.g., $\hat \beta$).
  - We can interpret the sign and significance
  - But, unlike OLS, most of the time, these are not linear changes in units of $Y$
  - We have to transform them into interpretable quantities


**Example: Normally distributed outcome**


Start with the log-likelihood

\begin{align*}
\ell(\theta | Y) &= \sum_{i = 1}^N \log \Bigg( \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(Y_i-\mu)^2}{2\sigma^2}}\Bigg)
= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-\mu)^2}{2\sigma^2}\\
&= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-x_i'\beta)^2}{2\sigma^2}
\end{align*}

Take the derivative wrt $\theta$. Note: we have to take two derivatives- one for $\mu$ ($\beta$) and one for $\sigma^2$. For this example we will focus only on the derivative wrt to $\beta$.^[Essentially, you need to take derivatives with respect to each of the parameters. Some models we use will have only one parameter, which is easier.]

\begin{align*}
\delta \ell(\theta | Y) &= -\frac{1}{2\sigma^2}\sum_{i = 1}^N \delta (Y_i-x_i'\hat \beta)^2
\end{align*}
The right term should look familar! We will end up with a $S(\hat \theta)_\beta = \frac{1}{\sigma^2}X'(Y - X\hat \beta)$. We set this equal to 0.
\begin{align*}
\frac{1}{\sigma^2}X'(Y - X\hat \beta) &= 0\\
\frac{1}{\sigma^2}X'Y &= \frac{1}{\sigma^2}X'X\hat \beta \\
(X'X)^{-1}X'Y = \hat \beta
\end{align*}


### Hessian and Information Matrix

The second derivative of the log-likelihood is the Hessian $(H(\theta))$.

  - The second derivative is a measure of the curvature of the likelihood function
  - The more curved, the more certainty we have
  - The $I$ stands for the information matrix. The $H$ stands for Hessian. $I(\theta) = - \mathbb{E}(H)$
      + $var(\theta) = [I(\theta)]^{-1} = ( - \mathbb{E}(H))^{-1}$
      + Standard errors are the square roots of the diagonals of this $k \times k$ matrix (like  `vcov()` in OLS)

***Example: Normal***

Start with the log-likelihood

\begin{align*}
\ell(\theta | Y) &= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-x_i'\beta)^2}{2\sigma^2}
\end{align*}

Because our $\theta$ has two parameters, the Hessian actually has four components. For this example, we will focus on one: the first and second derivatives wrt $\beta$. 

  - Recall the first derivative = $\frac{1}{\sigma^2}X'(Y - X\hat \beta)$. 
  - We now take the second derivative 
    
    \begin{align*}
    \frac{\delta^2}{\delta \hat \beta} \frac{1}{\sigma^2}X'(Y - X\hat \beta)&= -\frac{1}{\sigma^2}X'X
    \end{align*}
  - To get our variance, we take the negative (-) inverse of this: 
      + $\sigma^2(X'X)^{-1}$ Should look familiar!

With this example, we can start to see why `lm` and `glm` for a normally distributed outcome generate the same estimates.

### MLE Properties

Just like OLS had certain properties (BLUE) that made it worthwhile, MLE also has desirable features.

Large sample properties

  - Consistent: plim$\hat \theta^{ML} = \theta$
  - Under certain "regularity conditions", Asymptotically normal: $\hat \theta^{ML} \sim N(\theta, [I(\theta)]^{-1})$
      + We will use the normal approximation to calculate z-scores and p-values
  - Efficient


***Note on consistency***

What does it mean to say an estimator is consistent? As samples get larger and larger, we converge to the truth.

Consistency: $p\lim \hat{\theta} =\beta$ As $n \rightarrow \infty P(\hat{\theta} - \theta> e) \rightarrow 0$.

  - Convergence in probability: the probability that the absolute difference between the estimate and parameter being larger than $e$ goes to zero as $n$ gets bigger. 

Note that bias and consistency are different: Consistency means that as the sample size ($n$) gets large the estimate gets closer to the true value. Unbiasedness is not affected by sample size. An estimate is unbiased if over repeated samples, its expected value (average) is the true parameter.

It is possible for an estimator to be unbiased and consistent, biased and not consistent, or consistent yet biased.

![](images/UnbiasedandConsistent.png){width=40%}

![](images/BiasedandInconsistent.png){width=40%}

![](images/BiasedbutConsistent.png){width=40%}

Images taken from [here](https://eranraviv.com/bias-vs-consistency/)

### Hypothesis Tests

We can apply the same hypothesis testing framework to our estimates here. We can standardize our coefficient estimates by dividing them by the standard error. This will generate a "z score." Just like when we had the t value in OLS, we can use the z score to calculate the p-value and make assessments about the null hypothesis that a given $\hat \beta_k$ = 0.

\begin{align*}
z &= \frac{\hat \theta_k}{\sqrt{Var(\hat \theta)_k}} \sim N(0,1)
\end{align*}

Note: to get p-values, we typically now use, `2 * pnorm(abs(z), lower.tail=F)` instead of `pt()` and our critical values are based on `qnorm()` instead of `qt()`. R will follow the same in most circumstances.


### Model Output in R

As discussed, we can fit a GLM in R using the `glm` function: 

  - `glm(formula, data, family = XXX(link = "XXX", ...), ...)`
      + `formula`: The model written in the form similar to `lm()`
      + `data`: Data frame
      + `family`: Name of PDF for $Y_i$ (e.g. `binomial`, `gaussian`)
      + `link`: Name of the link function (e.g. `logit`, ``probit`, `identity`, `log`)


```{r}
## Load Data
florida <- read.csv("https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv")
fit.glm <- glm(Buchanan00 ~ Perot96, data=florida, 
               family=gaussian(link = "identity"))
```


We've already discussed the coefficient output. Like `lm()`, GLM wiil also display the standard errors, z-scores / t-statistics, and p-values of the model in the model summary.

```{r}
summary(fit.glm)$coefficients
```

For this example, R reverts to the t-value instead of the z-score given that we are using the linear model. In other examples, you may see `z` in place of `t`. There are only small differences in these approximations because as your sample size gets larger, the degrees of freedom (used in the calculation of p-calues for the t distribution) are big enough that the t distribution converges to the normal distribution.


**Goodness of fit**

The `glm()` model has a lot of summary output.

```{r}
summary(fit.glm)
```

Some of the output represents measures of the goodness of fit of the model. However, their values are not directly interpretable from a single model.

  - Larger (less negative) likelihood, the better the model fits the data. (`logLik(mod)`)
  - Deviance is calculated from the likelihood. A measure of discrepancy between observed and fitted values. (Smaller values, better fit)
\begin{itemize}
  - Null deviance: how well the outcome is predicted by a model that includes only the intercept. ($df = n - 1$)
  - Residual deviance: how well the outcome is predicted by a model with our parameters. ($df = n-k$)
  - AIC- used for model comparison. Smaller values indicate a more parsimonious model. Accounts for the number of parameters ($K$) in the model (like Adjusted R-squared, but without the ease of interpretation). Sometimes used as a criteria in prediction exercises (using a model on training data to predict test data).


**Likelihood Ratio Test**

The likelihood ratio test compares the fit of two models, with the null hypothesis being that the full model does not add more explanatory power to the reduced model.

```{r}
fit.glm2 <- glm(Buchanan00 ~ Perot96 + Clinton96, data=florida, 
               family=gaussian(link = "identity"))

fit.glm1 <- glm(Buchanan00 ~ Perot96, data=florida, 
               family=gaussian(link = "identity"))
               
anova(fit.glm1, fit.glm2, test = "Chisq") # Fail to reject the null
```


**Pseudo-R-squared**

We don't have an exact equivalent to the R-squared in OLS, but people have developed "pseudo" measures.

Example: McFadden's R-squared

  - $PR^2 = 1 - \frac{\ell(M)}{\ell(N)}$
      + where $\ell(M)$ is the log-likelihood for your fitted model and $\ell(N)$ is the log-likelihood for a model with only the intercept
  - Recall, greater (less negative) values of the log-likelihood indicate better fit
  - McFadden's values range from 0 to close to 1
\end{itemize}

```{r}
# install.packages("pscl")
library(pscl)
fit.glm.null <- glm(Buchanan00 ~ 1, florida, family = gaussian(link = "identity"))
pr <- pR2(fit.glm1)
pr["McFadden"]

## Or, by hand:
1 - (logLik(fit.glm)/logLik(fit.glm.null))
```

