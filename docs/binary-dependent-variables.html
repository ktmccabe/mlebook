<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.2 Binary Dependent Variables | MLE for Political Science</title>
  <meta name="description" content="6.2 Binary Dependent Variables | MLE for Political Science" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="6.2 Binary Dependent Variables | MLE for Political Science" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.2 Binary Dependent Variables | MLE for Political Science" />
  
  
  

<meta name="author" content="Instructor: Katie McCabe" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deriving-estimators.html"/>

<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/vembedr-0.1.4/css/vembedr.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course Overview</a></li>
<li class="chapter" data-level="2" data-path="rover.html"><a href="rover.html"><i class="fa fa-check"></i><b>2</b> R Overview</a>
<ul>
<li class="chapter" data-level="2.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html"><i class="fa fa-check"></i><b>2.1</b> First Time with R and RStudio</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#open-rstudio"><i class="fa fa-check"></i><b>2.1.1</b> <strong>Open RStudio</strong></a></li>
<li class="chapter" data-level="2.1.2" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#using-r-as-a-calculator"><i class="fa fa-check"></i><b>2.1.2</b> <strong>Using R as a Calculator</strong></a></li>
<li class="chapter" data-level="2.1.3" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#working-in-an-r-script"><i class="fa fa-check"></i><b>2.1.3</b> <strong>Working in an R Script</strong></a></li>
<li class="chapter" data-level="2.1.4" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#preparing-your-r-script"><i class="fa fa-check"></i><b>2.1.4</b> <strong>Preparing your R script</strong></a></li>
<li class="chapter" data-level="2.1.5" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#executing-commands-in-your-r-script"><i class="fa fa-check"></i><b>2.1.5</b> <strong>Executing Commands in your R script</strong></a></li>
<li class="chapter" data-level="2.1.6" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#objects"><i class="fa fa-check"></i><b>2.1.6</b> <strong>Objects</strong></a></li>
<li class="chapter" data-level="2.1.7" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#practice"><i class="fa fa-check"></i><b>2.1.7</b> <strong>Practice</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="tutorials.html"><a href="tutorials.html"><i class="fa fa-check"></i><b>2.2</b> Tutorials</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>3</b> The MATH</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html"><i class="fa fa-check"></i><b>3.1</b> Mathematical Operations</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html#order-of-operations"><i class="fa fa-check"></i><b>3.1.1</b> <strong>Order of Operations</strong></a></li>
<li class="chapter" data-level="3.1.2" data-path="mathematical-operations.html"><a href="mathematical-operations.html#exponents"><i class="fa fa-check"></i><b>3.1.2</b> <strong>Exponents</strong></a></li>
<li class="chapter" data-level="3.1.3" data-path="mathematical-operations.html"><a href="mathematical-operations.html#summations-and-products"><i class="fa fa-check"></i><b>3.1.3</b> <strong>Summations and Products</strong></a></li>
<li class="chapter" data-level="3.1.4" data-path="mathematical-operations.html"><a href="mathematical-operations.html#logarithms"><i class="fa fa-check"></i><b>3.1.4</b> <strong>Logarithms</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html"><i class="fa fa-check"></i><b>3.2</b> Mathematical Operations in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#pemdas"><i class="fa fa-check"></i><b>3.2.1</b> PEMDAS</a></li>
<li class="chapter" data-level="3.2.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#exponents-1"><i class="fa fa-check"></i><b>3.2.2</b> Exponents</a></li>
<li class="chapter" data-level="3.2.3" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#summations"><i class="fa fa-check"></i><b>3.2.3</b> Summations</a></li>
<li class="chapter" data-level="3.2.4" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#logarithms-1"><i class="fa fa-check"></i><b>3.2.4</b> Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="derivatives.html"><a href="derivatives.html"><i class="fa fa-check"></i><b>3.3</b> Derivatives</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="derivatives.html"><a href="derivatives.html#derivatives-1"><i class="fa fa-check"></i><b>3.3.1</b> <strong>Derivatives</strong></a></li>
<li class="chapter" data-level="3.3.2" data-path="derivatives.html"><a href="derivatives.html#critical-points-for-minima-or-maxima"><i class="fa fa-check"></i><b>3.3.2</b> <strong>Critical Points for Minima or Maxima</strong></a></li>
<li class="chapter" data-level="3.3.3" data-path="derivatives.html"><a href="derivatives.html#common-derivative-rules"><i class="fa fa-check"></i><b>3.3.3</b> <strong>Common Derivative Rules</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html"><i class="fa fa-check"></i><b>3.4</b> Vectors and Matrices</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>3.4.1</b> <strong>Matrix Basics</strong></a></li>
<li class="chapter" data-level="3.4.2" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-operations"><i class="fa fa-check"></i><b>3.4.2</b> Matrix Operations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html"><i class="fa fa-check"></i><b>3.5</b> Additional Matrix Tidbits that Will Come Up</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#transpose"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Transpose</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#additional-matrix-properties-and-rules"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Additional Matrix Properties and Rules</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#matrix-rules"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Matrix Rules</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#derivatives-with-matrices-and-vectors"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Derivatives with Matrices and Vectors</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="practice-problems.html"><a href="practice-problems.html"><i class="fa fa-check"></i><b>3.6</b> Practice Problems</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="practice-problems.html"><a href="practice-problems.html#practice-problem-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Practice Problem Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>4</b> Review of OLS</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducing-ols-regression.html"><a href="introducing-ols-regression.html"><i class="fa fa-check"></i><b>4.1</b> Introducing OLS Regression</a></li>
<li class="chapter" data-level="4.2" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html"><i class="fa fa-check"></i><b>4.2</b> Diving Deeper into OLS Matrix Representation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Estimating the Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html"><i class="fa fa-check"></i><b>4.3</b> OLS Regression in R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#example-predicting-current-election-votes-from-past-election-votes"><i class="fa fa-check"></i><b>4.3.1</b> Example: Predicting Current Election Votes from Past Election Votes</a></li>
<li class="chapter" data-level="4.3.2" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#plotting-regression-results"><i class="fa fa-check"></i><b>4.3.2</b> Plotting Regression Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#finding-coefficients-without-lm"><i class="fa fa-check"></i><b>4.3.3</b> Finding Coefficients without <code>lm</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="practice-problems.html"><a href="practice-problems.html#practice-problems"><i class="fa fa-check"></i><b>4.3.4</b> Practice Problems</a></li>
<li class="chapter" data-level="4.3.5" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#code-for-solutions"><i class="fa fa-check"></i><b>4.3.5</b> Code for solutions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-1-thursday-tutorial.html"><a href="week-1-thursday-tutorial.html"><i class="fa fa-check"></i><b>4.4</b> Week 1 Thursday Tutorial</a></li>
<li class="chapter" data-level="4.5" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html"><i class="fa fa-check"></i><b>4.5</b> Uncertainty and Regression</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#variance-of-the-coefficients"><i class="fa fa-check"></i><b>4.5.1</b> Variance of the Coefficients</a></li>
<li class="chapter" data-level="4.5.2" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.5.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="4.5.3" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>4.5.3</b> Goodness of Fit</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="generating-predictions-from-regression-models.html"><a href="generating-predictions-from-regression-models.html"><i class="fa fa-check"></i><b>4.6</b> Generating predictions from regression models</a></li>
<li class="chapter" data-level="4.7" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html"><i class="fa fa-check"></i><b>4.7</b> Wrapping up OLS</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html#practice-problems-1"><i class="fa fa-check"></i><b>4.7.1</b> Practice Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mle.html"><a href="mle.html"><i class="fa fa-check"></i><b>5</b> Introduction to MLE</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html"><i class="fa fa-check"></i><b>5.1</b> What is likelihood?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html#summarizing-steps-for-maximum-likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Summarizing Steps for Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-model."><i class="fa fa-check"></i><b>5.2.1</b> GLM Model.</a></li>
<li class="chapter" data-level="5.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#linking-likelihood-and-the-glm"><i class="fa fa-check"></i><b>5.2.2</b> Linking likelihood and the GLM</a></li>
<li class="chapter" data-level="5.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-in-r"><i class="fa fa-check"></i><b>5.2.3</b> GLM in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="mleest.html"><a href="mleest.html"><i class="fa fa-check"></i><b>6</b> MLE Estimation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="deriving-estimators.html"><a href="deriving-estimators.html"><i class="fa fa-check"></i><b>6.1</b> Deriving Estimators</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="deriving-estimators.html"><a href="deriving-estimators.html#score-function"><i class="fa fa-check"></i><b>6.1.1</b> Score function</a></li>
<li class="chapter" data-level="6.1.2" data-path="deriving-estimators.html"><a href="deriving-estimators.html#hessian-and-information-matrix"><i class="fa fa-check"></i><b>6.1.2</b> Hessian and Information Matrix</a></li>
<li class="chapter" data-level="6.1.3" data-path="deriving-estimators.html"><a href="deriving-estimators.html#mle-estimation-algorithm"><i class="fa fa-check"></i><b>6.1.3</b> MLE Estimation Algorithm</a></li>
<li class="chapter" data-level="6.1.4" data-path="deriving-estimators.html"><a href="deriving-estimators.html#mle-properties"><i class="fa fa-check"></i><b>6.1.4</b> MLE Properties</a></li>
<li class="chapter" data-level="6.1.5" data-path="deriving-estimators.html"><a href="deriving-estimators.html#hypothesis-tests"><i class="fa fa-check"></i><b>6.1.5</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="6.1.6" data-path="deriving-estimators.html"><a href="deriving-estimators.html#model-output-in-r"><i class="fa fa-check"></i><b>6.1.6</b> Model Output in R</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html"><i class="fa fa-check"></i><b>6.2</b> Binary Dependent Variables</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html#r-code-for-fitting-logistic-regression"><i class="fa fa-check"></i><b>6.2.1</b> R code for fitting logistic regression</a></li>
<li class="chapter" data-level="6.2.2" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html#writing-down-the-regression-model"><i class="fa fa-check"></i><b>6.2.2</b> Writing down the regression model</a></li>
<li class="chapter" data-level="6.2.3" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html#probit-regression"><i class="fa fa-check"></i><b>6.2.3</b> Probit Regression</a></li>
<li class="chapter" data-level="6.2.4" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html#to-logit-or-to-probit"><i class="fa fa-check"></i><b>6.2.4</b> To logit or to probit?</a></li>
<li class="chapter" data-level="6.2.5" data-path="binary-dependent-variables.html"><a href="binary-dependent-variables.html#latent-variable-representation"><i class="fa fa-check"></i><b>6.2.5</b> Latent variable representation</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MLE for Political Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="binary-dependent-variables" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Binary Dependent Variables</h2>
<p>Let’s say <span class="math inline">\(Y_i\)</span> is a set of 0’s and 1’s for whether two states have experienced a dispute, an outcome common in IR studies.</p>
<p><span class="math display">\[\begin{gather*}
Y_i = \begin{cases}1, \;\text{a dispute happened}\\ 0,\;
\text{a dispute did not happen}\end{cases}
\end{gather*}\]</span></p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="binary-dependent-variables.html#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of first 20 observations (Y1, Y2, ..., Y20)</span></span>
<span id="cb282-2"><a href="binary-dependent-variables.html#cb282-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span> <span class="dv">0</span> <span class="dv">1</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">0</span> <span class="dv">1</span></span></code></pre></div>
<p>We need to align these data with a data generating process and distribution.</p>
<ul>
<li>For each <span class="math inline">\(Y_i\)</span>, it is like a single trial, where you have a dispute with some probability <span class="math inline">\((\pi)\)</span>
<ul>
<li>This sounds like the Bernoulli distribution! <span class="math inline">\(Y_i \sim Bernouli(\pi)\)</span></li>
</ul></li>
</ul>
<p>Let’s do the steps</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>What is the data generating process?</em></strong> Based on this, describe the probability distribution for <span class="math inline">\(Y_i\)</span>.</p>
<ul>
<li>You may have picked up on this, but if you are using a function like <code>glm()</code> you can proceed directly there after this step. However, let’s work under the hood for a bit.</li>
</ul>
<p><span class="math display">\[\begin{align*}
Y_i \sim f(Y_i | \pi) &amp;= Pr(Y_i = y_i |\pi_i) = \underbrace{\pi^{y_i}(1 -\pi)^{(1-y_i)}}_\text{pmf for Bernoulli}
\end{align*}\]</span>
for <span class="math inline">\(y = 0,1\)</span></p></li>
<li><p><strong><em>Define the likelihood for a single observation</em></strong></p></li>
<li><p><strong><em>Define the likelihood for all observations</em></strong></p></li>
<li><p><strong><em>Find the log-likelihood</em></strong></p></li>
</ol>
<p><span class="math display">\[\begin{align*}
\mathcal L( \pi | Y_i) &amp;= \underbrace{\pi^{y_i}(1 -\pi)^{(1-y_i)}}_\text{Likelihood for single observation}\\
\mathcal L( \pi | Y) &amp;= \underbrace{\prod_{i=1}^n\pi^{y_i}(1 -\pi)^{(1-y_i)}}_\text{Likelihood for all observations}\\
\ell( \pi | Y) &amp;= \underbrace{\sum_{i=1}^n\log \pi^{y_i}(1 -\pi)^{(1-y_i)}}_\text{Log likelihood}\\
\hat \pi &amp;= \text{Next step: arg max } \ell( \pi | Y) \text{ wrt $\pi$}
\end{align*}\]</span></p>
<p><strong><em>Add step: nonlinear transformation to <span class="math inline">\(X\beta\)</span></em></strong></p>
<ul>
<li><p>Note that because we are likely using covariates, we need to express our parameter as a function of <span class="math inline">\(X\beta\)</span>. Now that we are outside of linear territory, <span class="math inline">\(\pi = g(X_i, \beta) \neq x_i&#39;\beta\)</span></p></li>
<li><p>At this point we need a transformation, such as the logit or probit to map our linear predictor into the outcome. The logit is one variety. We apply a logit transformation, which restricts our estimates to between 0 and 1 (a good thing for probability!) where:</p></li>
<li><p><span class="math inline">\(\pi_i = \text{logit}^{-1}(\eta_i) = \frac{exp^{\eta_i}}{1 + exp^{\eta_i}} = \frac{exp^{x_i&#39;\beta}}{1 + exp^{x_i&#39;\beta}}\)</span></p></li>
<li><p><span class="math inline">\(\eta_i = \text{logit}(\pi_i) = \log\frac{\pi_i}{1-\pi_i} = x_i&#39;\beta\)</span></p></li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><strong><em>Maximize the function with respect to (wrt) <span class="math inline">\(\theta\)</span></em></strong></li>
</ol>
<p>Where <span class="math inline">\(\pi_i = \frac{exp^{x_i&#39;\beta}}{1 + exp^{x_i&#39;\beta}}\)</span>
<span class="math display">\[\begin{align*}
\hat \pi &amp;= \text{arg max } \ell( \pi | Y) \text{ wrt $\pi$} \\
&amp;= \text{arg max } \sum_{i=1}^n y_i \log \Big( \frac{exp^{x_i&#39;\beta}}{1 + exp^{x_i&#39;\beta}}\Big) + (1-y_i)\log \Big(1-\frac{exp^{x_i&#39;\beta}}{1 + exp^{x_i&#39;\beta}}\Big)\\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
S(\theta) &amp;=  \sum_{i=1}^n (Y_i - \pi_i)x^T_i\\
&amp;= X^T(Y - \pi)
\end{align*}\]</span></p>
<ol start="6" style="list-style-type: decimal">
<li>Take the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates.</li>
</ol>
<p><span class="math display">\[\begin{align*}
H(\theta) &amp;=  - \sum_{i=1}^n x_ix^T_i(\pi_i)(1 - \pi_i)\\
&amp;= -X^TVX
\end{align*}\]</span>
where <span class="math inline">\(V\)</span> is <span class="math inline">\(n \times n\)</span> diagonal matrix with weights that are the ith element of <span class="math inline">\((\pi)(1 - \pi)\)</span></p>
<div id="r-code-for-fitting-logistic-regression" class="section level3" number="6.2.1">
<h3><span class="header-section-number">6.2.1</span> R code for fitting logistic regression</h3>
<p>We can fit logistic regressions in R through <code>glm()</code>. Let’s build on the ANES example and analyze a dichotomized measure of participation where 1=participated in at least some form and 0=did not participate.</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="binary-dependent-variables.html#cb283-1" aria-hidden="true" tabindex="-1"></a>anes <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv&quot;</span>)</span>
<span id="cb283-2"><a href="binary-dependent-variables.html#cb283-2" aria-hidden="true" tabindex="-1"></a>anes<span class="sc">$</span>partbinary <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(anes<span class="sc">$</span>participation <span class="sc">&gt;</span> <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span></code></pre></div>
<p>We can then fit using <code>glm</code> where <code>family = binomial(link="logit")</code></p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="binary-dependent-variables.html#cb284-1" aria-hidden="true" tabindex="-1"></a>out.logit <span class="ot">&lt;-</span> <span class="fu">glm</span>(partbinary <span class="sc">~</span> female <span class="sc">+</span> edu <span class="sc">+</span> age <span class="sc">+</span> sexism, <span class="at">data=</span>anes,</span>
<span id="cb284-2"><a href="binary-dependent-variables.html#cb284-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;logit&quot;</span>))</span></code></pre></div>
<p>The summary output includes the logit coefficients, standard errors, z-scores, and p-values.</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="binary-dependent-variables.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(out.logit)</span></code></pre></div>
<pre><code>
Call:
glm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = &quot;logit&quot;), 
    data = anes)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5668   0.3328   0.4475   0.6287   1.2936  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.016734   0.334656   3.038  0.00238 ** 
female      -0.382087   0.151516  -2.522  0.01168 *  
edu          0.321190   0.050945   6.305 2.89e-10 ***
age          0.008682   0.004046   2.146  0.03188 *  
sexism      -1.593694   0.336373  -4.738 2.16e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1361.5  on 1584  degrees of freedom
Residual deviance: 1252.9  on 1580  degrees of freedom
  (355 observations deleted due to missingness)
AIC: 1262.9

Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="writing-down-the-regression-model" class="section level3" number="6.2.2">
<h3><span class="header-section-number">6.2.2</span> Writing down the regression model</h3>
<p>In the articles you write, you will describe the methods you use in detail, including the variables in the model and the type of regression (e.g., logistic regression). Sometimes you may want to go a step further and be very explicit about the model that you ran. We’ve already seen the regression equations for linear models. For the GLMs, they wiil look very similar, but we need to make the link/response function an explicit part of the equation.</p>
<p>For example, for logistic regression we have a few ways of writing it, including:</p>
<ul>
<li><span class="math inline">\(\log \frac{\pi_i}{1-\pi_i} = \mathbf{x_i&#39;}\beta\)</span>, or alternatively</li>
<li><span class="math inline">\(Pr(Y_i = 1 | \mathbf{x}_i) = logit^{-1}(\mathbf{x}_i&#39;\beta) = \frac{exp(\mathbf{x_i&#39;}\beta)}{(1 + exp(\mathbf{x_i&#39;}\beta)}\)</span></li>
</ul>
<p>(You can also write out the individual variable names.) There is a new R package <a href="https://github.com/datalorax/equatiomatic">equatiomatic</a> that can also be used to help write the equations from regression models. It’s not perfect, but should get you there for most basic models.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="binary-dependent-variables.html#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="do">## First time, you need to install one of these</span></span>
<span id="cb287-2"><a href="binary-dependent-variables.html#cb287-2" aria-hidden="true" tabindex="-1"></a><span class="co">#remotes::install_github(&quot;datalorax/equatiomatic&quot;)</span></span>
<span id="cb287-3"><a href="binary-dependent-variables.html#cb287-3" aria-hidden="true" tabindex="-1"></a><span class="co">#install.packages(&quot;equatiomatic&quot;)</span></span>
<span id="cb287-4"><a href="binary-dependent-variables.html#cb287-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-5"><a href="binary-dependent-variables.html#cb287-5" aria-hidden="true" tabindex="-1"></a><span class="do">## Each time after, run library</span></span>
<span id="cb287-6"><a href="binary-dependent-variables.html#cb287-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(equatiomatic)</span>
<span id="cb287-7"><a href="binary-dependent-variables.html#cb287-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb287-8"><a href="binary-dependent-variables.html#cb287-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Will output in latex code, though see package for details on options</span></span>
<span id="cb287-9"><a href="binary-dependent-variables.html#cb287-9" aria-hidden="true" tabindex="-1"></a><span class="fu">extract_eq</span>(out.logit, <span class="at">wrap =</span> <span class="cn">TRUE</span>, <span class="at">terms_per_line =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><span class="math display">\[
\begin{aligned}
\log\left[ \frac { P( \operatorname{partbinary} = \operatorname{1} ) }{ 1 - P( \operatorname{partbinary} = \operatorname{1} ) } \right] &amp;= \alpha + \beta_{1}(\operatorname{female}) + \beta_{2}(\operatorname{edu})\ + \\
&amp;\quad \beta_{3}(\operatorname{age}) + \beta_{4}(\operatorname{sexism})
\end{aligned}
\]</span></p>
</div>
<div id="probit-regression" class="section level3" number="6.2.3">
<h3><span class="header-section-number">6.2.3</span> Probit Regression</h3>
<p>Very similar to logit except we use a different link function to map the linear predictor into the outcome. Both the logit and probit links are suitable for binary outcomes with a Bernouilli distribution. If we apply a probit transformation, this restricts our estimates to between 0 and 1.</p>
<ul>
<li><span class="math inline">\(\pi_i = Pr(Y_i = 1| X_i) = \Phi(\pi_i)\)</span></li>
<li><span class="math inline">\(\eta_i = \Phi^{-1}(\pi_i) = x_i&#39;\beta\)</span></li>
</ul>
<p>Here, our coefficients <span class="math inline">\(\hat \beta\)</span> represent changes in “probits” or changes “z-score” units. We use the Normal CDF (<span class="math inline">\(\Phi()\)</span>) aka <code>pnorm()</code> in R to transform them back into probabilities.</p>
<p>Let’s fit our binary model with probit. We just need to change the link function.</p>
<p>We can then fit using <code>glm</code> where <code>family = binomial(link="logit")</code></p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="binary-dependent-variables.html#cb288-1" aria-hidden="true" tabindex="-1"></a>out.probit <span class="ot">&lt;-</span> <span class="fu">glm</span>(partbinary <span class="sc">~</span> female <span class="sc">+</span> edu <span class="sc">+</span> age <span class="sc">+</span> sexism, <span class="at">data=</span>anes,</span>
<span id="cb288-2"><a href="binary-dependent-variables.html#cb288-2" aria-hidden="true" tabindex="-1"></a>                 <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link=</span><span class="st">&quot;probit&quot;</span>))</span></code></pre></div>
<p>Let’s apply the equation tool to this:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="binary-dependent-variables.html#cb289-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Each time after, run library</span></span>
<span id="cb289-2"><a href="binary-dependent-variables.html#cb289-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(equatiomatic)</span>
<span id="cb289-3"><a href="binary-dependent-variables.html#cb289-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb289-4"><a href="binary-dependent-variables.html#cb289-4" aria-hidden="true" tabindex="-1"></a><span class="do">## Will output in latex code, though see package for details on options</span></span>
<span id="cb289-5"><a href="binary-dependent-variables.html#cb289-5" aria-hidden="true" tabindex="-1"></a><span class="fu">extract_eq</span>(out.probit, <span class="at">wrap =</span> <span class="cn">TRUE</span>, <span class="at">terms_per_line =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><span class="math display">\[
\begin{aligned}
P( \operatorname{partbinary} = \operatorname{1} ) &amp;= \Phi[\alpha + \beta_{1}(\operatorname{female}) + \beta_{2}(\operatorname{edu})\ + \\
&amp;\qquad\ \beta_{3}(\operatorname{age}) + \beta_{4}(\operatorname{sexism})]
\end{aligned}
\]</span></p>
<p>The summary output includes the probit coefficients, standard errors, z-scores, and p-values.</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="binary-dependent-variables.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(out.probit)</span></code></pre></div>
<pre><code>
Call:
glm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = &quot;probit&quot;), 
    data = anes)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6343   0.3188   0.4470   0.6361   1.2477  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.603661   0.184864   3.265  0.00109 ** 
female      -0.202300   0.083407  -2.425  0.01529 *  
edu          0.179264   0.027611   6.493 8.44e-11 ***
age          0.005145   0.002257   2.280  0.02261 *  
sexism      -0.898871   0.186443  -4.821 1.43e-06 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1361.5  on 1584  degrees of freedom
Residual deviance: 1250.6  on 1580  degrees of freedom
  (355 observations deleted due to missingness)
AIC: 1260.6

Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="to-logit-or-to-probit" class="section level3" number="6.2.4">
<h3><span class="header-section-number">6.2.4</span> To logit or to probit?</h3>
<p>Both produce a monotonically increasing S-curve in probability between 0 and 1, which vary according to the linear predictor (<span class="math inline">\(x_i^T\beta\)</span>)</p>
<p><img src="images/logitprobit.png" style="width:50.0%" /></p>
<ul>
<li>Both start with <span class="math inline">\(Y_i\)</span> as bernoulli</li>
<li>Both produce the same function of the log-likelihood BUT define <span class="math inline">\(\pi_i\)</span> and link function differently</li>
<li>Results–in terms of sign and significance of coefficients– are very similar
<ul>
<li>Logit coefficients are roughly 1.6*probit coefficients</li>
</ul></li>
<li>Results–in terms of predicted probabilities– are very similar
<ul>
<li>Exception– at extreme probabilities– Logit has “thicker tails”, gets to 0 and 1 more slowly</li>
</ul></li>
<li>Sometimes useful–Logit can also be transformed into “odds ratios”</li>
<li>By convention, logit more typically used in political science but easy enough to find examples of either</li>
</ul>
<p><strong><em>Note on Odds Ratios in Logistic Regression</em></strong></p>
<p>Coefficients are in “logits” or changes in “log-odds” (<span class="math inline">\(\log \frac{\pi_i}{1 - \pi}\)</span>). Some disciplines like to report “odds ratios”</p>
<ul>
<li>Odds ratio: <span class="math inline">\(\frac{\pi_i(x1)/(1 - \pi(x1))}{\pi_i(x0)/(1 - \pi(x0))}\)</span> (at a value of x1 vs. x0)
<ul>
<li>If <span class="math inline">\(\log \frac{\pi_i}{1 - \pi} = logodds\)</span>; <span class="math inline">\(\exp(logodds) = \frac{\pi_i}{1 - \pi}\)</span></li>
<li>Therefore, if we exponentiate our coefficients, this represents an odds ratio: the odds of <span class="math inline">\(Y_i = 1\)</span> increase by a factor of (<span class="math inline">\(\exp(\hat \beta_k)\)</span>) due to 1-unit change in X</li>
</ul></li>
</ul>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb292-1"><a href="binary-dependent-variables.html#cb292-1" aria-hidden="true" tabindex="-1"></a><span class="do">## odds ratio for the 4th coefficient</span></span>
<span id="cb292-2"><a href="binary-dependent-variables.html#cb292-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(out.logit)[<span class="dv">4</span>])</span></code></pre></div>
<pre><code>    age 
1.00872 </code></pre>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb294-1"><a href="binary-dependent-variables.html#cb294-1" aria-hidden="true" tabindex="-1"></a><span class="do">## CI for odds ratios</span></span>
<span id="cb294-2"><a href="binary-dependent-variables.html#cb294-2" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(out.logit)[<span class="dv">4</span>, ])</span></code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>   2.5 %   97.5 % 
1.000790 1.016801 </code></pre>
<p>In political science, we usually opt to present predicted probabilities instead of odds ratios, but ultimately you should do whatever you think is best.</p>
</div>
<div id="latent-variable-representation" class="section level3" number="6.2.5">
<h3><span class="header-section-number">6.2.5</span> Latent variable representation</h3>
<p>Sometimes you will see the binary outcome problem represented as a latent propensity where <span class="math inline">\(Y^*_i\)</span> is a continuous variable that represents an unobserved propensity (e.g., to have a dispute, to be a toxic tweet, to participate), where</p>
<p><span class="math display">\[\begin{gather*}
Y_i = \begin{cases}1, \; y^*_i &gt; \tau \\ 0,\; 
y^*_i \leq \tau \end{cases}
\end{gather*}\]</span></p>
<p>and <span class="math inline">\(\tau\)</span> is some threshold after which a the event (e.g., dispute) occurs.</p>
<p>People (who me? yes, I admit, me) will sometimes still use a linear model when we have dichotomous outcomes. In that case, we interpret the results as a “linear probability model” where a one-unit change in <span class="math inline">\(x\)</span> is associated with a <span class="math inline">\(\hat \beta\)</span> change in the probability that <span class="math inline">\(Y_i = 1\)</span>.</p>
<p><img src="images/lpmjoke.jpeg" />
<em>Image from Chelsea Parlett-Pelleriti <span class="citation">@ChelseaParlett</span> on Twitter</em></p>
<p>This may sound like a disaster because linear models are generally meant for nice continuous outcomes, and there is no way to prevent extreme values of <span class="math inline">\(X\beta\)</span> from extend above 1 or below 0. This is not to mention the heteroskedasticity issues that come from binary outcome because the error terms depend on the values of <span class="math inline">\(X\)</span>. This website has a good overview of the potential problems with linear <a href="https://www.dummies.com/education/economics/econometrics/3-main-linear-probability-model-lpm-problems/">regression</a>.</p>
<p>However, 1) we can use robust standard errors , 2) if you look at the S-curve in the previous section, you will note that a large part of the curve is pretty linear over a wide range of <span class="math inline">\(X\beta\)</span> values. For many applications, the estimates transformed from a logit or probit into probability will look similar to the estimates from a linear probability. 3) Linear probability models are easier to interpret, and there is no need to transform coefficients.</p>
<p>LPM vs. logit/probit has spurred a lot of debate throughout the years. Reviewers disagree, twitter users disagree, etc. This is just something to be aware of as you choose modeling approaches. Particularly when it comes to experiments and other causal inference approaches, there is a growing push to stick with linear probability models when your key independent variable is a discrete treatment indicator variable. See this new <a href="https://psyarxiv.com/4gmbv/">article</a> from Robin Gomilla and who lays out the considerations for using LPM, particularly in experimental settings, as well as follow up discussion from <a href="https://statmodeling.stat.columbia.edu/2020/01/10/linear-or-logistic-regression-with-binary-outcomes/">Andrew Gelman</a>. That said, even if you run with an LPM and cite the Gomilla article, a reviewer may still ask you to do a logit/probit. And there are certainly circumstances where LPM will fall short. Probably try both, and choose your own adventure.</p>

</div>
</div>
<!-- </div> -->










            </section>

          </div>
        </div>
      </div>
<a href="deriving-estimators.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
