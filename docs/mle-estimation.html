<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>5.3 MLE Estimation | MLE for Political Science</title>
  <meta name="description" content="5.3 MLE Estimation | MLE for Political Science" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="5.3 MLE Estimation | MLE for Political Science" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="5.3 MLE Estimation | MLE for Political Science" />
  
  
  

<meta name="author" content="Instructor: Katie McCabe" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="generalized-linear-models.html"/>
<link rel="next" href="mle-properties.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/vembedr-0.1.4/css/vembedr.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course Overview</a></li>
<li class="chapter" data-level="2" data-path="rover.html"><a href="rover.html"><i class="fa fa-check"></i><b>2</b> R Overview</a>
<ul>
<li class="chapter" data-level="2.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html"><i class="fa fa-check"></i><b>2.1</b> First Time with R and RStudio</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#open-rstudio"><i class="fa fa-check"></i><b>2.1.1</b> <strong>Open RStudio</strong></a></li>
<li class="chapter" data-level="2.1.2" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#using-r-as-a-calculator"><i class="fa fa-check"></i><b>2.1.2</b> <strong>Using R as a Calculator</strong></a></li>
<li class="chapter" data-level="2.1.3" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#working-in-an-r-script"><i class="fa fa-check"></i><b>2.1.3</b> <strong>Working in an R Script</strong></a></li>
<li class="chapter" data-level="2.1.4" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#preparing-your-r-script"><i class="fa fa-check"></i><b>2.1.4</b> <strong>Preparing your R script</strong></a></li>
<li class="chapter" data-level="2.1.5" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#executing-commands-in-your-r-script"><i class="fa fa-check"></i><b>2.1.5</b> <strong>Executing Commands in your R script</strong></a></li>
<li class="chapter" data-level="2.1.6" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#objects"><i class="fa fa-check"></i><b>2.1.6</b> <strong>Objects</strong></a></li>
<li class="chapter" data-level="2.1.7" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#practice"><i class="fa fa-check"></i><b>2.1.7</b> <strong>Practice</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="tutorials.html"><a href="tutorials.html"><i class="fa fa-check"></i><b>2.2</b> Tutorials</a></li>
<li class="chapter" data-level="2.3" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>2.3</b> Data Wrangling</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-uninformative-variable-names"><i class="fa fa-check"></i><b>2.3.1</b> Dealing with Uninformative Variable Names</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-missing-data"><i class="fa fa-check"></i><b>2.3.2</b> Dealing with Missing Data</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-variable-codings-that-arent-quite-right"><i class="fa fa-check"></i><b>2.3.3</b> Dealing with Variable Codings that Aren’t Quite Right</a></li>
<li class="chapter" data-level="2.3.4" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-incomplete-data-merging"><i class="fa fa-check"></i><b>2.3.4</b> Dealing with Incomplete Data (Merging!)</a></li>
<li class="chapter" data-level="2.3.5" data-path="data-wrangling.html"><a href="data-wrangling.html#dealing-with-poorly-shaped-data"><i class="fa fa-check"></i><b>2.3.5</b> Dealing with Poorly Shaped Data</a></li>
<li class="chapter" data-level="2.3.6" data-path="data-wrangling.html"><a href="data-wrangling.html#reproducing-your-steps"><i class="fa fa-check"></i><b>2.3.6</b> Reproducing your steps</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="tools-for-writing-up-results.html"><a href="tools-for-writing-up-results.html"><i class="fa fa-check"></i><b>2.4</b> Tools for writing up results</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="tools-for-writing-up-results.html"><a href="tools-for-writing-up-results.html#r-markdown"><i class="fa fa-check"></i><b>2.4.1</b> R Markdown</a></li>
<li class="chapter" data-level="2.4.2" data-path="tools-for-writing-up-results.html"><a href="tools-for-writing-up-results.html#latex"><i class="fa fa-check"></i><b>2.4.2</b> LaTex</a></li>
<li class="chapter" data-level="2.4.3" data-path="tools-for-writing-up-results.html"><a href="tools-for-writing-up-results.html#formatting-and-exporting-r-results"><i class="fa fa-check"></i><b>2.4.3</b> Formatting and Exporting R Results</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>3</b> The MATH</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html"><i class="fa fa-check"></i><b>3.1</b> Mathematical Operations</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html#order-of-operations"><i class="fa fa-check"></i><b>3.1.1</b> <strong>Order of Operations</strong></a></li>
<li class="chapter" data-level="3.1.2" data-path="mathematical-operations.html"><a href="mathematical-operations.html#exponents"><i class="fa fa-check"></i><b>3.1.2</b> <strong>Exponents</strong></a></li>
<li class="chapter" data-level="3.1.3" data-path="mathematical-operations.html"><a href="mathematical-operations.html#summations-and-products"><i class="fa fa-check"></i><b>3.1.3</b> <strong>Summations and Products</strong></a></li>
<li class="chapter" data-level="3.1.4" data-path="mathematical-operations.html"><a href="mathematical-operations.html#logarithms"><i class="fa fa-check"></i><b>3.1.4</b> <strong>Logarithms</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html"><i class="fa fa-check"></i><b>3.2</b> Mathematical Operations in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#pemdas"><i class="fa fa-check"></i><b>3.2.1</b> PEMDAS</a></li>
<li class="chapter" data-level="3.2.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#exponents-1"><i class="fa fa-check"></i><b>3.2.2</b> Exponents</a></li>
<li class="chapter" data-level="3.2.3" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#summations"><i class="fa fa-check"></i><b>3.2.3</b> Summations</a></li>
<li class="chapter" data-level="3.2.4" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#logarithms-1"><i class="fa fa-check"></i><b>3.2.4</b> Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="derivatives.html"><a href="derivatives.html"><i class="fa fa-check"></i><b>3.3</b> Derivatives</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="derivatives.html"><a href="derivatives.html#derivatives-1"><i class="fa fa-check"></i><b>3.3.1</b> <strong>Derivatives</strong></a></li>
<li class="chapter" data-level="3.3.2" data-path="derivatives.html"><a href="derivatives.html#critical-points-for-minima-or-maxima"><i class="fa fa-check"></i><b>3.3.2</b> <strong>Critical Points for Minima or Maxima</strong></a></li>
<li class="chapter" data-level="3.3.3" data-path="derivatives.html"><a href="derivatives.html#common-derivative-rules"><i class="fa fa-check"></i><b>3.3.3</b> <strong>Common Derivative Rules</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html"><i class="fa fa-check"></i><b>3.4</b> Vectors and Matrices</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>3.4.1</b> <strong>Matrix Basics</strong></a></li>
<li class="chapter" data-level="3.4.2" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-operations"><i class="fa fa-check"></i><b>3.4.2</b> Matrix Operations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html"><i class="fa fa-check"></i><b>3.5</b> Additional Matrix Tidbits that Will Come Up</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#transpose"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Transpose</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#additional-matrix-properties-and-rules"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Additional Matrix Properties and Rules</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#matrix-rules"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Matrix Rules</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#derivatives-with-matrices-and-vectors"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Derivatives with Matrices and Vectors</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="practice-problems.html"><a href="practice-problems.html"><i class="fa fa-check"></i><b>3.6</b> Practice Problems</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="practice-problems.html"><a href="practice-problems.html#practice-problem-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Practice Problem Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>4</b> Review of OLS</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducing-ols-regression.html"><a href="introducing-ols-regression.html"><i class="fa fa-check"></i><b>4.1</b> Introducing OLS Regression</a></li>
<li class="chapter" data-level="4.2" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html"><i class="fa fa-check"></i><b>4.2</b> Diving Deeper into OLS Matrix Representation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Estimating the Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html"><i class="fa fa-check"></i><b>4.3</b> OLS Regression in R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#example-predicting-current-election-votes-from-past-election-votes"><i class="fa fa-check"></i><b>4.3.1</b> Example: Predicting Current Election Votes from Past Election Votes</a></li>
<li class="chapter" data-level="4.3.2" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#plotting-regression-results"><i class="fa fa-check"></i><b>4.3.2</b> Plotting Regression Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#finding-coefficients-without-lm"><i class="fa fa-check"></i><b>4.3.3</b> Finding Coefficients without <code>lm</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#ols-practice-problems"><i class="fa fa-check"></i><b>4.3.4</b> OLS Practice Problems</a></li>
<li class="chapter" data-level="4.3.5" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#code-for-solutions"><i class="fa fa-check"></i><b>4.3.5</b> Code for solutions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-1-thursday-tutorial.html"><a href="week-1-thursday-tutorial.html"><i class="fa fa-check"></i><b>4.4</b> Week 1 Thursday Tutorial</a></li>
<li class="chapter" data-level="4.5" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html"><i class="fa fa-check"></i><b>4.5</b> Uncertainty and Regression</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#variance-of-the-coefficients"><i class="fa fa-check"></i><b>4.5.1</b> Variance of the Coefficients</a></li>
<li class="chapter" data-level="4.5.2" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.5.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="4.5.3" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>4.5.3</b> Goodness of Fit</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="generating-predictions-from-regression-models.html"><a href="generating-predictions-from-regression-models.html"><i class="fa fa-check"></i><b>4.6</b> Generating predictions from regression models</a></li>
<li class="chapter" data-level="4.7" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html"><i class="fa fa-check"></i><b>4.7</b> Wrapping up OLS</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="practice-problems.html"><a href="practice-problems.html#practice-problems"><i class="fa fa-check"></i><b>4.7.1</b> Practice Problems</a></li>
<li class="chapter" data-level="4.7.2" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html#practice-problem-code-for-solutions"><i class="fa fa-check"></i><b>4.7.2</b> Practice Problem Code for Solutions</a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="week-2-thursday-example.html"><a href="week-2-thursday-example.html"><i class="fa fa-check"></i><b>4.8</b> Week 2 Thursday Example</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mle.html"><a href="mle.html"><i class="fa fa-check"></i><b>5</b> Introduction to MLE</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html"><i class="fa fa-check"></i><b>5.1</b> What is likelihood?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html#summarizing-steps-for-maximum-likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Summarizing Steps for Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-model."><i class="fa fa-check"></i><b>5.2.1</b> GLM Model.</a></li>
<li class="chapter" data-level="5.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#linking-likelihood-and-the-glm"><i class="fa fa-check"></i><b>5.2.2</b> Linking likelihood and the GLM</a></li>
<li class="chapter" data-level="5.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-in-r"><i class="fa fa-check"></i><b>5.2.3</b> GLM in R</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="mle-estimation.html"><a href="mle-estimation.html"><i class="fa fa-check"></i><b>5.3</b> MLE Estimation</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="mle-estimation.html"><a href="mle-estimation.html#deriving-estimators"><i class="fa fa-check"></i><b>5.3.1</b> Deriving Estimators</a></li>
<li class="chapter" data-level="5.3.2" data-path="mle-estimation.html"><a href="mle-estimation.html#score-function"><i class="fa fa-check"></i><b>5.3.2</b> Score function</a></li>
<li class="chapter" data-level="5.3.3" data-path="mle-estimation.html"><a href="mle-estimation.html#hessian-and-information-matrix"><i class="fa fa-check"></i><b>5.3.3</b> Hessian and Information Matrix</a></li>
<li class="chapter" data-level="5.3.4" data-path="mle-estimation.html"><a href="mle-estimation.html#mle-estimation-algorithm"><i class="fa fa-check"></i><b>5.3.4</b> MLE Estimation Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="mle-properties.html"><a href="mle-properties.html"><i class="fa fa-check"></i><b>5.4</b> MLE Properties</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="mle-properties.html"><a href="mle-properties.html#hypothesis-tests"><i class="fa fa-check"></i><b>5.4.1</b> Hypothesis Tests</a></li>
<li class="chapter" data-level="5.4.2" data-path="mle-properties.html"><a href="mle-properties.html#model-output-in-r"><i class="fa fa-check"></i><b>5.4.2</b> Model Output in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="binary.html"><a href="binary.html"><i class="fa fa-check"></i><b>6</b> Binary Dependent Variables</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-generating-process.html"><a href="data-generating-process.html"><i class="fa fa-check"></i><b>6.1</b> Data Generating Process</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="mle-estimation.html"><a href="mle-estimation.html#mle-estimation"><i class="fa fa-check"></i><b>6.1.1</b> MLE Estimation</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="r-code-for-fitting-logistic-regression.html"><a href="r-code-for-fitting-logistic-regression.html"><i class="fa fa-check"></i><b>6.2</b> R code for fitting logistic regression</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="r-code-for-fitting-logistic-regression.html"><a href="r-code-for-fitting-logistic-regression.html#writing-down-the-regression-model"><i class="fa fa-check"></i><b>6.2.1</b> Writing down the regression model</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="probit-regression.html"><a href="probit-regression.html"><i class="fa fa-check"></i><b>6.3</b> Probit Regression</a></li>
<li class="chapter" data-level="6.4" data-path="to-logit-or-to-probit.html"><a href="to-logit-or-to-probit.html"><i class="fa fa-check"></i><b>6.4</b> To logit or to probit?</a></li>
<li class="chapter" data-level="6.5" data-path="latent-propensity-representation.html"><a href="latent-propensity-representation.html"><i class="fa fa-check"></i><b>6.5</b> Latent propensity representation</a></li>
<li class="chapter" data-level="6.6" data-path="linear-probability-models.html"><a href="linear-probability-models.html"><i class="fa fa-check"></i><b>6.6</b> Linear Probability Models</a></li>
<li class="chapter" data-level="6.7" data-path="week-3-tutorial.html"><a href="week-3-tutorial.html"><i class="fa fa-check"></i><b>6.7</b> Week 3 Tutorial</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="week-3-tutorial.html"><a href="week-3-tutorial.html#loading-data-and-fitting-glm"><i class="fa fa-check"></i><b>6.7.1</b> Loading data and fitting glm</a></li>
<li class="chapter" data-level="6.7.2" data-path="week-3-tutorial.html"><a href="week-3-tutorial.html#numeric-optimization"><i class="fa fa-check"></i><b>6.7.2</b> Numeric Optimization</a></li>
<li class="chapter" data-level="6.7.3" data-path="week-3-tutorial.html"><a href="week-3-tutorial.html#predicted-probabilities"><i class="fa fa-check"></i><b>6.7.3</b> Predicted Probabilities</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="qoi.html"><a href="qoi.html"><i class="fa fa-check"></i><b>7</b> Quantities of Interest</a>
<ul>
<li class="chapter" data-level="7.1" data-path="using-the-response-functions-to-generate-quantities-of-interest.html"><a href="using-the-response-functions-to-generate-quantities-of-interest.html"><i class="fa fa-check"></i><b>7.1</b> Using the response functions to generate quantities of interest</a></li>
<li class="chapter" data-level="7.2" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html"><i class="fa fa-check"></i><b>7.2</b> QOI at Designated Values</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#marginal-effects"><i class="fa fa-check"></i><b>7.2.1</b> Marginal Effects</a></li>
<li class="chapter" data-level="7.2.2" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#marginal-effects-at-the-mean"><i class="fa fa-check"></i><b>7.2.2</b> Marginal effects at the mean</a></li>
<li class="chapter" data-level="7.2.3" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#marginal-effects-at-representative-values"><i class="fa fa-check"></i><b>7.2.3</b> Marginal effects at representative values</a></li>
<li class="chapter" data-level="7.2.4" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#average-marginal-effects"><i class="fa fa-check"></i><b>7.2.4</b> Average marginal effects</a></li>
<li class="chapter" data-level="7.2.5" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#prediction-and-margins-packages."><i class="fa fa-check"></i><b>7.2.5</b> <code>prediction</code> and <code>margins</code> packages.</a></li>
<li class="chapter" data-level="7.2.6" data-path="qoi-at-designated-values.html"><a href="qoi-at-designated-values.html#qoi-practice-problems"><i class="fa fa-check"></i><b>7.2.6</b> QOI Practice Problems</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="uncertainty.html"><a href="uncertainty.html"><i class="fa fa-check"></i><b>7.3</b> Uncertainty</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="uncertainty.html"><a href="uncertainty.html#bootstrapping"><i class="fa fa-check"></i><b>7.3.1</b> Bootstrapping</a></li>
<li class="chapter" data-level="7.3.2" data-path="uncertainty.html"><a href="uncertainty.html#simulated-confidence-intervals"><i class="fa fa-check"></i><b>7.3.2</b> Simulated Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="visualizing-results.html"><a href="visualizing-results.html"><i class="fa fa-check"></i><b>7.4</b> Visualizing Results</a></li>
<li class="chapter" data-level="7.5" data-path="additional-r-shortcuts.html"><a href="additional-r-shortcuts.html"><i class="fa fa-check"></i><b>7.5</b> Additional R shortcuts</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="additional-r-shortcuts.html"><a href="additional-r-shortcuts.html#prediction"><i class="fa fa-check"></i><b>7.5.1</b> Prediction</a></li>
<li class="chapter" data-level="7.5.2" data-path="additional-r-shortcuts.html"><a href="additional-r-shortcuts.html#margins"><i class="fa fa-check"></i><b>7.5.2</b> Margins</a></li>
<li class="chapter" data-level="7.5.3" data-path="additional-r-shortcuts.html"><a href="additional-r-shortcuts.html#zelig"><i class="fa fa-check"></i><b>7.5.3</b> Zelig</a></li>
<li class="chapter" data-level="7.5.4" data-path="additional-r-shortcuts.html"><a href="additional-r-shortcuts.html#using-expand.grid"><i class="fa fa-check"></i><b>7.5.4</b> Using <code>expand.grid</code></a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="week-4-tutorial.html"><a href="week-4-tutorial.html"><i class="fa fa-check"></i><b>7.6</b> Week 4 Tutorial</a></li>
<li class="chapter" data-level="7.7" data-path="putting-everything-together.html"><a href="putting-everything-together.html"><i class="fa fa-check"></i><b>7.7</b> Putting everything together</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ordinal.html"><a href="ordinal.html"><i class="fa fa-check"></i><b>8</b> Ordinal Outcomes</a>
<ul>
<li class="chapter" data-level="8.1" data-path="ordinal-outcome-data.html"><a href="ordinal-outcome-data.html"><i class="fa fa-check"></i><b>8.1</b> Ordinal Outcome Data</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="ordinal-outcome-data.html"><a href="ordinal-outcome-data.html#ordinal-model"><i class="fa fa-check"></i><b>8.1.1</b> Ordinal Model</a></li>
<li class="chapter" data-level="8.1.2" data-path="ordinal-outcome-data.html"><a href="ordinal-outcome-data.html#interpretation"><i class="fa fa-check"></i><b>8.1.2</b> Interpretation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="likelihood-framework.html"><a href="likelihood-framework.html"><i class="fa fa-check"></i><b>8.2</b> Likelihood Framework</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="likelihood-framework.html"><a href="likelihood-framework.html#likelihood"><i class="fa fa-check"></i><b>8.2.1</b> Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="fitting-ordinal-models-in-r.html"><a href="fitting-ordinal-models-in-r.html"><i class="fa fa-check"></i><b>8.3</b> Fitting Ordinal Models in R</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="fitting-ordinal-models-in-r.html"><a href="fitting-ordinal-models-in-r.html#quantities-of-interest"><i class="fa fa-check"></i><b>8.3.1</b> Quantities of Interest</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="assumptions.html"><a href="assumptions.html"><i class="fa fa-check"></i><b>8.4</b> Assumptions</a></li>
<li class="chapter" data-level="8.5" data-path="ordinal-practice-problems.html"><a href="ordinal-practice-problems.html"><i class="fa fa-check"></i><b>8.5</b> Ordinal Practice Problems</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="ordinal-practice-problems.html"><a href="ordinal-practice-problems.html#a-note-on-robust-standard-errors"><i class="fa fa-check"></i><b>8.5.1</b> A note on robust standard errors</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="week-6-tutorial.html"><a href="week-6-tutorial.html"><i class="fa fa-check"></i><b>8.6</b> Week 6 Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="ml.html"><a href="ml.html"><i class="fa fa-check"></i><b>9</b> Multinomial Outcomes</a>
<ul>
<li class="chapter" data-level="9.1" data-path="overview-of-nominal-data.html"><a href="overview-of-nominal-data.html"><i class="fa fa-check"></i><b>9.1</b> Overview of Nominal Data</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="overview-of-nominal-data.html"><a href="overview-of-nominal-data.html#multinomial-likelihood"><i class="fa fa-check"></i><b>9.1.1</b> Multinomial Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="motivating-example.html"><a href="motivating-example.html"><i class="fa fa-check"></i><b>9.2</b> Motivating Example</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="motivating-example.html"><a href="motivating-example.html#assumption-and-considerations"><i class="fa fa-check"></i><b>9.2.1</b> Assumption and Considerations</a></li>
<li class="chapter" data-level="9.2.2" data-path="motivating-example.html"><a href="motivating-example.html#key-assumption-independence-of-irrelevant-alternatives"><i class="fa fa-check"></i><b>9.2.2</b> Key Assumption: Independence of Irrelevant Alternatives</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="running-multinomial-logit-in-r.html"><a href="running-multinomial-logit-in-r.html"><i class="fa fa-check"></i><b>9.3</b> Running multinomial logit in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="running-multinomial-logit-in-r.html"><a href="running-multinomial-logit-in-r.html#multinomial-quantities-of-interest"><i class="fa fa-check"></i><b>9.3.1</b> Multinomial Quantities of Interest</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="practice-problems-for-multinomial.html"><a href="practice-problems-for-multinomial.html"><i class="fa fa-check"></i><b>9.4</b> Practice Problems for Multinomial</a></li>
<li class="chapter" data-level="9.5" data-path="week-7-tutorial.html"><a href="week-7-tutorial.html"><i class="fa fa-check"></i><b>9.5</b> Week 7 Tutorial</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="count.html"><a href="count.html"><i class="fa fa-check"></i><b>10</b> Count data</a>
<ul>
<li class="chapter" data-level="10.1" data-path="overview-of-count-data.html"><a href="overview-of-count-data.html"><i class="fa fa-check"></i><b>10.1</b> Overview of Count Data</a></li>
<li class="chapter" data-level="10.2" data-path="poisson-model.html"><a href="poisson-model.html"><i class="fa fa-check"></i><b>10.2</b> Poisson Model</a></li>
<li class="chapter" data-level="10.3" data-path="motivating-example-for-count-data.html"><a href="motivating-example-for-count-data.html"><i class="fa fa-check"></i><b>10.3</b> Motivating Example for Count Data</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="motivating-example-for-count-data.html"><a href="motivating-example-for-count-data.html#fitting-poisson-in-r"><i class="fa fa-check"></i><b>10.3.1</b> Fitting Poisson in R</a></li>
<li class="chapter" data-level="10.3.2" data-path="motivating-example-for-count-data.html"><a href="motivating-example-for-count-data.html#interpreting-regression-output"><i class="fa fa-check"></i><b>10.3.2</b> Interpreting regression output</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="poisson-quantities-of-interest.html"><a href="poisson-quantities-of-interest.html"><i class="fa fa-check"></i><b>10.4</b> Poisson Quantities of Interest</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="poisson-quantities-of-interest.html"><a href="poisson-quantities-of-interest.html#expected-counts"><i class="fa fa-check"></i><b>10.4.1</b> Expected Counts</a></li>
<li class="chapter" data-level="10.4.2" data-path="poisson-quantities-of-interest.html"><a href="poisson-quantities-of-interest.html#sidenote-multiplicative-coefficient-interpretation"><i class="fa fa-check"></i><b>10.4.2</b> Sidenote: Multiplicative coefficient interpretation</a></li>
<li class="chapter" data-level="10.4.3" data-path="poisson-quantities-of-interest.html"><a href="poisson-quantities-of-interest.html#incidence-rate-ratios"><i class="fa fa-check"></i><b>10.4.3</b> Incidence Rate Ratios</a></li>
<li class="chapter" data-level="10.4.4" data-path="poisson-quantities-of-interest.html"><a href="poisson-quantities-of-interest.html#where-poisson-is-poisonous"><i class="fa fa-check"></i><b>10.4.4</b> Where Poisson is poisonous:</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="quasipoisson-and-negative-binomial-models.html"><a href="quasipoisson-and-negative-binomial-models.html"><i class="fa fa-check"></i><b>10.5</b> Quasipoisson and Negative Binomial Models</a>
<ul>
<li class="chapter" data-level="10.5.1" data-path="quasipoisson-and-negative-binomial-models.html"><a href="quasipoisson-and-negative-binomial-models.html#negative-binomial-models"><i class="fa fa-check"></i><b>10.5.1</b> Negative Binomial Models</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="count-data-practice-problems.html"><a href="count-data-practice-problems.html"><i class="fa fa-check"></i><b>10.6</b> Count data practice problems</a></li>
<li class="chapter" data-level="10.7" data-path="week-8-tutorial.html"><a href="week-8-tutorial.html"><i class="fa fa-check"></i><b>10.7</b> Week 8 Tutorial</a></li>
<li class="chapter" data-level="10.8" data-path="additional-considerations.html"><a href="additional-considerations.html"><i class="fa fa-check"></i><b>10.8</b> Additional Considerations</a>
<ul>
<li class="chapter" data-level="10.8.1" data-path="additional-considerations.html"><a href="additional-considerations.html#offset"><i class="fa fa-check"></i><b>10.8.1</b> Offset</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="how-to-think-about-zero-counts.html"><a href="how-to-think-about-zero-counts.html"><i class="fa fa-check"></i><b>10.9</b> How to think about Zero Counts</a>
<ul>
<li class="chapter" data-level="10.9.1" data-path="how-to-think-about-zero-counts.html"><a href="how-to-think-about-zero-counts.html#hurdle-models"><i class="fa fa-check"></i><b>10.9.1</b> Hurdle Models</a></li>
<li class="chapter" data-level="10.9.2" data-path="how-to-think-about-zero-counts.html"><a href="how-to-think-about-zero-counts.html#zero-inflated-poissonnegative-binomial"><i class="fa fa-check"></i><b>10.9.2</b> Zero Inflated Poisson/Negative binomial</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MLE for Political Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="mle-estimation" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> MLE Estimation</h2>
<p>This section will discuss the general process for deriving maximum likelihood estimators. It’s all very exciting. It builds on the resources from the previous sections. In the next section, we will go through this process for a binary dependent variable. Here, we lay out the overview.</p>
<div id="deriving-estimators" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Deriving Estimators</h3>
<p>Recall, we’ve already gone through a few steps of maximum likelihood estimation.</p>
<p>Initial Setup</p>
<ol style="list-style-type: decimal">
<li>What is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for <span class="math inline">\(Y_i\)</span>.</li>
<li>Define the likelihood for a single observation</li>
<li>Define the likelihood for all observations</li>
<li>Find the log-likelihood</li>
</ol>
<p>Now we add steps building on the log-likelihood.</p>
<ol start="5" style="list-style-type: decimal">
<li>Maximize the function with respect to (wrt) <span class="math inline">\(\theta\)</span>
<ul>
<li>Take the derivative wrt <span class="math inline">\(\theta\)</span>. We call this the “score”</li>
<li>Set <span class="math inline">\(S(\theta) = 0\)</span> and solve for <span class="math inline">\(\hat \theta\)</span> (if possible)</li>
<li>If not possible (often the case), we use an optimization algorithm to maximize the log likelihood.</li>
</ul></li>
<li>Take the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates.</li>
</ol>
</div>
<div id="score-function" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Score function</h3>
<p>The first derivative of the log-likelihood is called the score function: <span class="math inline">\(\frac{\delta \ell}{\delta \theta} = S(\theta)\)</span>. This will tell us how steep the slope of the log likelihood is given certain values of the parameters. What we are looking for as we sift through possible values of the parameters, is the set of values that will make the slope zero, signalling that the function has reached a peak (maximizing the likelihood.)</p>
<p>We set the <span class="math inline">\(S(\theta) = 0\)</span> and solve for <span class="math inline">\(\hat \theta\)</span> (if possible).</p>
<ul>
<li><span class="math inline">\(\hat \theta\)</span> are the slopes/gradient, which we use as estimates (e.g., <span class="math inline">\(\hat \beta\)</span>).</li>
<li>We can interpret the sign and significance just as we do in OLS.</li>
<li>But, unlike OLS, most of the time, these are not linear changes in units of <span class="math inline">\(Y\)</span></li>
<li>We have to transform them into interpretable quantities</li>
</ul>
<p><strong>Example: Normally distributed outcome</strong></p>
<p>Start with the log-likelihood</p>
<p><span class="math display">\[\begin{align*}
\ell(\theta | Y) &amp;= \sum_{i = 1}^N \log \Bigg( \frac{1}{\sigma\sqrt{2\pi}}e^{\frac{-(Y_i-\mu)^2}{2\sigma^2}}\Bigg)\\
&amp;= \sum_{i = 1}^N \underbrace{\log \Bigg( \frac{1}{\sigma\sqrt{2\pi}}\Bigg) + \log e^{\frac{-(Y_i-\mu)^2}{2\sigma^2}}}_\text{Using the rule $\log ab = \log a + \log b$}\\
&amp;= \underbrace{\sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-\mu)^2}{2\sigma^2}}_\text{The second term was of the form $\log e ^ a$, we can re-write as $a * \log e$. $\log e$ cancels to 1, leaving us with just $a$.}\\
&amp;= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-\mathbf{x}_i&#39;\beta)^2}{2\sigma^2}
\end{align*}\]</span></p>
<ul>
<li>Note: when you see <span class="math inline">\(\mathbf{x}_i&#39;\beta\)</span>, usually that is the representation of the multiplication of <span class="math inline">\(k\)</span> covariates (a <span class="math inline">\(1 \times k\)</span> vector) for a particular observation <span class="math inline">\(i\)</span> by <span class="math inline">\(k \times 1\)</span> coefficient values <span class="math inline">\(\beta\)</span>. You can contrast this with <span class="math inline">\(X\beta\)</span>, which represents <span class="math inline">\(n \times k\)</span> rows of observations with <span class="math inline">\(k\)</span> covariates multiplied by the <span class="math inline">\(k \times 1\)</span> coefficients. You will see both notations depending on if notation is indexed by <span class="math inline">\(i\)</span> or represented fully in matrix form. The <span class="math inline">\(\mathbf{x_i&#39;}\)</span> representation tends to come up more when we are dealing with likelihood equations. Here is a short video relating these notations.</li>
</ul>
<div class="vembedr">
<div>
<iframe src="https://www.youtube.com/embed/5QFj6_D-aAw" width="533" height="300" frameborder="0" allowfullscreen=""></iframe>
</div>
</div>
<p>Take the derivative wrt <span class="math inline">\(\theta\)</span>. Note: we have to take two derivatives- one for <span class="math inline">\(\mu\)</span> (<span class="math inline">\(\beta\)</span>) and one for <span class="math inline">\(\sigma^2\)</span>. For this example we will focus only on the derivative wrt to <span class="math inline">\(\beta\)</span>, as that it what gets us the coefficient estimates.<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a></p>
<p><em>Note: Below, we can simplify the expression of the log likelihood for taking the derivative with respect to <span class="math inline">\(\beta\)</span> because any term (i.e., the first term in the log likelihood in this case) that does not have a <span class="math inline">\(\beta\)</span> will fall out of the derivative expression. This is because when we take the derivative with respect to <span class="math inline">\(\beta\)</span> we treat all other terms as constants, and the slope of a constant (the rate of change of a constant) is zero. The curly <span class="math inline">\(\delta\)</span> in the expression below means “the derivative of …” with respect to <span class="math inline">\(\beta\)</span>.</em></p>
<p><span class="math display">\[\begin{align*}
\delta_\beta \ell(\theta | Y) &amp;= -\frac{1}{2\sigma^2}\sum_{i = 1}^N \delta_\beta (Y_i-\mathbf{x}_i&#39;\hat \beta)^2
\end{align*}\]</span>
The right term should look familiar! It is the same derivative we take when we are minimizing the least squares. Therefore, we will end up with <span class="math inline">\(S(\hat \theta)_\beta = \frac{1}{\sigma^2}X&#39;(Y - X\hat \beta)\)</span>. We set this equal to 0.
<span class="math display">\[\begin{align*}
\frac{1}{\sigma^2}X&#39;(Y - X\hat \beta) &amp;= 0\\
\frac{1}{\sigma^2}X&#39;Y &amp;= \frac{1}{\sigma^2}X&#39;X\hat \beta \\
(X&#39;X)^{-1}X&#39;Y = \hat \beta
\end{align*}\]</span></p>
</div>
<div id="hessian-and-information-matrix" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Hessian and Information Matrix</h3>
<p>The second derivative of the log-likelihood is the Hessian <span class="math inline">\((H(\theta))\)</span>.</p>
<ul>
<li>The second derivative is a measure of the curvature of the likelihood function. This will help us confirm that we are at a maximum, and it will also help us calculate the uncertainty.</li>
<li>The more curved (i.e., the steeper the curve), the more certainty we have.</li>
<li>The <span class="math inline">\(I\)</span> stands for the information matrix. The <span class="math inline">\(H\)</span> stands for Hessian. <span class="math inline">\(I(\theta) = - \mathbb{E}(H)\)</span>
<ul>
<li><span class="math inline">\(var(\theta) = [I(\theta)]^{-1} = ( - \mathbb{E}(H))^{-1}\)</span></li>
<li>Standard errors are the square roots of the diagonals of this <span class="math inline">\(k \times k\)</span> matrix (like <code>vcov()</code> in OLS)</li>
</ul></li>
</ul>
<p><strong><em>Example: Normal</em></strong></p>
<p>Start with the log-likelihood</p>
<p><span class="math display">\[\begin{align*}
\ell(\theta | Y) &amp;= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-x_i&#39;\beta)^2}{2\sigma^2}
\end{align*}\]</span></p>
<p>Because our <span class="math inline">\(\theta\)</span> has two parameters, the Hessian actually has four components. For this example, we will focus on one: the first and second derivatives wrt <span class="math inline">\(\beta\)</span>.</p>
<ul>
<li><p>Recall the first derivative = <span class="math inline">\(\frac{1}{\sigma^2}X&#39;(Y - X\hat \beta)\)</span>.</p></li>
<li><p>We now take the second derivative with respect to <span class="math inline">\(\hat \beta\)</span></p>
<p><span class="math display">\[\begin{align*}
\frac{\delta^2}{\delta \hat \beta} \frac{1}{\sigma^2}X&#39;(Y - X\hat \beta)&amp;= -\frac{1}{\sigma^2}X&#39;X
\end{align*}\]</span></p></li>
<li><p>To get our variance, we take the inverse of the negative (-) of this:</p>
<ul>
<li><span class="math inline">\(\sigma^2(X&#39;X)^{-1}\)</span> Should look familiar!</li>
</ul></li>
</ul>
<p>With this example, we can start to see why <code>lm</code> and <code>glm</code> for a normally distributed outcome generate the same estimates. The maximum likelihood estimator is the same as the least squares estimator.</p>
</div>
<div id="mle-estimation-algorithm" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> MLE Estimation Algorithm</h3>
<p>Suppose we are interested in finding the true probability <span class="math inline">\(p\)</span> that a comment made on twitter is toxic, and we have a small sample of hand-coded data. Let’s say we have <span class="math inline">\(n=8\)</span> observations where we could observe a <span class="math inline">\(y_i = 1\)</span> or <span class="math inline">\(0\)</span>. For example, let’s say we read an online sample of tweets and we classified tweets as “toxic=1” or “nontoxic=0.” In our sample of <span class="math inline">\(n=8\)</span>, we coded 6 of them as toxic and 2 as nontoxic.</p>
<p><img src="images/toxictweets.png" style="width:40.0%" /></p>
<p>We can write down the likelihood for a single observation using the Bernouilli pmf:</p>
<p><span class="math inline">\(L(p | y_i) = p^{y_i}*(1-p)^{(1-y_i)}\)</span></p>
<p>We could then write out the likelihood for all 8 observations as follows:</p>
<ul>
<li>Where the equation simplifies to <span class="math inline">\(p\)</span> for observations where <span class="math inline">\(y_i\)</span> = 1 and (1-p) for observations where <span class="math inline">\(y_i\)</span> = 0. For simplicity, let’s say <span class="math inline">\(i=1\)</span> to <span class="math inline">\(6\)</span> were toxic, and <span class="math inline">\(i=7\)</span> to <span class="math inline">\(8\)</span> were nontoxic.</li>
<li><span class="math inline">\(L(p | \mathbf{y}) = p * p * p * p * p * p * (1-p) * (1-p)\)</span></li>
</ul>
<p>Now a naive way to maximize the likelihood would be to just try out different quantities for <span class="math inline">\(p\)</span> and see which give us the maximum.</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="mle-estimation.html#cb275-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Let&#39;s try this for different p&#39;s</span></span>
<span id="cb275-2"><a href="mle-estimation.html#cb275-2" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fu">seq</span>(.<span class="dv">1</span>, .<span class="dv">9</span>, .<span class="dv">05</span>)</span>
<span id="cb275-3"><a href="mle-estimation.html#cb275-3" aria-hidden="true" tabindex="-1"></a>L <span class="ot">&lt;-</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>p)</span></code></pre></div>
<p>We can then visualize the likelihood results and figure out about at which value for <span class="math inline">\(\hat p\)</span> we have maximized the likelihood.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="mle-estimation.html#cb276-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="at">x=</span>p, <span class="at">y=</span>L, <span class="at">type=</span><span class="st">&quot;b&quot;</span>,</span>
<span id="cb276-2"><a href="mle-estimation.html#cb276-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>)</span>
<span id="cb276-3"><a href="mle-estimation.html#cb276-3" aria-hidden="true" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, p, p)</span></code></pre></div>
<p><img src="mlebookout_files/figure-html/unnamed-chunk-120-1.svg" width="672" /></p>
<p>When we have more complicated models, we are taking a similar approach–trying out different values and comparing the likelihood (or log likelihood), but we will rely on a specific algorithm(s) that will help us get to the maximum a bit faster than a naive search would allow.</p>
<p>Don’t worry the built-in functions in R will do this for you (e.g., what happens under the hood of <code>glm()</code>), but if you were to need to develop your own custom likelihood function for some reason, you could directly solve it through an optimization algorithm if no such built-in function is appropriate.</p>
<p><strong><em>You can skip the details below if you wish and jump to the MLE Properties section. This content will only be involved in problem sets as extra credit, as you may not have to use optim in your own research.</em></strong></p>
<p>The <code>optim</code> function in R provides one such approach. For this optimization approach, we will need to.</p>
<ul>
<li>Derive the likelihood and/or log likelihood function and score</li>
<li>Create an R <code>function</code> for the quantity to want to optimize (often the log likelihood) where given we provide the function certain values, the function returns the resulting quantity. (Kind of like when we supply the function <code>mean()</code> with a set of values, it returns the average of the values by computing the average under the hood of the function.)</li>
<li>Use <code>optim()</code> to maximize
<ul>
<li><code>optim(par, fn, ..., gr, method, control, hessian,...)</code>, where</li>
<li><code>par</code>: initial values of the parameters</li>
<li><code>fn</code>: function to be maximized (minimized)</li>
<li><code>gr</code>: optional argument, can include the gradient to help with optimization</li>
<li><code>...</code>: (specify other variables in <code>fn</code>)</li>
<li><code>method</code>: optimization algorithm</li>
<li><code>control</code>: parameters to fine-tune optimization</li>
<li><code>hessian</code>: returns the Hessian matrix if <code>TRUE</code></li>
</ul></li>
</ul>
<p>By default, <code>optim</code> performs minimization. Make sure to set <code>control = list(fnscale=-1)</code> for maximization</p>
<ul>
<li>For starting values <code>par</code>, least squares estimates are often used. More sensible starting values help your optimize more quickly. You may need to adjust the <code>maxit</code> control parameter to make sure the optimization converges.</li>
<li>A commonly used <code>method</code> is <code>BFGS</code> (a variant of Newton-Raphson), similar to what <code>glm()</code> uses, but there are other methods available.</li>
</ul>
<p><strong><em>Example 1: estimating p</em></strong></p>
<p>Let’s take our relatively simple example about toxic tweets above and optimize the likelihood. First, we create a function for the likelihood that will calculate the likelihood for the values supplied. In the future, our models will be complicated enough, we will stick with the log likelihood, which allows us to take a sum instead of a product.</p>
<p>One benefit of R is that you can write your own functions, just like <code>mean()</code> is a built-in function in R. For more information on writing functions, you can review Imai <a href="https://assets.press.princeton.edu/chapters/s11025.pdf">QSS Chapter 1 pg. 19.</a>.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="mle-estimation.html#cb277-1" aria-hidden="true" tabindex="-1"></a>lik.p <span class="ot">&lt;-</span> <span class="cf">function</span>(p){</span>
<span id="cb277-2"><a href="mle-estimation.html#cb277-2" aria-hidden="true" tabindex="-1"></a>  lh <span class="ot">&lt;-</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> p <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>p) <span class="sc">*</span> (<span class="dv">1</span><span class="sc">-</span>p)</span>
<span id="cb277-3"><a href="mle-estimation.html#cb277-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(lh)</span>
<span id="cb277-4"><a href="mle-estimation.html#cb277-4" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Ok, now that we have our likelihood function, we can optimize. We just have to tell R a starting parameter for <span class="math inline">\(\hat p\)</span>. Let’s give it a (relatively) bad one just to show how it works (i.e., can <code>optim</code> find the sensible .75 value. If you give the function too bad of a value, it might not converge before it maxes out and instead return a local min/max instead of a global one.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="mle-estimation.html#cb278-1" aria-hidden="true" tabindex="-1"></a>startphat <span class="ot">&lt;-</span> .<span class="dv">25</span></span>
<span id="cb278-2"><a href="mle-estimation.html#cb278-2" aria-hidden="true" tabindex="-1"></a>opt.fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> startphat, <span class="at">fn=</span>lik.p, <span class="at">method=</span><span class="st">&quot;BFGS&quot;</span>,</span>
<span id="cb278-3"><a href="mle-estimation.html#cb278-3" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb278-4"><a href="mle-estimation.html#cb278-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb278-5"><a href="mle-estimation.html#cb278-5" aria-hidden="true" tabindex="-1"></a><span class="do">## This should match our plot</span></span>
<span id="cb278-6"><a href="mle-estimation.html#cb278-6" aria-hidden="true" tabindex="-1"></a>opt.fit<span class="sc">$</span>par</span></code></pre></div>
<pre><code>[1] 0.7500035</code></pre>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="mle-estimation.html#cb280-1" aria-hidden="true" tabindex="-1"></a><span class="do">## you should check convergence. Want this to be 0 to make sure it converged</span></span>
<span id="cb280-2"><a href="mle-estimation.html#cb280-2" aria-hidden="true" tabindex="-1"></a>opt.fit<span class="sc">$</span>convergence</span></code></pre></div>
<pre><code>[1] 0</code></pre>
<p><strong><em>Example 2: Linear Model</em></strong></p>
<p>We can use <code>optim</code> to find a solution for a linear model by supplying R with our log likelihood function.</p>
<p>For the MLE of the normal linear model, our log likelihood equation is:</p>
<p><span class="math display">\[\begin{align*}
\ell(\theta | Y) &amp;= \sum_{i = 1}^N \log  \frac{1}{\sigma\sqrt{2\pi}} - \frac{(Y_i-\mathbf{x}_i&#39;\beta)^2}{2\sigma^2}
\end{align*}\]</span></p>
<p>Now that we have our log likelihood, we can write a function that for a given set of <span class="math inline">\(\hat \beta\)</span> and <span class="math inline">\(\hat \sigma^2\)</span> parameter values, <span class="math inline">\(X\)</span>, and <span class="math inline">\(Y\)</span>, it will return the log likelihood.</p>
<ul>
<li>Below we indicate we will supply an argument <code>par</code> (an arbitrary name) that will inclue our estimates for the parameters: <span class="math inline">\(k\)</span> values for the set of <span class="math inline">\(\hat \beta\)</span> estimates and a <span class="math inline">\(k + 1\)</span> value for the <span class="math inline">\(\hat \sigma^2\)</span> estimate. Many models with only have one set of parameters. This is actually a slightly more tricky example.</li>
<li>The <code>lt</code> line is the translation of the equation above into R code</li>
</ul>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="mle-estimation.html#cb282-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Log Likelihood function for the normal model</span></span>
<span id="cb282-2"><a href="mle-estimation.html#cb282-2" aria-hidden="true" tabindex="-1"></a>l_lm <span class="ot">&lt;-</span> <span class="cf">function</span>(par, Y, X){</span>
<span id="cb282-3"><a href="mle-estimation.html#cb282-3" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb282-4"><a href="mle-estimation.html#cb282-4" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> par[<span class="dv">1</span><span class="sc">:</span>k]</span>
<span id="cb282-5"><a href="mle-estimation.html#cb282-5" aria-hidden="true" tabindex="-1"></a>  sigma2 <span class="ot">&lt;-</span> par[(k<span class="sc">+</span><span class="dv">1</span>)]</span>
<span id="cb282-6"><a href="mle-estimation.html#cb282-6" aria-hidden="true" tabindex="-1"></a>  lt <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">log</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="fu">sqrt</span>(sigma2)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">2</span><span class="sc">*</span>pi))) <span class="sc">-</span> ((Y <span class="sc">-</span> X <span class="sc">%*%</span> beta)<span class="sc">^</span><span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>sigma2)))</span>
<span id="cb282-7"><a href="mle-estimation.html#cb282-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(lt)</span>
<span id="cb282-8"><a href="mle-estimation.html#cb282-8" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<p>Now that we have our function, we can apply it to a problem.</p>
<p>Let’s use an example with a sample of Democrats from the 2016 American National Election Study dataset. This example is based on the article “Hostile Sexism, Racial Resentment, and Political Mobilization” by Kevin K. Banda and Erin C. Cassese published in <em>Political Behavior</em> in 2020. We are not replicating their article precisely, but we use similar data and study similar relationships.</p>
<p>The researchers were interested in how cross-pressures influence the political participation of different partisan groups. In particular, they hypothesized that Democrats in the U.S. who held more sexist views would be demobilized from political participation in 2016, a year in which Hillary Clinton ran for the presidency.</p>
<p>The data we are using are available <a href="https://github.com/ktmccabe/teachingdata">anesdems.csv</a> and represent a subset of the data for Democrats (including people who lean toward the Democratic party). We have a few variables of interest</p>
<ul>
<li><code>participation</code>: a 0 to 8 variable indicating the extent of a respondent’s political participation</li>
<li><code>female</code>: a 0 or 1 variable indicating if the respondent is female</li>
<li><code>edu</code>: a numeric variable indicating a respondent’s education level</li>
<li><code>age</code>: a numeric variable indicating a respondent’s age.</li>
<li><code>sexism</code>: a numeric variable indicating a respondent’s score on a battery of questions designed to assess hostile sexism, where higher values indicate more hostile sexism.</li>
</ul>
<p>Let’s regress participation on these variables and estimate it using OLS, GLM, and <code>optim</code>. Note, OLS and GLM fit through their functions in R will automatically drop any observations that have missing data on these variables. To make it comparable with <code>optim</code>, we will manually eliminate missing data.</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="mle-estimation.html#cb283-1" aria-hidden="true" tabindex="-1"></a>anes <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv&quot;</span>)</span>
<span id="cb283-2"><a href="mle-estimation.html#cb283-2" aria-hidden="true" tabindex="-1"></a><span class="do">## choose variables we will use</span></span>
<span id="cb283-3"><a href="mle-estimation.html#cb283-3" aria-hidden="true" tabindex="-1"></a>anes <span class="ot">&lt;-</span> <span class="fu">subset</span>(anes, <span class="at">select=</span><span class="fu">c</span>(<span class="st">&quot;participation&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;edu&quot;</span>, <span class="st">&quot;sexism&quot;</span>, <span class="st">&quot;female&quot;</span>))</span>
<span id="cb283-4"><a href="mle-estimation.html#cb283-4" aria-hidden="true" tabindex="-1"></a><span class="do">## omit observations with missing data on these variables</span></span>
<span id="cb283-5"><a href="mle-estimation.html#cb283-5" aria-hidden="true" tabindex="-1"></a>anes <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(anes)</span>
<span id="cb283-6"><a href="mle-estimation.html#cb283-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb283-7"><a href="mle-estimation.html#cb283-7" aria-hidden="true" tabindex="-1"></a><span class="do">## OLS and GLM regression</span></span>
<span id="cb283-8"><a href="mle-estimation.html#cb283-8" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(participation <span class="sc">~</span> female <span class="sc">+</span> edu <span class="sc">+</span> age <span class="sc">+</span> sexism, <span class="at">data=</span>anes)</span>
<span id="cb283-9"><a href="mle-estimation.html#cb283-9" aria-hidden="true" tabindex="-1"></a>fit.glm <span class="ot">&lt;-</span> <span class="fu">glm</span>(participation <span class="sc">~</span> female <span class="sc">+</span> edu <span class="sc">+</span> age <span class="sc">+</span> sexism, <span class="at">data=</span>anes,</span>
<span id="cb283-10"><a href="mle-estimation.html#cb283-10" aria-hidden="true" tabindex="-1"></a>               <span class="at">family=</span><span class="fu">gaussian</span>(<span class="at">link=</span><span class="st">&quot;identity&quot;</span>))</span></code></pre></div>
<p>Now we will build our data for <code>optim</code>. We need <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and a set of starting <span class="math inline">\(\hat \beta\)</span> and <span class="math inline">\(\hat \sigma^2\)</span> values.</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="mle-estimation.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="do">## X and Y data</span></span>
<span id="cb284-2"><a href="mle-estimation.html#cb284-2" aria-hidden="true" tabindex="-1"></a>X.anes <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(fit)</span>
<span id="cb284-3"><a href="mle-estimation.html#cb284-3" aria-hidden="true" tabindex="-1"></a>Y.anes <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(anes<span class="sc">$</span>participation)</span>
<span id="cb284-4"><a href="mle-estimation.html#cb284-4" aria-hidden="true" tabindex="-1"></a><span class="do">## make sure dimensions are the same</span></span>
<span id="cb284-5"><a href="mle-estimation.html#cb284-5" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(X.anes)</span></code></pre></div>
<pre><code>[1] 1585</code></pre>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="mle-estimation.html#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(Y.anes)</span></code></pre></div>
<pre><code>[1] 1585</code></pre>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb288-1"><a href="mle-estimation.html#cb288-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Pick starting values for parameters</span></span>
<span id="cb288-2"><a href="mle-estimation.html#cb288-2" aria-hidden="true" tabindex="-1"></a>startbetas <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit)</span>
<span id="cb288-3"><a href="mle-estimation.html#cb288-3" aria-hidden="true" tabindex="-1"></a><span class="do">## Recall our estimate for sigma-squared based on the residuals</span></span>
<span id="cb288-4"><a href="mle-estimation.html#cb288-4" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X.anes)</span>
<span id="cb288-5"><a href="mle-estimation.html#cb288-5" aria-hidden="true" tabindex="-1"></a>startsigma <span class="ot">&lt;-</span> <span class="fu">sum</span>(fit<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (<span class="fu">nrow</span>(X.anes) <span class="sc">-</span> k )</span>
<span id="cb288-6"><a href="mle-estimation.html#cb288-6" aria-hidden="true" tabindex="-1"></a>startpar <span class="ot">&lt;-</span> <span class="fu">c</span>(startbetas, startsigma)</span>
<span id="cb288-7"><a href="mle-estimation.html#cb288-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb288-8"><a href="mle-estimation.html#cb288-8" aria-hidden="true" tabindex="-1"></a><span class="do">## Fit model</span></span>
<span id="cb288-9"><a href="mle-estimation.html#cb288-9" aria-hidden="true" tabindex="-1"></a><span class="do">## But let&#39;s make it harder on the optimization by providing arbitrary starting values</span></span>
<span id="cb288-10"><a href="mle-estimation.html#cb288-10" aria-hidden="true" tabindex="-1"></a><span class="do">## (normally you wouldn&#39;t do this)</span></span>
<span id="cb288-11"><a href="mle-estimation.html#cb288-11" aria-hidden="true" tabindex="-1"></a>startpar <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb288-12"><a href="mle-estimation.html#cb288-12" aria-hidden="true" tabindex="-1"></a>opt.fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> startpar, <span class="at">fn=</span>l_lm, <span class="at">X =</span> X.anes,</span>
<span id="cb288-13"><a href="mle-estimation.html#cb288-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">Y=</span>Y.anes, <span class="at">method=</span><span class="st">&quot;BFGS&quot;</span>,</span>
<span id="cb288-14"><a href="mle-estimation.html#cb288-14" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb288-15"><a href="mle-estimation.html#cb288-15" aria-hidden="true" tabindex="-1"></a>                   <span class="at">hessian=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>We can compare this optimization approach to the output in <code>glm()</code>.</p>
<p>We can first compare the log likelihoods</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="mle-estimation.html#cb289-1" aria-hidden="true" tabindex="-1"></a><span class="fu">logLik</span>(fit.glm)</span></code></pre></div>
<pre><code>&#39;log Lik.&#39; -2661.428 (df=6)</code></pre>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="mle-estimation.html#cb291-1" aria-hidden="true" tabindex="-1"></a>opt.fit<span class="sc">$</span>value</span></code></pre></div>
<pre><code>[1] -2661.428</code></pre>
<p>We can compare the coefficients.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="mle-estimation.html#cb293-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Coefficients</span></span>
<span id="cb293-2"><a href="mle-estimation.html#cb293-2" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">coef</span>(fit), <span class="at">digits=</span><span class="dv">4</span>)</span>
<span id="cb293-3"><a href="mle-estimation.html#cb293-3" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">coef</span>(fit.glm), <span class="at">digits=</span><span class="dv">4</span>)</span>
<span id="cb293-4"><a href="mle-estimation.html#cb293-4" aria-hidden="true" tabindex="-1"></a><span class="fu">round</span>(opt.fit<span class="sc">$</span>par, <span class="at">digits=</span><span class="dv">4</span>)[<span class="dv">1</span><span class="sc">:</span>k]</span></code></pre></div>
<pre><code>(Intercept)      female         edu         age      sexism 
     0.9293     -0.2175      0.1668      0.0088     -0.9818 
(Intercept)      female         edu         age      sexism 
     0.9293     -0.2175      0.1668      0.0088     -0.9818 
[1]  0.9294 -0.2175  0.1668  0.0088 -0.9819</code></pre>
<p>We can add the gradient of the log likelihood to help improve optimization. This requires specifying the first derivative (the score) of the parameters. Unfortunately this means taking the derivative of that ugly normal log likelihood above. Again, with the normal model, we have two scores because of <span class="math inline">\(\hat \beta\)</span> and <span class="math inline">\(\hat \sigma^2\)</span>. For others, we may just have one.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="mle-estimation.html#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="do">## first derivative function</span></span>
<span id="cb295-2"><a href="mle-estimation.html#cb295-2" aria-hidden="true" tabindex="-1"></a>score_lm <span class="ot">&lt;-</span> <span class="cf">function</span>(par, Y, X){</span>
<span id="cb295-3"><a href="mle-estimation.html#cb295-3" aria-hidden="true" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb295-4"><a href="mle-estimation.html#cb295-4" aria-hidden="true" tabindex="-1"></a>  beta <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(par[<span class="dv">1</span><span class="sc">:</span>k])</span>
<span id="cb295-5"><a href="mle-estimation.html#cb295-5" aria-hidden="true" tabindex="-1"></a>  scorebeta <span class="ot">&lt;-</span> (<span class="dv">1</span><span class="sc">/</span>par[k<span class="sc">+</span><span class="dv">1</span>]) <span class="sc">*</span> (<span class="fu">t</span>(X) <span class="sc">%*%</span> (Y <span class="sc">-</span> X <span class="sc">%*%</span> beta))</span>
<span id="cb295-6"><a href="mle-estimation.html#cb295-6" aria-hidden="true" tabindex="-1"></a>  scoresigma <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fu">nrow</span>(X)<span class="sc">/</span>(par[k<span class="sc">+</span><span class="dv">1</span>]<span class="sc">*</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">sum</span>((Y <span class="sc">-</span> X <span class="sc">%*%</span> beta)<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span>(<span class="dv">2</span> <span class="sc">*</span> par[k<span class="sc">+</span><span class="dv">1</span>]<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb295-7"><a href="mle-estimation.html#cb295-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(scorebeta, scoresigma))</span>
<span id="cb295-8"><a href="mle-estimation.html#cb295-8" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb295-9"><a href="mle-estimation.html#cb295-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb295-10"><a href="mle-estimation.html#cb295-10" aria-hidden="true" tabindex="-1"></a><span class="do">## Fit model</span></span>
<span id="cb295-11"><a href="mle-estimation.html#cb295-11" aria-hidden="true" tabindex="-1"></a>opt.fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> startpar, <span class="at">fn=</span>l_lm, <span class="at">gr=</span>score_lm, <span class="at">X =</span> X.anes,</span>
<span id="cb295-12"><a href="mle-estimation.html#cb295-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">Y=</span>Y.anes, <span class="at">method=</span><span class="st">&quot;BFGS&quot;</span>,</span>
<span id="cb295-13"><a href="mle-estimation.html#cb295-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">control=</span><span class="fu">list</span>(<span class="at">fnscale=</span><span class="sc">-</span><span class="dv">1</span>),</span>
<span id="cb295-14"><a href="mle-estimation.html#cb295-14" aria-hidden="true" tabindex="-1"></a>                   <span class="at">hessian=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<p>In addition to using <code>optim</code>, we can program our own Newton-Raphson algorithm, which is a method that continually updates the coefficient estimates <span class="math inline">\(\hat \beta\)</span> until it converges on a set of estimates. We will see this in a future section. The general algorithm involves the components we’ve seen before: values for <span class="math inline">\(\hat \beta\)</span>, the score, and the Hessian.</p>
<ul>
<li>Newton-Raphson: <span class="math inline">\(\hat \beta_{new} = \hat \beta_{old} - H(\beta_{old})^{-1}S(\hat \beta_{old})\)</span></li>
</ul>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="5">
<li id="fn5"><p>Essentially, you need to take derivatives with respect to each of the parameters. Some models we use will have only one parameter, which is easier.<a href="mle-estimation.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="generalized-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="mle-properties.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
