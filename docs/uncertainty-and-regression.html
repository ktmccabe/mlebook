<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.5 Uncertainty and Regression | MLE for Political Science</title>
  <meta name="description" content="4.5 Uncertainty and Regression | MLE for Political Science" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="4.5 Uncertainty and Regression | MLE for Political Science" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.5 Uncertainty and Regression | MLE for Political Science" />
  
  
  

<meta name="author" content="Instructor: Katie McCabe" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="week-1-thursday-tutorial.html"/>
<link rel="next" href="generating-predictions-from-regression-models.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/vembedr-0.1.4/css/vembedr.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course Overview</a></li>
<li class="chapter" data-level="2" data-path="rover.html"><a href="rover.html"><i class="fa fa-check"></i><b>2</b> R Overview</a>
<ul>
<li class="chapter" data-level="2.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html"><i class="fa fa-check"></i><b>2.1</b> First Time with R and RStudio</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#open-rstudio"><i class="fa fa-check"></i><b>2.1.1</b> <strong>Open RStudio</strong></a></li>
<li class="chapter" data-level="2.1.2" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#using-r-as-a-calculator"><i class="fa fa-check"></i><b>2.1.2</b> <strong>Using R as a Calculator</strong></a></li>
<li class="chapter" data-level="2.1.3" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#working-in-an-r-script"><i class="fa fa-check"></i><b>2.1.3</b> <strong>Working in an R Script</strong></a></li>
<li class="chapter" data-level="2.1.4" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#preparing-your-r-script"><i class="fa fa-check"></i><b>2.1.4</b> <strong>Preparing your R script</strong></a></li>
<li class="chapter" data-level="2.1.5" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#executing-commands-in-your-r-script"><i class="fa fa-check"></i><b>2.1.5</b> <strong>Executing Commands in your R script</strong></a></li>
<li class="chapter" data-level="2.1.6" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#objects"><i class="fa fa-check"></i><b>2.1.6</b> <strong>Objects</strong></a></li>
<li class="chapter" data-level="2.1.7" data-path="first-time-with-r-and-rstudio.html"><a href="first-time-with-r-and-rstudio.html#practice"><i class="fa fa-check"></i><b>2.1.7</b> <strong>Practice</strong></a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="tutorials.html"><a href="tutorials.html"><i class="fa fa-check"></i><b>2.2</b> Tutorials</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="math.html"><a href="math.html"><i class="fa fa-check"></i><b>3</b> The MATH</a>
<ul>
<li class="chapter" data-level="3.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html"><i class="fa fa-check"></i><b>3.1</b> Mathematical Operations</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="mathematical-operations.html"><a href="mathematical-operations.html#order-of-operations"><i class="fa fa-check"></i><b>3.1.1</b> <strong>Order of Operations</strong></a></li>
<li class="chapter" data-level="3.1.2" data-path="mathematical-operations.html"><a href="mathematical-operations.html#exponents"><i class="fa fa-check"></i><b>3.1.2</b> <strong>Exponents</strong></a></li>
<li class="chapter" data-level="3.1.3" data-path="mathematical-operations.html"><a href="mathematical-operations.html#summations-and-products"><i class="fa fa-check"></i><b>3.1.3</b> <strong>Summations and Products</strong></a></li>
<li class="chapter" data-level="3.1.4" data-path="mathematical-operations.html"><a href="mathematical-operations.html#logarithms"><i class="fa fa-check"></i><b>3.1.4</b> <strong>Logarithms</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html"><i class="fa fa-check"></i><b>3.2</b> Mathematical Operations in R</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#pemdas"><i class="fa fa-check"></i><b>3.2.1</b> PEMDAS</a></li>
<li class="chapter" data-level="3.2.2" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#exponents-1"><i class="fa fa-check"></i><b>3.2.2</b> Exponents</a></li>
<li class="chapter" data-level="3.2.3" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#summations"><i class="fa fa-check"></i><b>3.2.3</b> Summations</a></li>
<li class="chapter" data-level="3.2.4" data-path="mathematical-operations-in-r.html"><a href="mathematical-operations-in-r.html#logarithms-1"><i class="fa fa-check"></i><b>3.2.4</b> Logarithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="derivatives.html"><a href="derivatives.html"><i class="fa fa-check"></i><b>3.3</b> Derivatives</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="derivatives.html"><a href="derivatives.html#derivatives-1"><i class="fa fa-check"></i><b>3.3.1</b> <strong>Derivatives</strong></a></li>
<li class="chapter" data-level="3.3.2" data-path="derivatives.html"><a href="derivatives.html#critical-points-for-minima-or-maxima"><i class="fa fa-check"></i><b>3.3.2</b> <strong>Critical Points for Minima or Maxima</strong></a></li>
<li class="chapter" data-level="3.3.3" data-path="derivatives.html"><a href="derivatives.html#common-derivative-rules"><i class="fa fa-check"></i><b>3.3.3</b> <strong>Common Derivative Rules</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html"><i class="fa fa-check"></i><b>3.4</b> Vectors and Matrices</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-basics"><i class="fa fa-check"></i><b>3.4.1</b> <strong>Matrix Basics</strong></a></li>
<li class="chapter" data-level="3.4.2" data-path="vectors-and-matrices.html"><a href="vectors-and-matrices.html#matrix-operations"><i class="fa fa-check"></i><b>3.4.2</b> Matrix Operations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html"><i class="fa fa-check"></i><b>3.5</b> Additional Matrix Tidbits that Will Come Up</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#transpose"><i class="fa fa-check"></i><b>3.5.1</b> <strong>Transpose</strong></a></li>
<li class="chapter" data-level="3.5.2" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#additional-matrix-properties-and-rules"><i class="fa fa-check"></i><b>3.5.2</b> <strong>Additional Matrix Properties and Rules</strong></a></li>
<li class="chapter" data-level="3.5.3" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#matrix-rules"><i class="fa fa-check"></i><b>3.5.3</b> <strong>Matrix Rules</strong></a></li>
<li class="chapter" data-level="3.5.4" data-path="additional-matrix-tidbits-that-will-come-up.html"><a href="additional-matrix-tidbits-that-will-come-up.html#derivatives-with-matrices-and-vectors"><i class="fa fa-check"></i><b>3.5.4</b> <strong>Derivatives with Matrices and Vectors</strong></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="practice-problems.html"><a href="practice-problems.html"><i class="fa fa-check"></i><b>3.6</b> Practice Problems</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="practice-problems.html"><a href="practice-problems.html#practice-problem-solutions"><i class="fa fa-check"></i><b>3.6.1</b> Practice Problem Solutions</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ols.html"><a href="ols.html"><i class="fa fa-check"></i><b>4</b> Review of OLS</a>
<ul>
<li class="chapter" data-level="4.1" data-path="introducing-ols-regression.html"><a href="introducing-ols-regression.html"><i class="fa fa-check"></i><b>4.1</b> Introducing OLS Regression</a></li>
<li class="chapter" data-level="4.2" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html"><i class="fa fa-check"></i><b>4.2</b> Diving Deeper into OLS Matrix Representation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="diving-deeper-into-ols-matrix-representation.html"><a href="diving-deeper-into-ols-matrix-representation.html#estimating-the-coefficients"><i class="fa fa-check"></i><b>4.2.1</b> Estimating the Coefficients</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html"><i class="fa fa-check"></i><b>4.3</b> OLS Regression in R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#example-predicting-current-election-votes-from-past-election-votes"><i class="fa fa-check"></i><b>4.3.1</b> Example: Predicting Current Election Votes from Past Election Votes</a></li>
<li class="chapter" data-level="4.3.2" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#plotting-regression-results"><i class="fa fa-check"></i><b>4.3.2</b> Plotting Regression Results</a></li>
<li class="chapter" data-level="4.3.3" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#finding-coefficients-without-lm"><i class="fa fa-check"></i><b>4.3.3</b> Finding Coefficients without <code>lm</code></a></li>
<li class="chapter" data-level="4.3.4" data-path="practice-problems.html"><a href="practice-problems.html#practice-problems"><i class="fa fa-check"></i><b>4.3.4</b> Practice Problems</a></li>
<li class="chapter" data-level="4.3.5" data-path="ols-regression-in-r.html"><a href="ols-regression-in-r.html#code-for-solutions"><i class="fa fa-check"></i><b>4.3.5</b> Code for solutions</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="week-1-thursday-tutorial.html"><a href="week-1-thursday-tutorial.html"><i class="fa fa-check"></i><b>4.4</b> Week 1 Thursday Tutorial</a></li>
<li class="chapter" data-level="4.5" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html"><i class="fa fa-check"></i><b>4.5</b> Uncertainty and Regression</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#variance-of-the-coefficients"><i class="fa fa-check"></i><b>4.5.1</b> Variance of the Coefficients</a></li>
<li class="chapter" data-level="4.5.2" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.5.2</b> Hypothesis Testing</a></li>
<li class="chapter" data-level="4.5.3" data-path="uncertainty-and-regression.html"><a href="uncertainty-and-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>4.5.3</b> Goodness of Fit</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="generating-predictions-from-regression-models.html"><a href="generating-predictions-from-regression-models.html"><i class="fa fa-check"></i><b>4.6</b> Generating predictions from regression models</a></li>
<li class="chapter" data-level="4.7" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html"><i class="fa fa-check"></i><b>4.7</b> Wrapping up OLS</a>
<ul>
<li class="chapter" data-level="4.7.1" data-path="wrapping-up-ols.html"><a href="wrapping-up-ols.html#practice-problems-1"><i class="fa fa-check"></i><b>4.7.1</b> Practice Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="mle.html"><a href="mle.html"><i class="fa fa-check"></i><b>5</b> Introduction to MLE</a>
<ul>
<li class="chapter" data-level="5.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html"><i class="fa fa-check"></i><b>5.1</b> What is likelihood?</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="what-is-likelihood.html"><a href="what-is-likelihood.html#summarizing-steps-for-maximum-likelihood"><i class="fa fa-check"></i><b>5.1.1</b> Summarizing Steps for Maximum Likelihood</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>5.2</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-model."><i class="fa fa-check"></i><b>5.2.1</b> GLM Model.</a></li>
<li class="chapter" data-level="5.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#linking-likelihood-and-the-glm"><i class="fa fa-check"></i><b>5.2.2</b> Linking likelihood and the GLM</a></li>
<li class="chapter" data-level="5.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#glm-in-r"><i class="fa fa-check"></i><b>5.2.3</b> GLM in R</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MLE for Political Science</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="uncertainty-and-regression" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Uncertainty and Regression</h2>
<p>We have now gone through the process of minimizing the sum of squared errors (<span class="math inline">\(\mathbf{e&#39;e}\)</span>) and deriving estimates for the OLS coefficients <span class="math inline">\(\hat \beta = (X&#39;X)^{-1}X&#39;Y\)</span>. In this section, we will discuss how to generate estimates of the uncertainty around these estimates.</p>
<p>Where we are going:</p>
<ul>
<li>In the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X).</li>
</ul>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="uncertainty-and-regression.html#cb186-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Load Data</span></span>
<span id="cb186-2"><a href="uncertainty-and-regression.html#cb186-2" aria-hidden="true" tabindex="-1"></a>florida <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv&quot;</span>)</span>
<span id="cb186-3"><a href="uncertainty-and-regression.html#cb186-3" aria-hidden="true" tabindex="-1"></a>fit<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(Buchanan00 <span class="sc">~</span> Perot96, <span class="at">data =</span> florida)</span>
<span id="cb186-4"><a href="uncertainty-and-regression.html#cb186-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>
Call:
lm(formula = Buchanan00 ~ Perot96, data = florida)

Residuals:
    Min      1Q  Median      3Q     Max 
-612.74  -65.96    1.94   32.88 2301.66 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.34575   49.75931   0.027    0.979    
Perot96      0.03592    0.00434   8.275 9.47e-12 ***
---
Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1

Residual standard error: 316.4 on 65 degrees of freedom
Multiple R-squared:  0.513,	Adjusted R-squared:  0.5055 
F-statistic: 68.48 on 1 and 65 DF,  p-value: 9.474e-12</code></pre>
<p>The summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the <code>lm()</code> function neatly places these right next to the coefficients. We will now discuss how we get to these values.</p>
<div id="variance-of-the-coefficients" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Variance of the Coefficients</h3>
<p>The standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates <span class="math inline">\(\hat \beta\)</span> of the parameter <span class="math inline">\(\beta\)</span> across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance.</p>
<p>Let’s take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables.</p>
<ul>
<li>Start with the model: <span class="math inline">\(Y = X\beta + \epsilon\)</span>
<ul>
<li>We want to generate uncertainty for our estimate of <span class="math inline">\(\hat \beta =(X&#39;X)^{-1}X&#39;Y\)</span></li>
</ul></li>
<li>Note: Conditional on fixed values of <span class="math inline">\(X\)</span> (I say fixed values because this is our data. We know <span class="math inline">\(X\)</span> from our dataset.), the only random component is <span class="math inline">\(\epsilon\)</span>.
<ul>
<li>What does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If <span class="math inline">\(Y\)</span> was a deterministic result of <span class="math inline">\(X\)</span>, we would have no need for it, but it’s not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by <span class="math inline">\(\epsilon\)</span>.</li>
</ul></li>
</ul>
<p>Below we go through how to arrive at the mathematical quantity representing the variance of <span class="math inline">\(\hat \beta\)</span> which we will notate as <span class="math inline">\(\mathbf{V}(\hat\beta)\)</span>. The first part of the math below is just substituting terms:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{V}(\widehat{\beta}) &amp;= 
\mathbf{V}( (X^T X) ^{-1} X^T Y))  \\
&amp;= \underbrace{\mathbf{V}( (X^T X) ^{-1} X^T (X\beta + \epsilon))}_\text{Sub in the expression for Y from above}  \\
&amp;= \underbrace{\mathbf{V}((X^T X) ^{-1} X^T X \beta + (X^T X) ^{-1} X^T \epsilon)}_\text{Distribute the term to the items in the parentheses}  \\
&amp;= \underbrace{\mathbf{V}(\beta + (X^T X) ^{-1} X^T \epsilon)}_\text{Using the rules of inverses, the two terms next to $\beta$ canceled each other out}  
\end{align*}\]</span></p>
<p>The next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular:</p>
<ul>
<li>The variance of a constant is zero.</li>
<li>When you have a constant multipled by a random variable, e.g., <span class="math inline">\(\mathbf{V}(4d)\)</span>, it can come out of the variance operator, but must be squared: <span class="math inline">\(16\mathbf{V}(d)\)</span></li>
<li>Putting these together: <span class="math inline">\(\mathbf{V}(2 + 4d)= 16\mathbf{V}(d)\)</span></li>
</ul>
<p>Knowing these rules, we can proceed:
<span class="math display">\[\begin{align*}
\mathbf{V}(\widehat{\beta}) &amp;= \mathbf{V}(\beta + (X^T X) ^{-1} X^T \epsilon) \\
&amp;=\underbrace{ \mathbf{V}((X^T X) ^{-1} X^T \epsilon)}_\text{$\beta$ drops out because in a regression it is an unkown &quot;parameter&quot;-- it&#39;s constant, which means its variance is zero.}\\
&amp;= \underbrace{(X^T X)^{-1}X^T \mathbf{V}( \epsilon) ((X^T X)^{-1}X^T)^T}_\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to &quot;square&quot; the matrix.}\\
&amp;= (X^T X)^{-1}X^T \mathbf{V}( \epsilon) X (X^T X)^{-1}
\end{align*}\]</span></p>
<p>The resulting quantity is our expression for the <span class="math inline">\(\mathbf{V}(\hat \beta)\)</span>. However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka “constant” or “equal error variance” which says that the variance of the errors are the same across observations: <span class="math inline">\(\mathbf{V}(\epsilon) = \sigma^2 I_n\)</span>.</p>
<ul>
<li>If we assume homoscedastic errors, then Var<span class="math inline">\((\epsilon) = \sigma^2 I_n\)</span></li>
</ul>
<p><span class="math display">\[\begin{align*}
\mathbf{V}(\widehat{\beta})  &amp;= (X^T X)^{-1}X^T \mathbf{V}( \epsilon) X (X^T X)^{-1}\\
&amp;= \underbrace{(X^T X)^{-1}X^T \sigma^2I_n X (X^T X)^{-1}}_\text{Assume homoskedasticity}\\
&amp;= \underbrace{\sigma^2I_n(X^T X)^{-1} X^T X (X^T X)^{-1}}_\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.}  \\
&amp;= \sigma^2(X^T X)^{-1} 
\end{align*}\]</span></p>
<p>All done! This expression: <span class="math inline">\(\sigma^2(X^T X)^{-1}\)</span> represents the variance of our coefficient estimates. Note its dimensions: <span class="math inline">\(k \times k\)</span>. It has the same number of rows and columns as the number of our independent variables (plus the intercept).</p>
<p>There is one catch, though. How do we know what <span class="math inline">\(\sigma^2\)</span> is? Well, we don’t. Just like the unknown parameter <span class="math inline">\(\beta\)</span>, we have to estimate it in our regression model.</p>
<p>Just like with the coefficients, we notate our estimate as <span class="math inline">\(\widehat{\sigma}^2\)</span>. Our estimate is based on the observed residual errors in the model and is as follows:</p>
<ul>
<li><span class="math inline">\(\widehat{\sigma}^2 = \frac{1}{N-K}\sum_{i=1}^N \widehat{\epsilon_i^2} = \frac{1}{N-K} \mathbf{e&#39;e}\)</span></li>
</ul>
<p>That means our <strong>estimate</strong> of the variance of the coefficients is found within: <span class="math inline">\(\hat \sigma^2(X^T X)^{-1}\)</span></p>
<p>Again, this is a <span class="math inline">\(k \times k\)</span> matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using <code>vcov()</code>.</p>
<div class="sourceCode" id="cb188"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb188-1"><a href="uncertainty-and-regression.html#cb188-1" aria-hidden="true" tabindex="-1"></a><span class="fu">vcov</span>(fit<span class="fl">.1</span>)</span></code></pre></div>
<pre><code>             (Intercept)       Perot96
(Intercept) 2475.9885768 -1.360074e-01
Perot96       -0.1360074  1.883619e-05</code></pre>
<p>This is the same that we would get if manually we took the residuals and multiplied it by our <span class="math inline">\(X\)</span> matrix according to the formula above:</p>
<div class="sourceCode" id="cb190"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb190-1"><a href="uncertainty-and-regression.html#cb190-1" aria-hidden="true" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, florida<span class="sc">$</span>Perot96)</span>
<span id="cb190-2"><a href="uncertainty-and-regression.html#cb190-2" aria-hidden="true" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="fu">residuals</span>(fit<span class="fl">.1</span>))</span>
<span id="cb190-3"><a href="uncertainty-and-regression.html#cb190-3" aria-hidden="true" tabindex="-1"></a>sigmahat <span class="ot">&lt;-</span> ((<span class="fu">t</span>(e) <span class="sc">%*%</span> e) <span class="sc">/</span> (<span class="fu">nrow</span>(florida) <span class="sc">-</span><span class="dv">2</span>)) </span>
<span id="cb190-4"><a href="uncertainty-and-regression.html#cb190-4" aria-hidden="true" tabindex="-1"></a><span class="do">## tell r to stop treating sigmahat as a matrix</span></span>
<span id="cb190-5"><a href="uncertainty-and-regression.html#cb190-5" aria-hidden="true" tabindex="-1"></a>sigmahat <span class="ot">&lt;-</span><span class="fu">as.numeric</span>(sigmahat)</span>
<span id="cb190-6"><a href="uncertainty-and-regression.html#cb190-6" aria-hidden="true" tabindex="-1"></a>XtX <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span>X)</span>
<span id="cb190-7"><a href="uncertainty-and-regression.html#cb190-7" aria-hidden="true" tabindex="-1"></a>sigmahat <span class="sc">*</span> XtX</span></code></pre></div>
<pre><code>             [,1]          [,2]
[1,] 2475.9885768 -1.360074e-01
[2,]   -0.1360074  1.883619e-05</code></pre>
<p>The terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient <span class="math inline">\(k\)</span> is: s.e.(<span class="math inline">\(\hat{\beta_k}) = \sqrt{\widehat{\sigma}^2 (X&#39;X)^{-1}}_{kk}\)</span>. The off-diagonal components represent the covariances between the coefficients.</p>
<p>Recall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="uncertainty-and-regression.html#cb192-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(fit<span class="fl">.1</span>)))</span></code></pre></div>
<pre><code> (Intercept)      Perot96 
49.759306434  0.004340068 </code></pre>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="uncertainty-and-regression.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit<span class="fl">.1</span>)<span class="sc">$</span>coefficients[,<span class="dv">2</span>]</span></code></pre></div>
<pre><code> (Intercept)      Perot96 
49.759306434  0.004340068 </code></pre>
<p>Why should I care?</p>
<ol style="list-style-type: decimal">
<li>Well R actually doesn’t make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model.</li>
<li>Sometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or “heteroscedasticity.” Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression <span class="math inline">\((X^T X)^{-1}X^T \mathbf{V}( \epsilon) X (X^T X)^{-1}\)</span> and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as “robust standard errors.” There are many different robust estimators, and you will likely come across them in your research.</li>
</ol>
<p>Some of you may have learned the formal, general definition for variance as defined in terms of expected value: <span class="math inline">\(\mathbb{E}[(\widehat{m} - \mathbb{E}(\widehat{m}))^2 ]\)</span>. We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{V}(\widehat{\beta}) &amp;= \mathbb{E}[(\widehat{\beta} - \mathbb{E}(\hat \beta))^2)] \\
&amp;= \mathbb{E}[(\widehat{\beta} - \beta)^2]\\
&amp;= \mathbb{E}[(\widehat{\beta} - \beta)(\widehat{\beta} - \beta)^T ] \\ &amp;= 
\mathbb{E}[(X^T X) ^{-1} X^TY - \beta)(X^T X) ^{-1} X^TY - \beta)^T]  \\
&amp;= \mathbb{E}[(X^T X) ^{-1} X^T(X\beta + \epsilon) - \beta)(X^T X) ^{-1} X^T(X\beta + \epsilon) - \beta)^T]\\
&amp;=  \mathbb{E}[(X^T X) ^{-1} X^TX\beta + (X^T X) ^{-1} X^T\epsilon - \beta)(X^T X) ^{-1} X^TX\beta + (X^T X) ^{-1} X^T\epsilon - \beta)^T]\\
&amp;=  \mathbb{E}[(\beta + (X^T X) ^{-1} X^T\epsilon - \beta)(\beta + (X^T X) ^{-1} X^T\epsilon - \beta)^T]\\
&amp;=  \mathbb{E}[(X^T X) ^{-1} X^T\epsilon)(X^T X) ^{-1} X^T\epsilon)^T]\\
&amp;= (X^T X) ^{-1} X^T\mathbb{E}(\epsilon\epsilon^T)X(X^T X) ^{-1}\\
&amp;= \underbrace{(X^T X) ^{-1} X^T\sigma^2I_nX(X^T X) ^{-1}}_\text{Assume homoskedasticity}\\
&amp;= \sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\
&amp;= \sigma^2(X^T X) ^{-1}
\end{align*}\]</span></p>
<p>Note: Along the way, in writing <span class="math inline">\(\mathbb{E}(\hat \beta) = \beta\)</span>, we have implicitly assumed that <span class="math inline">\(\hat \beta\)</span> is an “unbiased” estimator of <span class="math inline">\(\beta\)</span>. This is not free. It depends on an assumption that the error term in the regression <span class="math inline">\(\epsilon\)</span> is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section.</p>
</div>
<div id="hypothesis-testing" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Hypothesis Testing</h3>
<p>Most of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable.</p>
<p>In OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is:</p>
<ul>
<li>Null hypothesis: <span class="math inline">\(\beta_k = 0\)</span>
<ul>
<li>This essentially means that we don’t expect a particular <span class="math inline">\(x_k\)</span> independent variable to have a relationship with our outcome variable.</li>
</ul></li>
<li>Alternative hypothesis: <span class="math inline">\(\beta_k \neq 0\)</span>
<ul>
<li>We do expect a positive or negative relationship between a particular <span class="math inline">\(x_k\)</span> and the dependent variable.</li>
</ul></li>
</ul>
<p>We can use our estimates for <span class="math inline">\(\hat \beta\)</span> coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test.</p>
<p>In a t-test, we take our coefficient estimates and divide them by the standard error in order to “standardize” them on a scale that we can use to determine how likely it is we would have observed a value for <span class="math inline">\(\hat \beta\)</span> as extreme or more extreme as the one we observed in a world where the true <span class="math inline">\(\beta = 0\)</span>. This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is <span class="math inline">\(\hat \beta\)</span>.</p>
<p><span class="math display">\[\begin{align*}
t_{\hat \beta_k} &amp;= \frac{\hat \beta_k}{s.e.(\hat \beta_k)}
\end{align*}\]</span></p>
<p>Generally speaking, when <span class="math inline">\(t\)</span> is about +/-2 or greater in magnitude, the coefficient will be “significant” at conventional levels (i.e., <span class="math inline">\(p &lt;0.05\)</span>), meaning that we are saying that it is really unlikely we would have observed a value as big as <span class="math inline">\(\hat \beta_k\)</span> if the null hypothesis were true. Therefore, we can reject the null hypothesis.</p>
<p>However, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are <span class="math inline">\(N-k\)</span>, the number of observations in the model minus the number of independent variables plus the intercept.</p>
<p>In R, we can calculate p-values using the <code>pt()</code> function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="uncertainty-and-regression.html#cb196-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Let&#39;s say t was 2.05 and </span></span>
<span id="cb196-2"><a href="uncertainty-and-regression.html#cb196-2" aria-hidden="true" tabindex="-1"></a><span class="do">## And there were 32 observations and 3 variables in the regression plus an intercept</span></span>
<span id="cb196-3"><a href="uncertainty-and-regression.html#cb196-3" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fl">2.05</span></span>
<span id="cb196-4"><a href="uncertainty-and-regression.html#cb196-4" aria-hidden="true" tabindex="-1"></a>df.t <span class="ot">&lt;-</span> <span class="dv">32</span> <span class="sc">-</span><span class="dv">4</span></span>
<span id="cb196-5"><a href="uncertainty-and-regression.html#cb196-5" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="fu">pt</span>(<span class="fu">abs</span>(t), <span class="at">df=</span>df.t, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span>
<span id="cb196-6"><a href="uncertainty-and-regression.html#cb196-6" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code>[1] 0.04983394</code></pre>
<p>Let’s do this for the florida example. First, we can find t by dividing our coefficients by the standard errors.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="uncertainty-and-regression.html#cb198-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit<span class="fl">.1</span>) <span class="sc">/</span> (<span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(fit<span class="fl">.1</span>))))</span>
<span id="cb198-2"><a href="uncertainty-and-regression.html#cb198-2" aria-hidden="true" tabindex="-1"></a>t </span></code></pre></div>
<pre><code>(Intercept)     Perot96 
 0.02704523  8.27522567 </code></pre>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="uncertainty-and-regression.html#cb200-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Compare with output</span></span>
<span id="cb200-2"><a href="uncertainty-and-regression.html#cb200-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit<span class="fl">.1</span>)<span class="sc">$</span>coefficients[, <span class="dv">3</span>]</span></code></pre></div>
<pre><code>(Intercept)     Perot96 
 0.02704523  8.27522567 </code></pre>
<p>We can then find the p-values.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="uncertainty-and-regression.html#cb202-1" aria-hidden="true" tabindex="-1"></a>t <span class="ot">&lt;-</span> <span class="fu">coef</span>(fit<span class="fl">.1</span>) <span class="sc">/</span> (<span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(fit<span class="fl">.1</span>))))</span>
<span id="cb202-2"><a href="uncertainty-and-regression.html#cb202-2" aria-hidden="true" tabindex="-1"></a>df.t <span class="ot">&lt;-</span> fit<span class="fl">.1</span><span class="sc">$</span>df.residual</span>
<span id="cb202-3"><a href="uncertainty-and-regression.html#cb202-3" aria-hidden="true" tabindex="-1"></a>p.value <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (<span class="fu">pt</span>(<span class="fu">abs</span>(t), <span class="at">df=</span>df.t, <span class="at">lower.tail =</span> <span class="cn">FALSE</span>))</span>
<span id="cb202-4"><a href="uncertainty-and-regression.html#cb202-4" aria-hidden="true" tabindex="-1"></a>p.value</span></code></pre></div>
<pre><code> (Intercept)      Perot96 
9.785065e-01 9.473505e-12 </code></pre>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="uncertainty-and-regression.html#cb204-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit<span class="fl">.1</span>)<span class="sc">$</span>coefficients[, <span class="dv">4</span>]</span></code></pre></div>
<pre><code> (Intercept)      Perot96 
9.785065e-01 9.473505e-12 </code></pre>
<p>We see that the coefficient for <code>Perot96</code> is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is <span class="math inline">\(p &lt; 0.001\)</span>. R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it.</p>
<p><em>Confidence Intervals</em></p>
<p>Instead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant).</p>
<p>The precise formula is</p>
<p><span class="math inline">\(\widehat{\beta}_k\)</span> Confidence intervals: <span class="math inline">\(\widehat{\beta}_k - t_{crit.value} \times s.e._{\widehat{\beta}_k}, \widehat{\beta}_k + t_{crit.value} \times s.e_{\widehat{\beta}_k}\)</span></p>
<p>In R, we can use <code>qt()</code> to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut <code>confint()</code> function to extract the coefficients from the model. Below we do this for the <code>Perot96</code> coefficient.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="uncertainty-and-regression.html#cb206-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Critical values from t distribution at .95 level</span></span>
<span id="cb206-2"><a href="uncertainty-and-regression.html#cb206-2" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> fit<span class="fl">.1</span><span class="sc">$</span>df.residual) <span class="co"># n- k degrees of freedom</span></span></code></pre></div>
<pre><code>[1] 1.997138</code></pre>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="uncertainty-and-regression.html#cb208-1" aria-hidden="true" tabindex="-1"></a><span class="do">## Shortcut</span></span>
<span id="cb208-2"><a href="uncertainty-and-regression.html#cb208-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(fit<span class="fl">.1</span>)[<span class="dv">2</span>,]</span></code></pre></div>
<pre><code>     2.5 %     97.5 % 
0.02724733 0.04458275 </code></pre>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="uncertainty-and-regression.html#cb210-1" aria-hidden="true" tabindex="-1"></a><span class="do">## By hand</span></span>
<span id="cb210-2"><a href="uncertainty-and-regression.html#cb210-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit<span class="fl">.1</span>)[<span class="dv">2</span>] <span class="sc">-</span> <span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> fit<span class="fl">.1</span><span class="sc">$</span>df.residual)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(fit<span class="fl">.1</span>)))[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>   Perot96 
0.02724733 </code></pre>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="uncertainty-and-regression.html#cb212-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit<span class="fl">.1</span>)[<span class="dv">2</span>] <span class="sc">+</span> <span class="fu">qt</span>(.<span class="dv">975</span>, <span class="at">df =</span> fit<span class="fl">.1</span><span class="sc">$</span>df.residual)<span class="sc">*</span><span class="fu">sqrt</span>(<span class="fu">diag</span>(<span class="fu">vcov</span>(fit<span class="fl">.1</span>)))[<span class="dv">2</span>]</span></code></pre></div>
<pre><code>   Perot96 
0.04458275 </code></pre>
</div>
<div id="goodness-of-fit" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Goodness of Fit</h3>
<p>A last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood.</p>
<p>These are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth.</p>
<p>For the first two measures R-squared and Adjusted R-squared, we draw on three quantities:</p>
<ul>
<li><p>Total Sum of Squares–how much variance in <span class="math inline">\(Y_i\)</span> is there to explain?</p>
<ul>
<li><span class="math inline">\(TSS: \sum_{i=1}^N (Y_i -\overline Y_i)^2\)</span></li>
</ul></li>
<li><p>Estimated Sum of Squares–how much of this variance do we explain?</p>
<ul>
<li><span class="math inline">\(ESS: \sum_{i=1}^N (\widehat Y_i -\overline Y_i)^2\)</span></li>
</ul></li>
<li><p>Residual Sum of Squares–how much variance is unexplained?</p>
<ul>
<li><span class="math inline">\(RSS: \sum_{i=1}^N ( Y_i -\widehat Y_i)^2\)</span></li>
</ul></li>
<li><p><span class="math inline">\(TSS = ESS + RSS\)</span></p></li>
<li><p>Multiple R-squared: <span class="math inline">\(\frac{ESS}{TSS}\)</span></p>
<ul>
<li>This is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value.</li>
</ul></li>
<li><p>Adjusted R-squared: <span class="math inline">\(1 - \frac{\frac{RSS}{n - k}}{\frac{TSS}{n - 1}}\)</span></p>
<ul>
<li>This is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors.</li>
</ul></li>
<li><p>Residual standard error aka root mean squared error aka square root of the mean squared residual: <span class="math inline">\(r.s.e = \sqrt{\frac{RSS}{n-k}}\)</span></p>
<ul>
<li>This represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome.</li>
</ul></li>
</ul>
<p><em>F-Statistic</em></p>
<p>So far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output.</p>
<p>The F-statistic helps us test the null hypothesis that <strong>all</strong> of the regression slopes are 0: <span class="math inline">\(H_0 = \beta_1 = \beta_2 = \dots = \beta_k = 0\)</span></p>
<ul>
<li><span class="math inline">\(F_0 = \frac{ESS/(k - 1)}{RSS/(n - k)}\)</span></li>
<li>The F-Statistic has two separate degrees of freedom.
<ul>
<li>The model sum of squares degrees of freedom (ESS) are <span class="math inline">\(k - 1\)</span>.</li>
<li>The residual error degrees of freedom (RSS) are <span class="math inline">\(n - k\)</span>.</li>
<li>In a regression output, the model degrees of freedom are generally the first presented: “F-statistic: 3.595 on <span class="math inline">\((k - 1) = 1\)</span> and <span class="math inline">\((n - k) = 48\)</span> DF.”</li>
</ul></li>
</ul>
<p>Note: This test is different from our separate hypothesis tests that a <span class="math inline">\(k\)</span> regression slope is 0. For that, we use the t-tests discussed above.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="week-1-thursday-tutorial.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generating-predictions-from-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
