[
["index.html", "MLE for Political Science Section 1 Course Overview", " MLE for Political Science Instructor: Katie McCabe Section 1 Course Overview This document will include important links and course notes for 16:790:677:01: Maximum Likelihood Estimation. This webpage will be updated throughout the semester with new content. This initial content in Sections 2 and 3 is optional, but encouraged, and includes tutorials and resources for basics in R and mathematical concepts used in the course. Material introduced in the course will assume some familiarity with these concepts. I recommend you go through on your own leading up to and during the first week of the course. You can then always return to it throughout the course and beyond. I always have to return to it because I don’t often keep the special rules of logarithms and derivatives in my working memory. Sprinkled throughout this document are links to additional resources that might provide more in-depth explanations of a given topic. In particular, several of the sections have benefited from previous materials developed by Kosuke Imai and the book Quantitative Social Science, Chris Bail and SICSS, Marc Ratkovic, In Song Kim, Will Lowe, and others. This is a new and living document. If you spot errors or have questions or suggestions, please email me at k.mccabe@rutgers.edu. "],
["rover.html", "Section 2 R Overview", " Section 2 R Overview This course will primarily use R for analysis, though we will briefly discuss a few areas where Stata may be more efficient. Learning to program in R is not a primary goal of this course, but in proceeding through the course, you will gain and/or get practice with a lot of R skills. For those brand new to R, I strongly recommend you complete the following tutorials prior to or at the beginning of the course. Goal By the end of the first week of the course, you will want to have R and RStudio installed on your computer (both free) and feel comfortable using R as a calculator and loading datasets into R. R and RStudio Installation This video from Christopher Bail explains the R and RStudio installation process. This involves Going to cran, select the link that matches your operating system, and then follow the installation instructions, and Visiting RStudio and follow the download and installation instructions. R is the statistical software and programming language used for analysis. RStudio provides a convenient user interface for running R code. You do not need RStudio to use R, but it is free and can make your life easier! After installing R and RStudio, you can also follow along with Christopher Bail’s R Basics and Data Wrangling videos to learn the basic functionality of R. Supplemental Resources To supplement the above resources, I would recommend playing around with one of the following: An additional great resource is Kosuke Imai’s book Quantitative Social Science. The first chapter provides a written overview of installing R and the basic functions of R and R Studio, including loading data and packages into R. Data for the book is available at the bottom of this page. Alternate coding in tidyverse for the book is available here. If you were having difficulties following Chris Bail or Kosuke Imai’s instructions for installation, you can try following the first couple of Rutgers Data Librarian Ryan Womack’s videos, which similarly start from the point of installation. They are here. He goes at a slow pace and codes along in the videos. He also has a number of videos on more advanced data analysis topics. R for Data Science is another great resource and focuses on “tidyverse” code in R, which can be particularly helpful in data wrangling and visualization. Note: Much of the code used in the course will rely on “base R” functions (functions that already exist in R). People have also developed tidyverse packages that can be easily installed in R, which supplements base R tools with alternative functions and a syntax based on a particular design philosophy, grammar, and data structure that they find preferable to base R. Using base R vs. tidyverse is often just a matter of personal taste. Either is fine to use in this course, and you will get exposure to code that relies on both. This is a lot of information to digest all at once. Don’t worry. No one remembers everything. Plan on going back to these resources often throughout the course and beyond. We will have office hours the first week of the course to help troubleshoot issues. "],
["first-time-with-r-and-rstudio.html", "2.1 First Time with R and RStudio", " 2.1 First Time with R and RStudio This next section provides a few notes on using R and RStudio now that you have installed it. This is mostly repetitive of the other resources. This includes only the bare essential information for opening an R script and digging into using R as a calculator, which we will do in chapter 3. In this section, we cover the following materials: Using R as a calculator and assigning objects using &lt;- Setting your working directory and the setwd() function. Creating and saving an R script 2.1.1 Open RStudio RStudio is an open-source and free program that greatly facilitates the use of R, especially for users new to programming. Once you have downloaded and installed R and RStudio, to work in R, all you need to do now is open RStudio (it will open R). It should look like this, though your version number may be different: (Image from Kosuke Imai’s Quantitative Social Science Figure 1.1) Note: If you only have three windows (e.g., no upper-left window), you may need to open an R script. To do this, in RStudio, click on File, New, and then R script. This will open a blank document for text editing in the upper left of the RStudio window. We will return to this window in a moment. 2.1.2 Using R as a Calculator The bottom left window in your RStudio is the Console. You can type in this window to use R as a calculator or to try out commands. It will show the raw output of any commands you type. For example, we can try to use R as a calculator. Type the following in the Console (the bottom left window) and hit “enter” or “return” on your keyboard: 5 + 3 ## [1] 8 5 - 3 ## [1] 2 5^2 ## [1] 25 5 * 3 ## [1] 15 5/3 ## [1] 1.666667 (5 + 3) * 2 ## [1] 16 In the other RStudio windows, the upper right will show a history of commands that you have sent from the text editor to the R console, along with other items. The lower right will show graphs, help documents and other features. These will be useful later in the course. 2.1.3 Working in an R Script Earlier, I asked you to open an R script in the upper left window by doing File, then New File, then R Script. Let’s go back to working in that window. Set your working directory setwd() (Almost) every time you work in RStudio, the first thing you will do is set your working directory. This is a designated folder in your computer where you will save your R scripts and datasets. There are many ways to do this. An easy way is to go to Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory. I suggest choosing a folder in your computer that you can easily find and that you will routinely use for this class. Go ahead and create/select it. Note: when you selected your directory, code came out in the bottom left Console window. This is the setwd() command which can also be used directly to set your working directory in the future. If you aren’t sure where your directory has been set, you can also type getwd() in your Console. Try it now ## Example of where my directory was getwd() If I want to change the working directory, I can go to the top toolbar of my computer and use Session \\(\\rightarrow\\) Set Working Directory \\(\\rightarrow\\) Choose Directory or just type my file pathway using the setwd() below: ## Example of setting the working directory using setwd(). ## Your computer will have your own file path. setwd(&quot;/Users/ktmccabe/Dropbox/Rutgers Teaching/&quot;) Saving the R Script Let’s now save our R script to our working directory and give it an informative name. To do so, go to File, then Save As, make sure you are in the same folder on your computer as the folder you chose for your working directory. Give the file an informative name, such as: “McCabeWeek1.R”. Note: all of your R scripts will have the .R extension. 2.1.4 Preparing your R script Now that we have saved our R script, let’s work inside of it. Remember, we are in the top-left RStudio window now. Just like the beginning of a paper, you will want to title your R script. In R, any line that you start with a # will not be treated as a programming command. You can use this to your advantage to write titles/comments. Below is a screenshot example of a template R script. You can specify your working directory at the top, too. Add your own filepath inside setwd() Then you can start answering problems in the rest of the script. Think of the R script as where you write the final draft of your paper. In the Console (the bottom-left window), you can mess around and try different things, like you might when you are taking notes or outlining an essay. Then, write the final programming steps that lead you to your answer in the R script. For example, if I wanted to add 5 + 3, I might try different ways of typing it in the Console, and then when I found out 5 + 3 is the right approach, I would type that into my script. 2.1.5 Executing Commands in your R script The last thing we will note in this initial handout is how to execute commands in your R script. To run / execute a command in your R script (the upper left window), you can Highlight the code you want to run, and then hold down “command + return” on a Mac or “control + enter” on Windows Place your cursor at the end of the line of code (far right), and hit “command + return” on a Mac or “control + return” on Windows, or Do 1 or 2, but instead of using the keyboard to execute the commands, click “Run” in the top right corner of the upper-left window. Try it: Type 5 + 3 in the R script. Then, try to execute 5 + 3. It should look something like this: After you executed the code, you should see it pop out in your Console: 5 + 3 ## [1] 8 Note: The symbol # also allows for annotation behind commands or on a separate line. Everything that follows # will be ignored by R. You can annotate your own code so that you and others can understand what each part of the code is designed to do. ## Example sum53 &lt;- 5 + 3 # example of assigning an addition calculation 2.1.6 Objects Sometimes we will want to store our calculations as “objects” in R. We use &lt;- to assign objects by placing it to the left of what we want to store. For example, let’s store the calculation 5 + 3 as an object named sum53: sum53 &lt;- 5 + 3 After we execute this code, sum53 now stores the calculation. This means, that if we execute a line of code that just hassum53`, it will output 8. Try it: sum53 ## [1] 8 Now we no longer have to type 5 + 3, we can just type sum53. For example, let’s say we wanted to subtract 2 from this calculation. We could do: sum53 - 2 ## [1] 6 Let’s say we wanted to divide two stored calculations: ten &lt;- 5 + 5 two &lt;- 1 + 1 ten / two ## [1] 5 The information stored does not have to be numeric. For example, it can be a word, or what we would call a character string, in which case you need to use quotation marks. mccabe &lt;- &quot;professor for this course&quot; mccabe ## [1] &quot;professor for this course&quot; Note: Object names cannot begin with numbers and no spacing is allowed. Avoid using special characters such as % and $, which have specific meanings in R. Finally, use concise and intuitive object names.} GOOD CODE: practice.calc &lt;- 5 + 3 BAD CODE: meaningless.and.unnecessarily.long.name &lt;- 5 + 3 While these are simple examples, we will use objects all the time for more complicated things to store (e.g., like full datasets!) throughout the course. We can also store an array or “vector” of information using c() somenumbers &lt;- c(3, 6, 8, 9) somenumbers ## [1] 3 6 8 9 Importance of Clean Code Ideally, when you are done with your R script, you should be able to highlight the entire script and execute it without generating any error messages. This means your code is clean. Code with typos in it may generate a red error message in the Console upon execution. This can happen when there are typos or commands are misused. For example, R is case sensitive. Let’s say we assigned our object like before: sum53 &lt;- 5 + 3 However, when we went to execute sum53, we accidentally typed Sum53: Sum53 ## Error in eval(expr, envir, enclos): object &#39;Sum53&#39; not found Only certain types of objects can be used in mathematical calculations. Let’s say we tried to divide mccabe by 2: mccabe / 2 ## Error in mccabe/2: non-numeric argument to binary operator A big part of learning to use R will be learning how to troubleshoot and detect typos in your code that generate error messages. 2.1.7 Practice Below is an exercise that will demonstrate you are able to use R as a calculator and create R scripts. Create an R script saved as ``LastnameSetup1.R\" (use your last name). Within the R script, follow the example from this handout and title the script. Set your working directory, and include the file pathway (within setwd()) at the top of your .R script. Do the calculation 4 + 3 - 2 in R. Store it as an object with an informative name. Do the calculation 5 \\(\\times\\) 4 in R. Store it as an object with an informative name. Add these two calculations together. In R, try to do this by adding together the objects you created, not the underlying raw calculations. "],
["tutorials.html", "2.2 Tutorials", " 2.2 Tutorials Occasionally in this course, there will be some interactive tutorials I’ve created that you can do to practice the concepts and R skills. In order to be able to run the tutorials, you need to run the following in your R script. install.packages(\"devtools\", dependencies=T) After devtools is installed, you shouldn’t have to run that line of code again. However, you will need to run the below line each time there is a new tutorial, as this package will be updated throughout the course. devtools::install_github(\"ktmccabe/interactivepack\", dependencies = T) Our first tutorial is practicing R skills to load and manipulate dataframes. You should be able to complete that after reviewing the Chris Bail video and Kosuke Imai QSS Chapter 1 resource. The tutorial is based on information provided in Imai Chapter 1. To load the tutorial, run the line of code below in your RStudio Console. It should open up a web browser on your local machine with the tutorial. If a browser does not open, you may need to manually copy/paste the numeric url that pops out on the console into a browser window. To close out of a tutorial, you can click the stop sign or hit the “Escape” key in the console. If you encounter an error, you may need to install a package or application that is indicated in the error message. learnr::run_tutorial(\"rintro\", package=\"interactivepack\") This is a new program that I am using in this course for the first time to create tutorials, so feel free to email if you encounter bugs!! "],
["data-wrangling.html", "2.3 Data Wrangling", " 2.3 Data Wrangling So you have some data…. AND it’s a mess!!! A lot of the data we may encounter in courses has been simplified to allow students to focus on other concepts. We may have data that look like the following: nicedata &lt;- data.frame(gender = c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;), age = c(16, 20, 66, 44), voterturnout = c(1, 0, 1, 0)) gender age voterturnout Male 16 1 Female 20 0 Female 66 1 Male 44 0 In the real world, our data may hit us like a ton of bricks, like the below: uglydata &lt;- data.frame(VV160002 = c(2, NA, 1, 2), VV1400068 = c(16, 20, 66, 44), VV20000 = c(1, NA, 1, NA)) VV160002 VV1400068 VV20000 2 16 1 NA 20 NA 1 66 1 2 44 NA A lot of common datasets we use in the social sciences are messy, uninformative, sprawling, misshaped, and/or incomplete. What do I mean by this? The data might have a lot of missing values. For example, we may have NA values in R, or perhaps a research firm has used some other notation for missing data, such as a 99. The variable names may be uninformative. For example, there may be no way to know by looking at the data, which variable represents gender. We have to look at a codebook. Even if we can tell what a variable is, its categories may not be coded in a way that aligns with how we want to use the data for our research question. For example, perhaps you are interested in the effect of a policy on people below vs. 65 and over in age. Well, your age variables might just be a numeric variable. You will have to create a new variable that aligns with your theoretical interest. Datasets are often sprawling. Some datasets may have more than 1000 variables. It is hard to sort through all of them. Likewise, datasets may have millions of observations. We cannot practically look through all the values of a column to know what is there. Sometimes we have data shaped into separate columns when we’d rather it be reshaped into different rows. Maybe you have encountered a beautiful dataset that provides many measures of your independent variables of interest, but there’s one catch– it has no variable related to your outcome! You have to merge data from multiple sources to answer your research question. Below are a few tips and resources. Ultimately, research is a constant debugging process. The nice thing about R is that a lot of researchers constantly post coding tips and questions online. Google ends up being your friend, but it’s entirely normal to have to devote several hours (days?) to cleaning data. 2.3.1 Dealing with Uninformative Variable Names Hopefully, there is an easy fix for dealing with uninformative variable names. I say “hopefully” because hopefully when you encounter a dataset with uninformative variable names, the place where you downloaded the data will also include a codebook telling you what each variable name means, and how the corresponding values are coded. Unfortunately, this may not always be the case. One thing you can do as a researcher is when you create a dataset for your own work, keep a record (in written form, on a word document or in a pdf or code file) of what each variable means (e.g., the survey question it corresponds to or the exact economic measure), as well as how the values of the variables are coded. This good practice will help you in the short-term, as you pause and come back to working on a project over the course of a year, as well as benefit other researchers in the long run after you publish your research. For examples of large codebooks, you can view the 2016 American National Election Study Survey and click on a codebook. I recommend that once you locate the definition of a variable of interest, rename the variable in your dataset to be informative. You can do this by creating a new variable or overwriting the name of the existing variable. You might also comment a note for yourself of what the values mean. ## Option 1: create new variable ## gender 2=Male, 1=Female uglydata$gender &lt;- uglydata$VV160002 ## Option 2: Overwrite names(uglydata)[1] &lt;- &quot;gender2&quot; 2.3.2 Dealing with Missing Data When we have a column with missing data, it is best to do a few things: Try to quantify how much missing data there is and poke at the reason why data are missing. Is it minor non-response data? Or is it indicative of a more systematic issue? For example, maybe data from a whole group of people or countries is missing for certain variables. If the data are missing at a very minor rate and/or there is a logical explanation for the missing data that should not affect your research question, you may choose to “ignore” the missing data when performing common analyses, such as taking the mean or running a regression. If missing data are a bigger problem, you may consider alternative solutions, such as “imputing” missing data or similarly using some type of auxilliary information to help fill in the missing values. If we want to figure out how much missing data we have in a variable, we have a couple of approaches: ## Summarize this variable summary(uglydata$gender) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.000 1.500 2.000 1.667 2.000 2.000 1 ## What is the length of the subset of the variable where the data are missing length(uglydata$gender[is.na(uglydata$gender) == T]) ## [1] 1 If we choose to ignore missing data, this can often be easily accomplished in common operations. For example, when taking the mean we just add an argument na.rm = T: mean(uglydata$VV1400068, na.rm=T) ## [1] 36.5 If we do a regression using lm or glm, R will automatically “listwise” delete any observation that has missing data (NA) on any of the variables in our regression model. We should always be careful with missing data to understand how R is treating it in a particular scenario. For example if we were to run table(uglydata$gender), we would have no idea there were missing data unless we knew that the total number of observations nrow(uglydata) was greater than 3. The table() command is omitting the missing values by default. table(gender= uglydata$gender) ## gender ## 1 2 ## 1 2 2.3.3 Dealing with Variable Codings that Aren’t Quite Right Often times the ways that variables are coded in datasets we get off-the-shelf are not coded exactly as how we were dreaming up operationalizing our concepts. Instead, we are going to have to wrangle the data to get them into shape. This may involve creating new variables that recode certain values, creating new variables that collapse some values into a smaller number of categories, combining multiple variables into a single variable (e.g., representing the average), or setting some of the variable values to be missing (NA). All of these scenarios may come up when you are dealing with real data. Chapter 2 of Kosuke Imai’s book Quantitative Social Science walks through some examples of how to summarize your data, subset the data (2.2.3), create new variables using conditional statements (Section 2.2.4, e.g., “If age is below 65, assign the new variable a value of”0“, otherwise, assign it a value of”1“), and creating new factor variables (2.2.5, e.g., coding anyone who is Protestant, Catholic, or Lutheran in the data as”Christian\"). Here is a short video working through the example from 2.2.4 using conditional statements to construct new variables. It uses the resume dataframe, which can be loaded below. resume &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) R Studio has its own set of primers on various topics, including summarizing and working with data. See the Work with Data primer, as well as the full list of other topics. These will often rely on tidyverse coding. 2.3.4 Dealing with Incomplete Data (Merging!) Sometimes in order to answer our research questions, we need to combine data from multiple sources. If we have a large amount of data, this may be quite daunting. Fortunately, R has several commands that allow us to merge or append datasets. Here is a video working through examples of merging and appending data based on the tutorial below. Here are a few resources on merging and appending data: Using the merge command in R. See Explanation. It will look for variable(s) held in common between datasets and join the datasets by the matching values on these variables. Appending data in R (e.g., Maybe you have one dataset from 2010 and another from 2012, and you want to stack them on top of each other). See Explanation. Some merging problems are extremely difficult. For example, some researchers need to merge large datasets–like the voter file– with other administrative records. However, how someone’s name is displayed in one dataset might not match at all with the other dataset. For these complex problems, we might need “fuzzy matching.” Here is an R package that helps with this more complex case and related paper. 2.3.5 Dealing with Poorly Shaped Data Data can come in a variety of shapes and sizes. It’s a beautiful disaster. Sometimes it’s particularly useful to have data in wide formats, where every row relates to a particular unit of data– such as a country or a survey respondent. And perhaps each column represents information about that unit at a particular point in time. For example, perhaps you have a column with information on that subject for the past five years. countrywide &lt;- data.frame(country = c(&quot;Canada&quot;, &quot;USA&quot;), economy2016 = c(10, 12), economy2017 = c(11, 11), economy2018 = c(9, 5), economy2019 = c(13, 8), economy2020 = c(12, 6)) country economy2016 economy2017 economy2018 economy2019 economy2020 Canada 10 11 9 13 12 USA 12 11 5 8 6 However, other times, it would be more useful to you, as you dig into your data analysis, to have this information arranged in “long” format, such that every row is now a unit-year combination. You have a row for Canada in 2020, a row for Canada in 2019, and so on. Countries are now represented in multiple rows of your data. countrylong &lt;- data.frame(country = rep(c(&quot;Canada&quot;, &quot;USA&quot;),5), year = 2016:2020, economy= c(10, 12,11, 11,9, 5,13, 8,12, 6)) country year economy Canada 2016 10 USA 2017 12 Canada 2018 11 USA 2019 11 Canada 2020 9 USA 2016 5 Canada 2017 13 USA 2018 8 Canada 2019 12 USA 2020 6 Ultimately, different shapes of data are advantageous for different research questions. This means it is best if we have a way to (at least somewhat) easily shift between the two formats. Here is a resource on how to “reshape” your data between wide and long from UCLA. Here are a few additional resources: More on reshape – For the tidyverse fans. Using gather() and spread() in tidyverse from R for Data Science and explained by Chris Bail here. 2.3.6 Subsetting data by rows and columns Sometimes we do not want to deal with our entire dataset for an analysis. Instead, we might want to only analyze certain rows (e.g., maybe if we are just studying Democrats, for example). Similarly, we might have a dataframe with 1000 columns, from which we are only using about 20. We might want to remove those extra columns to make it easier to work with our dataframes. Below are a few examples of subsetting data and selecting columns. We will use the resume dataset from the Kosuke Imai QSS book for demonstration. This is a dataset from an experiment describing whether certain applicants, who varied in the gender (sex) and race (race) signaled by their name (firstname), received callbacks (call) for their employment applications. Here is a short video working through these examples. Let’s load the data. resume &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) Subset particular rows To do this, put the row numbers you want to keep on the left side of the comma. Putting nothing on the right side means you want to keep all columns. ## numerically resume[1,] # first row ## firstname sex race call ## 1 Allison female white 0 resume[1:4,] # first through 4th rows ## firstname sex race call ## 1 Allison female white 0 ## 2 Kristen female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 resume[c(1, 3, 4),] # 1, 3, 4 rows ## firstname sex race call ## 1 Allison female white 0 ## 3 Lakisha female black 0 ## 4 Latonya female black 0 Using the subset command with logical expressions &gt; &gt;= == &lt; &lt;= != ## by logical expressions women &lt;- subset(resume, sex == &quot;female&quot;) women &lt;- resume[resume$sex == &quot;female&quot;, ] ## alternative calledback &lt;- subset(resume, call == 1) calledback &lt;- subset(resume, call &gt; 0) And or Or statements &amp; or | blackwomen &lt;- subset(resume, sex == &quot;female&quot; &amp; race == &quot;black&quot;) bradbrendan &lt;- subset(resume, firstname == &quot;Brad&quot; | firstname == &quot;Brendan&quot;) The tidyverse also has commands for subsetting. Here is an example using filter. library(tidyverse) blackwomen &lt;- resume %&gt;% filter(sex == &quot;female&quot; &amp; race == &quot;black&quot;) Selecting particular columns Note, now we are working on the right side of the comma. ## numerically first &lt;- resume[, 1] # first column firstsecond &lt;- resume[, 1:2] # first and second column notfourth &lt;- resume[, -4] # all but the fourth column ## by labels justthese &lt;- resume[, c(&quot;firstname&quot;, &quot;sex&quot;)] Using the select command ## install.packages(&quot;dplyr&quot;) library(dplyr) subdata &lt;- resume %&gt;% dplyr::select(firstname, sex) ## just these subdata2 &lt;- resume %&gt;% dplyr::select(-firstname, -sex) ## all but these two 2.3.7 Visualizing Data There are many commands for plotting your data in R. The most common functions in base R are plot(), barplot() and hist(). You will see many examples throughout the notes with each of these functions. To get you started, the most simple thing to note about the plot() command is that it is based on a coordinate system. You specify the x and y coordinates for which you want to plot a series of points. For example, here is a plot at points 1,40; 3,50; and 4,60. plot(x = c(1,3,4), y=c(40, 50, 60)) Instead of putting raw numbers as the coordinates, you can provide object names. E.g., xcoord &lt;- c(1,3,4) ycoord &lt;- c(40, 50, 60) plot(x = xcoord, y=ycoord) Beyond that, you can play around with many aesthetics in R, such as the type, pch, lty, as well as labels main, ylab, xlab, font sizes cex.main, cex.axis, cex.lab, and axis limits ylim, xlim. Below is an example. Play around with changing some of the specifications, and see how the plot changes. xcoord &lt;- c(1,3,4) ycoord &lt;- c(40, 50, 60) plot(x = xcoord, y=ycoord, main = &quot;Example plot&quot;, ylab= &quot;Example y axis&quot;, xlab = &quot;Example x axis&quot;, cex.main = .8, ylim = c(0, 80), xlim = c(1,4), pch = 15, col=&quot;red&quot;, type = &quot;b&quot;, lty=2) The function barplot takes a single vector of values. This can be a raw vector you have created or a table object or tapply object, for example, displaying the counts of different observations or means. You can add a names.arg argument to specify particular names for each bar. Many of the other aesthetics are the same as plot. You can play around with adding aesthetics. Example: barplot(ycoord, names.arg = c(&quot;First&quot;, &quot;Second&quot;, &quot;Third&quot;), col=&quot;blue&quot;) For more on visualizing data, you can see the RStudio primers. R also has a package called ggplot2 which includes the function ggplot and many elaborate ways to plot your data. The gg stands for the grammar of graphics. For a video introduction to ggplot I recommend watching Ryan Womack’s video from 27:30 on. It uses the data diamonds which can be loaded in R through the following command. See approximately minute 35 for an example with a bar plot. library(ggplot2) data(diamonds) 2.3.8 Reproducing your steps It is really important to keep a record of all of the changes you have made to the original data. An R script or R markdown file is a useful way to do this, so long as you add comments that explain what you are doing. You want to get your code to a place when a stranger can open your R file, load your data, and reproduce each step you took to get to the final results— all while never even needing to contact you with questions. That can be difficult, but it’s good to aim for that high bar, even if sometimes, we fall short in how we are documenting each step. This website provides a nice introduction to R Markdown, one tool for embedding R code inside textual documents. See here. If you want to get advanced with reproducibility, you can watch Chris Bail’s on the subject describing the uses of R Markdown and GitHub among other tools for communicating and collaboration. He also links to other resources. "],
["tools-for-writing-up-results.html", "2.4 Tools for writing up results", " 2.4 Tools for writing up results 2.4.1 R Markdown R Markdown is a free tool within RStudio that allows you to weave together text and code (along with images and tables) into the same document. It can compile these documents (written inside R Studio) into html, pdf, or Word doc output files. R Markdown can be incredibly helpful for doing things like generating replication files, writing up problem sets, or even writing papers. This site includes an introduction to R Markdown. See also here and here R Markdown has its own syntax, which includes functionality for writing mathematical equations. The pdf output option in R Markdown requires LaTex, described in the next section. 2.4.2 LaTex LaTex is a typesetting program for drafting documents, much like Microsoft Word. Some advantages of using LaTex in writing empirical research papers is, once you learn the basics of the program, it can become easier to add tables, figures, and equations into a paper with little effort. The downside is that it has its own syntax, which takes a little time to learn. LaTex also has a feature called “beamer” which uses LaTex to generate slides. You can use this for presentations. LaTex “compiles” documents into pdf files. Here is one introduction for getting started with LaTex that starts from the installation stage. Here also is a link to a set of slides from Overleaf discussing the basics of LaTex: Slides You can download the Tex distribution for your operating system here. In addition to this, there are many programs available for running LaTex on your computer, which range from free, very basic tools (e.g., TexWorks) to tools with fancy capabilities. Overleaf is an online program for drafting LaTex documents. It has a nice feature where it allows you to share a document so that multiple people can work on the document simultaneously. This makes Overleaf a great program for projects where you have co-authors. The basic tools in Overleaf are available for free, but if you want to start sharing documents with a lot of co-authors, it requires a paid account. LaTex also has the ability to integrate citations into your documents. The part 2 tutorial from Overleaf goes over this. RStudio also has a program built-in called Sweave (.Rnw) documents that works with knitR, which weave together R code and LaTex syntax, allowing you to compile them into pdf documents and slide presentations. This is very similar to how R Markdown works, but with somewhat different syntax. See here for an overview. Your problem sets are generally Sweave/knitR documents. 2.4.3 Formatting and Exporting R Results R has a number of tools, including the packages texreg, xtable, and stargazer, which can be used to export tables made in R to nicely formatted LaTex or html output. Here is a link to the texreg package documentation. Section 5 has examples of the texreg and htmlreg functions within the texreg package. These can be integrated into R Markdown and Sweave documents, and their output can be pasted into LaTex or Microsoft Word. Your choice of function will depend on where you ultimately want your results to be compiled. If you are generating results that will be compiled to pdf using LaTex, then texreg works well. If you are exporting results to Word, than you may wish to use the htmlreg function within the texreg package, which will generate output that can be pasted into Word. A simple example using R Markdown html output. (Note, if you wanted to export the table to Word, you would add an argument specifying file = \"myfit.doc\" to the function. See the above link for examples: mydata &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/resume.csv&quot;) fit &lt;- lm(call ~ race, data=mydata) ## First time you use texreg, install it install.packages(&quot;texreg&quot;) library(texreg) htmlreg(list(fit), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Race&quot;) Regression of Call Backs on Race Model 1 (Intercept) 0.06*** (0.01) racewhite 0.03*** (0.01) R2 0.00 Adj. R2 0.00 Num. obs. 4870 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 You can add more arguments to the function to customize the name of the model and the coefficients. You can also add multiple models inside the list argument, for example, if you wanted to present a table with five regression models at once. Here is an example with two: fit2 &lt;- lm(call ~ race + sex, data=mydata) library(texreg) htmlreg(list(fit, fit2), stars=c(0.001, 0.01, 0.05), caption = &quot;Regression of Call Backs on Rac and Sex&quot;) Regression of Call Backs on Rac and Sex Model 1 Model 2 (Intercept) 0.06*** 0.07*** (0.01) (0.01) racewhite 0.03*** 0.03*** (0.01) (0.01) sexmale -0.01 (0.01) R2 0.00 0.00 Adj. R2 0.00 0.00 Num. obs. 4870 4870 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 "],
["math.html", "Section 3 The MATH", " Section 3 The MATH This section will provide an overview of a selection of mathematical skills that will be useful in the course. It will not go into everything, but at least provides a start. MLE will include a range of concepts and skills from linear algebra, calculus, and probability and statistics. You do not need to be an expert in these to succeed in the course, but there are some fundamentals that will make your life easier. Again, the end goal here is not the math. The end goal is to help your become great social scientists. Some popular methods in social science rely on these concepts. The more you can build an intuition for how, when, where, and why these methods work, the more tools you will have to carry out your research and the more credibility and command you will have over the research you conduct. Think about restaurants you have visited. There is a difference between a server who says, “Well we have a chicken” and a server who can tell you exactly how the chicken was cooked, what ingredients were used in the sauce, and maybe even how the chicken was raised. Both dishes may be delicious, but you may gain more trust and understanding from the second. This will not happen overnight nor by the end of the course, but the goal is to continue to progress in understanding. If you pursue a career in academia, this is only the start of years of continued learning and practice. It is a marathon, not a sprint. Extra Resources These are some additional resources that may help supplement the sections to follow. These provide far more detail than you will need in the course, but because of that, are much more comprehensive than the notes that will be outlined: A linear algebra web series. The notes on vector and matrices here will be brief, so this playlist 3Blue1Brown goes into more detail on specific. topics An alternative way of thinking about vectors and matrices is here which gives a more geometric interpretation, which some people find helpful. Will Moore and David Siegel. A Mathematics Course for Political and Social Research. You can get the book or visit the website, which includes some problem sets and solutions, as well as a video syllabus. "],
["mathematical-operations.html", "3.1 Mathematical Operations", " 3.1 Mathematical Operations In this first section, we will review mathematical operations that you have probably encountered before. In many cases, this will be a refresher on the rules and how to read notation. 3.1.1 Order of Operations Many of you may have learned the phrase, “Please Excuse My Dear Aunt Sally” which stands for Parentheses (and other grouping symbols), followed by Exponents, followed by Multiplication and Division from left to right, followed by Addition and Subtraction from left to right. There will be many equations in our future, and we must remember these rules. Example: \\(((1+2)^3)^2 = (3^3)^2 = 27^2 = 729\\) To get the answer, we focused on respecting the parentheses first, identifying the inner-most expression \\(1 + 2 = 3\\), we then moved out and conducted the exponents to get to \\((3^3)^2 = 27^2\\). Note how this is different from the answer to \\(1 + (2^3)^2 = 1 + 8^2 = 65\\), where the addition is no longer part of the parentheses. 3.1.2 Exponents Here is a cheat sheet of some basic rules for exponents. These can be hard to remember if you haven’t used them in a long time. Think of \\(a\\) in this case as a number, e.g., 4, and \\(b\\), \\(k\\), and \\(l\\), as other numbers. \\(a^0 = 1\\) \\(a^1 = a\\) \\(a^k * a^l = a^{k + l}\\) \\((a^k)^l = a^{kl}\\) \\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\) These last two rules can be somewhat tricky. Note that a negative exponent can be re-written as a fraction. Likewise an exponent that is a fraction, the most common of which we will encounter is \\(\\frac{1}{2}\\) can be re-written as a root, in this case the square root (e.g., \\(\\sqrt{a}\\)). \\(a^{-k} = \\frac{1}{a^k}\\) \\(a^{1/2} = \\sqrt{a}\\) 3.1.3 Summations and Products The symbol \\(\\sum\\) can be read “take the sum of” whatever is to the right of the symbol. This is used to make the written computation of a sum much shorter than it might be otherwise. For example, instead of writing the addition operations separately in the example below, we can simplify it with the \\(\\sum\\) symbol. This is especially helpful if you would need to add together 100 or 1000 or more things. We will see these appear a lot in the course, for better or worse, so getting comfortable with the notation will be useful. Usually, there is notation just below and just above the symbol (e.g., \\(\\sum_{i=1}^3\\)). This can be read as \"take the sum of the following from \\(i=1\\) to \\(i=3\\). We perform the operation in the expression, each time changing \\(i\\) to a different number, from 1 to 2 to 3. We then add each expression’s output together. Example: \\(\\sum_{i=1}^3 (i + 1)^2 = (1 + 1)^2 + (2 + 1)^2 + (3+1)^2 = 29\\) We will also encounter the product symbol in this course: \\(\\prod\\). This is similar to the summation symbol, but this time we are multiplying instead of adding. Example: \\(\\prod_{k = 1}^3 k^2 = 1^2 \\times 2^2 \\times 3^2 = 36\\) 3.1.4 Logarithms In this class, we will generally assume that \\(\\log\\) takes the natural base \\(e\\), which is a mathematical constant equal to 2.718…. In other books, the base might be 10 by default. If we have, \\(\\log_{10} x = 2\\), this is like saying 10^2 = 100. With base \\(e\\), we have \\(\\log_e x =2\\), which is \\(e^2 = 7.389\\). We are just going to write \\(\\log_e\\) as \\(\\log\\) but know that the \\(e\\) is there. A key part of maximum likelihood estimation is writing down the log of the likelihood equation, so this is a must-have for later in the course. Here is a cheat sheet of common rules for working with logarithms. \\(\\log x = 8 \\rightarrow e^8 = x\\) \\(e^\\pi = y \\rightarrow \\log y = \\pi\\) \\(\\log (a \\times b) = \\log a + \\log b\\) \\(\\log a^n = n \\log a\\) \\(\\log \\frac{a}{b} = \\log a - \\log b\\) Why logarithms? There are many different reasons why social scientists use logs. Some social phenomena grow exponentially, and logs make it is easier to visualize exponential growth, as is the case in visualizing the growth in COVID cases. See this example. Relatedly, taking the log of a distribution that is skewed, will make it look more normal or symmetrical, which has some nice properties. Sometimes the rules of logarithms are more convenient than non-logarithms. In MLE, we will take particular advantage of this rule: \\(\\log (a \\times b) = \\log a + \\log b\\), which turns a multiplication problem into an addition problem. "],
["mathematical-operations-in-r.html", "3.2 Mathematical Operations in R", " 3.2 Mathematical Operations in R We will use R for this course, and these operations are all available with R code, allowing R to become a calculator for you. Here are some examples applying the tools above. 3.2.1 PEMDAS ((1+2)^3)^2 [1] 729 1 + (2^3)^2 [1] 65 3.2.2 Exponents You can compare the computation below to match with the rules above. Note how the caret ^ symbol is used for exponents, and the asterisk * is used for multiplication. We also have a function sqrt() for taking the square root. ## Let&#39;s say a = 4 for our purposes a &lt;- 4 ## And let&#39;s say k= 3 and l=5, b=2 k &lt;- 3 l &lt;- 5 b &lt;- 2 \\(a^0 = 1\\) \\(a^1 = a\\) a^0 a^1 [1] 1 [1] 4 Note how we use parentheses in R to make it clear that the exponent includes not just k but (k + l) \\(a^k * a^l = a^{k + l}\\) a^k * a^l a^(k + l) [1] 65536 [1] 65536 Note how we use the asterisk to make it clear we want to multiply k*l \\((a^k)^l = a^{kl}\\) (a^k)^l a^(k*l) [1] 1073741824 [1] 1073741824 \\((\\frac{a}{b})^k = (\\frac{a^k}{b^k})\\) (a / b)^k (a ^k)/(b^k) [1] 8 [1] 8 \\(a^{-k} = \\frac{1}{a^k}\\) a^(-k) 1 / (a^k) [1] 0.015625 [1] 0.015625 \\(a^{1/2} = \\sqrt{a}\\) a^(1/2) sqrt(a) [1] 2 [1] 2 3.2.3 Summations Summations and products are a little more nuanced in R, depending on what you want to accomplish. But here is one example. Let’s take a vector (think a list of numbers) that goes from 1 to 4. We will call it ourlist. ourlist &lt;- c(1,2,3,4) ourlist ## An alternative is to write this: ourlist &lt;- 1:4 [1] 1 2 3 4 Now let’s do \\(\\sum_{i = 1}^4 (i + 1)^2 = (1 +1)^2 + (1+2)^2 + (1 + 3)^2 + (1 + 4)^2 = 54\\) In R, when you add a number to a vector, it will add that number to each entry in the vector. Example ourlist + 1 [1] 2 3 4 5 We can use that now to do the inside part of the summation. Note: the exponent works the same way, squaring each element of the inside expression. (ourlist + 1)^2 [1] 4 9 16 25 Now, we will embed this expression inside a function in R called sum standing for summation. It will add together each of the inside components. sum((ourlist + 1)^2) [1] 54 If instead we wanted the product, multiplying each element of the inside together, we could use prod(). We won’t use that function very much in this course. 3.2.4 Logarithms R also has functions related to logarithms, called log() and exp() for the for the natural base \\(e\\). By default, the natural exponential is the base in the log() R function. \\(\\log x = 8 \\rightarrow e^8 = x\\) exp(8) [1] 2980.958 log(2980.958) [1] 8 Note that R also has a number of built-in constants, like pi. \\(e^\\pi = y \\rightarrow \\log y = \\pi\\) exp(pi) [1] 23.14069 log(23.14069) [1] 3.141593 \\(\\log (a \\times b) = \\log a + \\log b\\) log(a * b) log(a) + log(b) [1] 2.079442 [1] 2.079442 Let’s treat \\(n=3\\) in this example and enter 3 directly where we see \\(n\\) below. Alternatively you could store 3 as n, as we did with the other letters above. \\(\\log a^n = n \\log a\\) log(a ^ 3) 3 * log(a) [1] 4.158883 [1] 4.158883 \\(\\log \\frac{a}{b} = \\log a - \\log b\\) log(a/b) log(a) - log(b) [1] 0.6931472 [1] 0.6931472 "],
["derivatives.html", "3.3 Derivatives", " 3.3 Derivatives We will need to take some derivatives in the course. The reason is because a derivative gets us closer to understanding how to minimize and maximize certain functions, where a function is a relationship that maps elements of a set of inputs into a set of outputs, where each input is related to one output. This is useful in social science, with methods such as linear regression and maximum likelihood estimation because it helps us estimate the values that we think will best describe the relationship between our independent variables and a dependent variable. For example, in ordinary least squares (linear regression), we choose coefficients, which describe the relationship between the independent and dependent variables (for every 1 unit change in x, we estimate \\(\\hat \\beta\\) amount of change in y), based on a method that tries to minimize the squared error between our estimated outcomes and the actual outcomes. In MLE, we will have a different quantity, which we will try to maximize. To understand derivatives, we will briefly define limits. Limits A limit describes how a function behaves as it approaches (gets very close to) a certain value \\(\\lim_{x \\rightarrow a} f(x) = L\\) Example: \\(\\lim_{x \\rightarrow 3} x^2 = 9\\) The limit of this function as \\(x\\) approaches three, is 9. Limits will appear in the expression for calculating derivatives. 3.3.1 Derivatives For intuition on a derivative, watch this video from The Math Sorcerer. A derivative is the instantaneous rate at which the function is changing at x: the slope of a function at a particular point. There are different notations for indicating something is a derivative. Below, we use \\(f&#39;(x)\\) because we write our functions as \\(f(x) = x\\). Many times you might see a function equation like \\(y = 3x\\) There, it will be common for the derivative to be written like \\(\\frac{dy}{dx}\\). Let’s break down the definition of a derivative by looking at its similarity to the simple definition of a slope, as the rise over the run: Slope (on average): rise over run: change in \\(f(x)\\) over an interval (\\([c, b]\\) where \\(b-c =h\\)): \\(\\frac{f(b) - f(c)}{b-c}\\) For slope at a specific point \\(x\\) (the derivative of f(x) at x), we just make the interval \\(h\\) very small: \\(f&#39;(x)= \\lim_{h \\rightarrow 0}\\frac{f(a + h) - f(a)}{h}\\) Example \\(f(x) = 2x + 3\\). \\(f&#39;(x) = \\lim_{h \\rightarrow 0}\\frac{2(x + h) + 3 - (2x + 3)}{h} = \\lim_{h \\rightarrow 0}\\frac{2x + 2h - 2x}{h} = \\lim_{h \\rightarrow 0}2 = 2\\) This twitter thread by the brilliant teacher and statistician Allison Horst, provides a nice cartoon-based example of the derivative. (Note that sometimes the interval \\(h\\) is written as \\(\\Delta x\\), the change in \\(x\\)). Images by Allison Horst 3.3.2 Critical Points for Minima or Maxima In both OLS and MLE, we reach points where take what are called “first order conditions.” This means we take the derivative with respect to a parameter of interest and then set the derivative = 0 and solve for the parameter to get an expression for our estimator. (E.g., In OLS, we take the derivative of the sum of squared residuals, set it equal to zero, and solve to get an expression for \\(\\hat \\beta\\)). The reason we are interested in when the derivative is zero, is because this is when the instanaeous rate of change is zero, i.e., the slope at a particular point is zero. When does this happen? At a critical point- maximum or minimum. Think about it– at the top of a mountain, there is no more rise (and no decline). You are completing level on the mountaintop. The slope at that point is zero. Let’s take an example. The function \\(f(x) = x^2 + 1\\) has the derivative \\(f&#39;(x) = 2x\\). This is zero when \\(x = 0\\). The question remains: How do we know if it is a maximum or minimum? We need to figure out if our function is concave or convex around this critical value. Convex is a “U” shape, meaning we are at a minimum, while concavity is an upside-down-U, which means we are at a maximum. We do so by taking the second derivative. This just means we take the derivative of the expression we already have for our first derivative. In our case, \\(f&#39;&#39;(x) = 2\\). So what? Well the key thing we are looking for is if this result is positive or negative. Here, it is positive, which means our function is convex at this critical value, and therefore, we are at a minimum. Just look at the function in R if we plot it. ## Let&#39;s define an arbitrary set of values for x x &lt;- -3:3 ## Now let&#39;s map the elements of x into y using our function fx &lt;- x^2 + 1 ## Let&#39;s plot the results plot(x = x, y=fx, xlab = &quot;x&quot;, type = &quot;l&quot;, main = &quot;f(x) = x^2 + 1&quot;) Notice that when x=0, we are indeed at a minimum, just as the positive value of the second derivative would suggest. A different example: \\(f(x) = -2x^2 +1\\). \\(f&#39;(x) = -4x\\) When we set this equal to 0 we find a critical value at \\(x = 0\\). \\(f&#39;&#39;(x) = -4\\). Here, the value is negative, and we know it is concave. Sure enough, let’s plot it, and notice how we can draw a horiztonal line at the maximum, representing that zero slope at the critical point: x &lt;- -3:3 fx &lt;- -2*x^2 + 1 plot(x = x, y = fx, ylab = &quot;f(x)&quot;, xlab = &quot;x&quot;, type = &quot;l&quot;, main = &quot;f(x) = -2x^2 + 1&quot;) abline(h=1, col = &quot;red&quot;, lwd=2) 3.3.3 Common Derivative Rules Below is a cheat sheet of rules for quickly identifying the derivatives of functions. The derivative of a constant is 0. \\(f(x) = a; f&#39;(x) = 0\\) Example: The derivative of 5 is 0. Here is the power rule. \\(f(x) = ax^n; f&#39;(x) = n\\times a \\times x^{n-1}\\) Example: The derivative of \\(x^3 = 3x^{(3-1)} = 3x^2\\) We saw logs in the last section, and, yes, we see logs again here. \\(f(x) = e^{ax}; f&#39;(x) = ae^{ax}\\) \\(f(x) = \\log(x); f&#39;(x) = \\frac{1}{x}\\) A very convenient rule is that a derivative of a sum = sum of the derivatives. \\(f(x) = g(x) + h(x); f&#39;(x) = g&#39;(x) + h&#39;(x)\\) Products can be more of a headache. In this course, we will turn some product expressions into summation expressions to avoid the difficulties of taking derivatives with products. Product Rule: \\(f(x) = g(x)h(x); f&#39;(x) = g&#39;(x)h(x) + h&#39;(x)g(x)\\) The chain rule below looks a bit tricky, but it can be very helpful for simplifying the way you take a derivative. See this video from NancyPi for a helpful explainer, as well as a follow-up for more complex applications here. Chain Rule: \\(f(x) = g(h(x)); f&#39;(x) = g&#39;(h(x))h&#39;(x)\\) Example: What is the derivative of \\(f(x) = \\log 5x\\)? First, we will apply the rule which tells us the derivative of a \\(\\log x\\) is \\(\\frac{1}{x}\\). However, here, we do not just have \\(x\\), we have \\(5x\\). We are in chain rule territory. After we apply the derivative to the log, which is \\(\\frac{1}{5x}\\), we then have to take the derivative of \\(5x\\) and multiply the two expressions together. The derivative of \\(5x\\) is \\(5\\). So, putting this together, our full derivative is f′(x) = 5 ∗ \\(\\frac{1}{5x}\\) = \\(\\frac{1}{x}\\). "],
["vectors-and-matrices.html", "3.4 Vectors and Matrices", " 3.4 Vectors and Matrices Vectors For our purposes, a vector is a list or “array” of numbers. For example, this might be a variable in our data– a list of the ages of all politicians in a country. Addition If we have two vectors and , where \\(\\mathbf{u} \\ = \\ (u_1, u_2, \\dots u_n)\\) and \\(\\mathbf{v} \\ = \\ (v_1, v_2, \\dots v_n)\\), \\(\\mathbf{u} + \\mathbf{v} = (u_1 + v_1, u_2 + v_2, \\dots u_n + v_n)\\) Note: \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) must be of the same dimensionality - number of elements in each must be the same - for addition. Scalar multiplication If we have a scalar (i.e., a single number) \\(\\lambda\\) and a vector \\(\\mathbf{u}\\) \\(\\lambda \\mathbf{u} = (\\lambda u_1, \\lambda u_2, \\dots \\lambda u_n)\\) We can implement vector addition and scalar multiplication in R. Let’s create a vector \\(\\mathbf{u}\\), a vector \\(\\mathbf{v}\\), and a number lambda. u &lt;- c(33, 44, 22, 11) v &lt;- c(6, 7, 8, 2) lambda &lt;- 3 When you add two vectors in R, it adds each component together. u + v [1] 39 51 30 13 We can multiply each element of a vector, u by lambda: lambda * u [1] 99 132 66 33 Element-wise Multiplication Note: When you multiply two vectors together in R, it will take each element of one vector and multiply it by each element of the other vector. u * v \\(= (u_1 * v_1, u_2 * v_2, \\dots u_n * v_n)\\) u * v [1] 198 308 176 22 3.4.1 Matrix Basics A matrix represents arrays of numbers in a rectangle, with rows and columns. A matrix with \\(m\\) rows and \\(n\\) columns is defined as (\\(m\\) x \\(n\\)). What is the dimensionality of the matrix A below? \\(A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\\\ a_{41} &amp; a_{42} &amp; a_{43} \\end{pmatrix}\\) In R, we can think of a matrix as a set of vectors. For example, we could combine the vectors u and v we created above into a matrix defined as W. ## cbind() binds together vectors as columns Wcol &lt;- cbind(u, v) Wcol u v [1,] 33 6 [2,] 44 7 [3,] 22 8 [4,] 11 2 ## rbind() binds together vectors as rows Wrow &lt;- rbind(u, v) Wrow [,1] [,2] [,3] [,4] u 33 44 22 11 v 6 7 8 2 There are other ways to create matrices in R, but using cbind and rbind() are common. We can find the dimensions of our matrices using dim() or nrow() and ncol() together. For example: dim(Wcol) [1] 4 2 nrow(Wcol) [1] 4 ncol(Wcol) [1] 2 Note how the dimensions are different from the version created with rbind(): dim(Wrow) [1] 2 4 nrow(Wrow) [1] 2 ncol(Wrow) [1] 4 Extracting specific components The element \\(a_{ij}\\) signifies the element is in the \\(i\\)th row and \\(j\\)th column of matrix A. For example, \\(a_{12}\\) is in the first row and second column. Square matrices have the same number of rows and columns Vectors have just one row or one column (e.g., \\(x_1\\) element of \\(\\mathbf{x}\\) vector) In R, we can use brackets to extract a specific \\(ij\\) element of a matrix or vector. Wcol u v [1,] 33 6 [2,] 44 7 [3,] 22 8 [4,] 11 2 Wcol[2,1] # element in the second row, first column u 44 Wcol[2,] # all elements in the second row u v 44 7 Wcol[, 1] # all elements in the first column [1] 33 44 22 11 For matrices, to extract a particular entry in R, you have a comma between entries because there are both rows and columns. For vectors, you only have one entry, so no comma is needed. u [1] 33 44 22 11 u[2] # second element in the u vector [1] 44 3.4.2 Matrix Operations Matrix Addition To be able to add matrix A and matrix B, they must have the same dimensions. Like vector addition, to add matrices, you add each of the components together. \\(A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{pmatrix}\\) and \\(B = \\begin{pmatrix} b_{11} &amp; b_{12} &amp; b_{13}\\\\ b_{21} &amp; b_{22} &amp; b_{23} \\\\ b_{31} &amp; b_{32} &amp; b_{33} \\end{pmatrix}\\) \\(A + B = \\begin{pmatrix} a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; a_{13} + b_{13}\\\\ a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; a_{23} + b_{23} \\\\ a_{31} + b_{31} &amp; a_{32} + b_{32} &amp; a_{33} + b_{33} \\end{pmatrix}\\) \\(Q = \\begin{pmatrix} 2 &amp; 4 &amp; 1\\\\ 6 &amp; 1 &amp; 5 \\end{pmatrix}\\) \\(+\\) \\(R = \\begin{pmatrix} 9 &amp; 4 &amp; 2\\\\ 11 &amp; 8 &amp; 7 \\end{pmatrix} = Q + R = \\begin{pmatrix} 11 &amp; 8 &amp; 3\\\\ 17 &amp; 9 &amp; 12 \\end{pmatrix}\\) Scalar Multiplication Take a scalar \\(\\nu\\). Just like vectors, we multiply each component of a matrix by the scalar. \\(\\nu Q = \\begin{pmatrix} \\nu q_{11} &amp; \\nu q_{12} &amp; \\dots &amp; \\nu q_{1n}\\\\ \\nu q_{21} &amp; \\nu q_{22} &amp; \\dots &amp; \\nu q_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\nu q_{m1} &amp; \\nu q_{m2} &amp; \\dots &amp; \\nu q_{mn} \\end{pmatrix}\\) Example: Take \\(c = 2\\) and a matrix A. \\(cA = c *\\begin{pmatrix} 4 &amp; 6 &amp; 1\\\\ 3 &amp; 2 &amp; 8 \\end{pmatrix}\\) = \\(\\begin{pmatrix} 8 &amp; 12 &amp; 2\\\\ 6 &amp; 4 &amp; 16 \\end{pmatrix}\\) Note the Commutativity/Associativity: For scalar \\(c\\): \\(c(AB) = (cA)B = A(cB) = (AB)c\\). Matrix Multiplication A matrix A and B must be conformable to multiply AB. To be comformable, for \\(m_A\\) x \\(n_A\\) matrix A and \\(m_B\\) x \\(n_B\\) matrix B, the “inside” dimensions must be equal: \\(n_A = m_B\\). The resulting AB has the “outside” dimensions: \\(m_A\\) x \\(n_B\\). For each \\(c_{ij}\\) component of \\(C = AB\\), we take the inner product of the \\(i^{th}\\) row of matrix A and the \\(j^{th}\\) column of matrix B. Their product C = AB is the \\(m\\) x \\(n\\) matrix where: \\(c_{ij} =a_{i1}b_{1j} + a_{i2}b_{2j} + \\dots + a_{ik}b_{kj}\\) Example: This \\(2 \\times 3\\) matrix is multiplied by a \\(3 \\times 2\\) matrix, resulting in the \\(2 \\times 2\\) matrix. \\(\\begin{pmatrix} 4 &amp; 6 &amp; 1\\\\ 3 &amp; 2 &amp; 8 \\end{pmatrix}\\) \\(\\times\\) \\(\\begin{pmatrix} 8 &amp; 12 \\\\ 6 &amp; 4 \\\\ 7 &amp; 10 \\end{pmatrix}\\) = \\(\\begin{pmatrix} (4*8 + 6*6 + 1*7) &amp; (4*12 + 6*4 + 1*10) \\\\ (3*8 + 2*6 + 8*7) &amp; (3*12 + 2*4 + 8*10) \\end{pmatrix}\\) For example, the entry in the first row and second column of the new matrix \\(c_{12} = (a_{11} = 4* b_{11} = 12) + (a_{12} = 6*b_{21} = 4) + (a_{13} = 1*b_{31} = 10)\\) We can also do matrix multiplication in R. ## Create a 3 x 2 matrix A A &lt;- cbind(c(3, 4, 6), c(5, 6, 8)) A [,1] [,2] [1,] 3 5 [2,] 4 6 [3,] 6 8 ## Create a 2 x 4 matrix B B &lt;- cbind(c(6,8), c(7, 9), c(3, 6), c(1, 11)) B [,1] [,2] [,3] [,4] [1,] 6 7 3 1 [2,] 8 9 6 11 Note that the multiplication AB is conformable because the number of columns in A matches the number of rows in B: ncol(A) nrow(B) [1] 2 [1] 2 To multiply matrices together in R, we need to add symbols around the standard asterisk for multiplication: A %*% B [,1] [,2] [,3] [,4] [1,] 58 66 39 58 [2,] 72 82 48 70 [3,] 100 114 66 94 That is necessary for multiplying matrices together. It is not necessary for scalar multiplication, where we take a single number (e.g., c = 3) and multiply it with a matrix: c &lt;- 3 c*A [,1] [,2] [1,] 9 15 [2,] 12 18 [3,] 18 24 Note the equivalence of the below expressions, which combine scalar and matrix multiplication: c* (A %*% B) [,1] [,2] [,3] [,4] [1,] 174 198 117 174 [2,] 216 246 144 210 [3,] 300 342 198 282 (c* A) %*% B [,1] [,2] [,3] [,4] [1,] 174 198 117 174 [2,] 216 246 144 210 [3,] 300 342 198 282 A %*% (c * B) [,1] [,2] [,3] [,4] [1,] 174 198 117 174 [2,] 216 246 144 210 [3,] 300 342 198 282 In social science, one matrix of interest is often a rectangular dataset that includes column vectors representing independent variables, as well as another vector that includes your dependent variable. These might have 1000 or more rows and a handful of columns you care about. "],
["additional-matrix-tidbits-that-will-come-up.html", "3.5 Additional Matrix Tidbits that Will Come Up", " 3.5 Additional Matrix Tidbits that Will Come Up Inverse An \\(n\\) x \\(n\\) matrix A is invertible if there exists an \\(n\\) x \\(n\\) inverse matrix \\(A^{-1}\\) such that: \\(AA^{-1} = A^{-1}A = I_n\\) where \\(I_n\\) is the identity matrix (\\(n\\) x \\(n\\)), that takes diagonal elements of 1 and off-diagonal elements of 0. Example: \\(I_n = \\begin{pmatrix} 1_{11} &amp; 0 &amp; \\dots &amp; 0\\\\ 0&amp; 1_{22} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; 1_{nn} \\end{pmatrix}\\) Multiplying a matrix by the identity matrix returns in the matrix itself: \\(AI_n = A\\) It’s like the matrix version of multiplying a number by one. Note: A matrix must be square \\(n\\) x \\(n\\) to be invertible. (But not all square matrices are invertible.) A matrix is invertible if and only if its columns are linearly independent. This is important for understanding why you cannot have two perfectly colinear variables in a regression model. We will not do much solving for inverses in this course. However, the inverse will be useful in solving for and simplifying expressions. 3.5.1 Transpose When we transpose a matrix, we flip the \\(i\\) and \\(j\\) components. Example: Take a 4 X 3 matrix A and find the 3 X 4 matrix \\(A^{T}\\). A transpose is usually denoted with as \\(A^{T}\\) or \\(A&#39;\\) \\(A = \\begin{pmatrix} a_{11} &amp; a_{12} &amp; a_{13}\\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\\\ a_{41} &amp; a_{42} &amp; a_{43} \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} a&#39;_{11} &amp; a&#39;_{12} &amp; a&#39;_{13} &amp; a&#39;_{14}\\\\ a&#39;_{21} &amp; a&#39;_{22} &amp; a&#39;_{23} &amp; a&#39;_{24} \\\\ a&#39;_{31} &amp; a&#39;_{32} &amp; a&#39;_{33} &amp; a&#39;_{34} \\end{pmatrix}\\) If \\(A = \\begin{pmatrix} 1 &amp; 4 &amp; 2 \\\\ 3 &amp; 1 &amp; 11 \\\\ 5 &amp; 9 &amp; 4 \\\\ 2 &amp; 11&amp; 4 \\end{pmatrix}\\) then \\(A^T = \\begin{pmatrix} 1 &amp; 3 &amp; 5 &amp; 2\\\\ 4 &amp; 1 &amp; 9 &amp; 11 \\\\ 2&amp; 11 &amp; 4 &amp; 4 \\end{pmatrix}\\) Check for yourself: What was in the first row (\\(i=1\\)), second column (\\(j=2\\)) is now in the second row (\\(i=2\\)), first column (\\(j=1\\)). That is \\(a_{12} =4 = a&#39;_{21}\\). We can transpose matrices in R using t(). For example, take our matrix A: A [,1] [,2] [1,] 3 5 [2,] 4 6 [3,] 6 8 t(A) [,1] [,2] [,3] [1,] 3 4 6 [2,] 5 6 8 In R, you can find the inverse of a square matrix with solve() solve(A) Error in solve.default(A): &#39;a&#39; (3 x 2) must be square Note, while A is not square A’A is square: AtA &lt;- t(A) %*% A solve(AtA) [,1] [,2] [1,] 2.232143 -1.553571 [2,] -1.553571 1.089286 3.5.2 Additional Matrix Properties and Rules These are a few additional properties and rules that will be useful to us at various points in the course: Symmetric: Matrix A is symmetric if \\(A = A^T\\) Idempotent: Matrix A is idempotent if \\(A^2 = A\\) Trace: The trace of a matrix is the sum of its diagonal components \\(Tr(A) = a_{11} + a_{22} + \\dots + a_{mn}\\) Example of symmetric matrix: \\(D = \\begin{pmatrix} 1 &amp; 6 &amp; 22 \\\\ 6 &amp; 4 &amp; 7 \\\\ 22 &amp; 7 &amp; 11 \\end{pmatrix}\\) ## Look at the equivalence D &lt;- rbind(c(1,6,22), c(6,4,7), c(22,7,11)) D [,1] [,2] [,3] [1,] 1 6 22 [2,] 6 4 7 [3,] 22 7 11 t(D) [,1] [,2] [,3] [1,] 1 6 22 [2,] 6 4 7 [3,] 22 7 11 What is the trace of this matrix? ## diag() pulls out the diagonal of a matrix sum(diag(D)) [1] 16 3.5.3 Matrix Rules Due to conformability and other considerations, matrix operations are somewhat more restrictive, particularly when it comes to commutativity. Associative \\((A + B) + C = A + (B + C)\\) and \\((AB) C = A(BC)\\) Commutative \\(A + B = B + A\\) Distributive \\(A(B + C) = AB + AC\\) and \\((A + B) C = AC + BC\\) Commutative law for multiplication does not hold– the order of multiplication matters: $ AB BA$ Rules for Inverses and Transposes These rules will be helpful for simplifying expressions. Treat \\(A\\), \\(B\\), and \\(C\\) as matrices below, and \\(s\\) as a scalar. \\((A + B)^T = A^T + B^T\\) \\((s A)^T\\) \\(= s A^T\\) \\((AB)^T = B^T A^T\\) \\((A^T)^T = A\\) and \\(( A^{-1})^{-1} = A\\) \\((A^T)^{-1} = (A^{-1})^T\\) \\((AB)^{-1} = B^{-1} A^{-1}\\) \\((ABCD)^{-1} = D^{-1} C^{-1} B^{-1} A^{-1}\\) 3.5.4 Derivatives with Matrices and Vectors Let’s say we have a \\(p \\times 1\\) “column” vector \\(\\mathbf{x}\\) and another \\(p \\times 1\\) vector \\(\\mathbf{a}\\). Taking the derivative with respect to vector \\(\\mathbf{x}\\). Let’s say we have \\(y = \\mathbf{x}&#39;\\mathbf{a}\\). This process is explained here. Taking the derivative of this is called the gradient. \\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{dy}{dx_p} \\end{pmatrix}\\) \\(y\\) will have dimensions \\(1 \\times 1\\). \\(y\\) is a scalar. Note: \\(y = a_1x_1 + a_2x_2 + ... + a_px_p\\). From this expression, we can take a set of “partial derivatives”: \\(\\frac{\\delta y}{\\delta x_1} = a_1\\) \\(\\frac{\\delta y}{\\delta x_2} = a_2\\), and so on \\(\\frac{\\delta y}{\\delta x} = \\begin{pmatrix}\\frac{\\delta y}{\\delta x_1} \\\\ \\frac{\\delta y}{\\delta x_2} \\\\ \\vdots \\\\ \\frac{\\delta y}{\\delta x_p} \\end{pmatrix} = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_p \\end{pmatrix}\\) Well, this is just vector \\(\\mathbf{a}\\) Answer: \\(\\frac{\\delta }{\\delta x} \\mathbf{x}^T\\mathbf{a} = \\mathbf{a}\\). We can apply this general rule in other situations. Example 2 Let’s say we want to differentiate the following where vector \\(\\mathbf{y}\\) is \\(n \\times 1\\), \\(X\\) is \\(n \\times k\\), and \\(\\mathbf{b}\\) is \\(k \\times 1\\). Take the derivative with respect to \\(b\\). \\(\\mathbf{y}&#39;\\mathbf{y} - 2\\mathbf{b}&#39;X&#39;\\mathbf{y}\\) Note that the dimensions of the output are \\(1 \\times 1\\), a scalar quantity. Remember the derivative of a sum is the sum of derivatives. This allows us to focus on particular terms. The first term has no \\(\\mathbf{b}\\) in it, so this will contribute 0. The second term is \\(2\\mathbf{b}&#39;X&#39;\\mathbf{y}\\). We can think about this like the previous example \\(\\frac{\\delta }{\\delta b} 2\\mathbf{b}&#39;X&#39;\\mathbf{y} = \\begin{pmatrix} \\frac{\\delta }{\\delta b_1}\\\\ \\frac{\\delta }{\\delta b_2} \\\\ \\vdots \\\\ \\frac{\\delta }{\\delta b_k} \\end{pmatrix}\\) The output is needs to be \\(k \\times 1\\) like \\(\\mathbf{b}\\), which is what \\(2 * X&#39;\\mathbf{y}\\) is. The derivative is \\(-2X&#39;\\mathbf{y}\\) Example 3 Another useful rule when a matrix \\(A\\) is symmetric: \\(\\frac{\\delta}{\\delta \\mathbf{x}} \\mathbf{x}^TA\\mathbf{x} = (A + A^T)\\mathbf{x} = 2A\\mathbf{x}\\). Details on getting to this result. We are treating the vector \\(\\mathbf{x}\\) as \\(n \\times 1\\) and the matrix \\(A\\) as symmetric. When we take \\(\\frac{\\delta}{\\delta \\mathbf{x}}\\) (the derivative with respect to \\(\\mathbf{x}\\)), we will be looking for a result with the same dimensions \\(\\mathbf{x}\\). \\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\\\ \\vdots \\\\ \\frac{d}{dx_n} \\end{pmatrix}\\) Let’s inspect the dimensions of \\(\\mathbf{x}^TA\\mathbf{x}\\). They are \\(1 \\times 1\\). If we perform this matrix multiplication, we would be multiplying: \\(\\begin{pmatrix} x_1 &amp; x_2 &amp; \\ldots &amp; x_i \\end{pmatrix} \\times \\begin{pmatrix} a_{11} &amp; a_{12} &amp; \\ldots &amp; a_{1j} \\\\ a_{21} &amp; a_{22} &amp; \\ldots &amp; a_{2j} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ a_{i1} &amp; a_{i2} &amp; \\ldots &amp; a_{ij} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_j \\end{pmatrix}\\) To simplify things, let’s say we have the following matrices, where \\(A\\) is symmetric: \\(\\begin{pmatrix} x_1 &amp; x_2 \\end{pmatrix} \\times \\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} \\times \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}\\) We can perform the matrix multiplication for the first two quantities, which will result in a \\(1 \\times 2\\) vector. Recall in matrix multiplication we take the sum of the element-wise multiplication of the \\(ith\\) row of the first object by the \\(jth\\) column of the second object. This means multiply the first row of \\(\\mathbf{x}\\) by the first column of \\(A\\) for the entry in cell \\(i=1; j=1\\), and so on. \\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) &amp; (x_1a_{12} + x_2a_{22}) \\end{pmatrix}\\) We can then multiply this quantity by the last quantity \\(\\begin{pmatrix} (x_1a_{11} + x_2a_{21}) &amp; (x_1a_{12} + x_2a_{22}) \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix}\\) This will results in the \\(1 \\times 1\\) quantity: \\((x_1a_{11} + x_2a_{21})x_1 + (x_1a_{12} + x_2a_{22})x_2 = x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22}\\) We can now take the derivatives with respect to \\(\\mathbf{x}\\). Because \\(\\mathbf{x}\\) is \\(2 \\times 1\\), our derivative will be a vector of the same dimensions with components: \\(\\frac{\\delta }{\\delta \\mathbf{x}} = \\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix}\\) These represent the partial derivatives of each component within \\(\\mathbf{x}\\) Let’s focus on the first: \\(\\frac{\\delta }{\\delta x_1}\\). \\[\\begin{align*} \\frac{\\delta }{\\delta x_1} x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22} &amp;=\\\\ &amp;= 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\end{align*}\\] We can repeat this for \\(\\frac{\\delta }{\\delta x_2}\\) \\[\\begin{align*} \\frac{\\delta }{\\delta x_2} x_1^2a_{11} + x_2a_{21}x_1 + x_1a_{12}x_2 + x_2^2a_{22} &amp;=\\\\ &amp;= a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\\\ \\end{align*}\\] Now we can put the result back into our vector format: \\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + x_2a_{21} + a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = a_{21}x_1 + x_1a_{12} + 2x_2a_{22}\\end{pmatrix}\\) Now it’s just about simplifying to show that we have indeed come back to the rule. Recall that for a symmetric matrix, the elements in rows and columns \\(ij\\) = the elements in \\(ji\\). This allows us to read \\(a_{21} = a_{12}\\) and combine those terms (e.g., \\(x_2a_{21} + a_{12}x_2 =2a_{12}x_2\\)) : \\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} = 2x_1a_{11} + 2a_{12}x_2\\\\ \\frac{\\delta }{\\delta x_2} = 2a_{21}x_1 + 2x_2a_{22}\\end{pmatrix}\\) Second, we can now bring the 2 out front. \\(\\begin{pmatrix}\\frac{\\delta }{\\delta x_1} \\\\ \\frac{\\delta }{\\delta x_2} \\end{pmatrix} = 2 * \\begin{pmatrix} x_1a_{11} + a_{12}x_2\\\\ a_{21}x_1 + x_2a_{22}\\end{pmatrix}\\) Finally, let’s inspect this and show it is equivalent to this multiplication where we have a \\(2 \\times 2\\) \\(A\\) matrix multiplied by a \\(2 \\times 1\\) \\(\\mathbf x\\) vector. Minor note: Because any individual element of a vector is just a single quantity, we can change the order (e.g., \\(a_{11}*x_1\\) vs. \\(x_1*a_{11}\\)). We just can’t do that for full vectors or matrices \\(2A\\mathbf{x} = 2* \\begin{pmatrix} a_{11} &amp; a_{12} \\\\ a_{21} &amp; a_{22} \\end{pmatrix} \\times \\begin{pmatrix}x_1 \\\\ x_2 \\end{pmatrix} = 2 * \\begin{pmatrix} a_{11}x_1 + a_{12}x_2 \\\\ a_{21}x_1 + a_{22}x_2 \\end{pmatrix}\\) The last quantity is the same as the previous step. That’s the rule! Applying the rules This has a nice analogue to the derivative we’ve seen before \\(q^2 = 2*q\\). Let’s say we want to take the derivative of \\(\\mathbf{b}&#39;X&#39;X\\mathbf{b}\\) with respect to \\(\\mathbf{b}\\). We can think of \\(X&#39;X\\) as if it is \\(A\\). This gives us \\(2X&#39;X\\mathbf{b}\\) as the result. Why on earth would we care about this? For one, it helps us understand how we get to our estimates for \\(\\hat \\beta\\) in linear regression. When we have multiple variables, we don’t just want the best estimate for one coefficient, but a vector of coefficients. See more here. In MLE, we will find the gradient of the log likelihood function. We will further go into the second derivatives to arrive at what is called the Hessian. More on that later. "],
["practice-problems.html", "3.6 Practice Problems", " 3.6 Practice Problems What is \\(24/3 + 5^2 - (8 -4)\\)? What is \\(\\sum_{i = 1}^5 (i*3)\\)? Take the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\). Take the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\). Take the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\). Given \\(X\\) is an \\(n\\) x \\(k\\) matrix, \\((X^{T}X)^{-1}X^{T}X\\) can be simplified to? \\(((X^{T}X)^{-1}X^{T})^{T} =\\) ? If \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify? If a matrix \\(P\\) is idempotent, \\(PP =\\) ? 3.6.1 Practice Problem Solutions What is \\(24/3 + 5^2 - (8 -4)\\)? 24/3 + 5^2 - (8 -4) [1] 29 What is \\(\\sum_{i = 1}^5 (i*3)\\)? By hand: \\(1 \\times 3 + 2 \\times 3 + 3 \\times 3 + 4 \\times 3 + 5 \\times 3\\) ## sol 1 1*3 + 2*3 + 3*3 + 4*3 + 5*3 [1] 45 ## sol 2 i &lt;- 1:5 sum(i*3) [1] 45 Take the derivative of \\(f(x) =v(4x^2 + 6)^2\\) with respect to \\(x\\). We can treat \\(v\\) as a number. \\[\\begin{align*} f&#39;(x) &amp;= 2* v(4x^2 + 6) * 8x\\\\ &amp;= 16vx(4x^2 + 6) \\end{align*}\\] Take the derivative of \\(f(x) = e^{2x + 3}\\) with respect to \\(x\\). \\[\\begin{align*} f&#39;(x) &amp;= 2* e^{2x + 3}\\\\ &amp;= 2e^{2x + 3} \\end{align*}\\] Take the derivative of \\(f(x) = log (x + 3)^2\\) with respect to \\(x\\). Note we can re-write this as \\(2 * log (x + 3)\\). \\[\\begin{align*} f&#39;(x) &amp;= 2 * \\frac{1}{(x + 3)} * 1\\\\ &amp;= \\frac{2}{(x + 3)} \\end{align*}\\] If we didn’t take that simplifying step, we can still solve: \\[\\begin{align*} f&#39;(x) &amp;= \\frac{1}{(x + 3)^2} * 2 * (x + 3) *1\\\\ &amp;= \\frac{2}{(x + 3)} \\end{align*}\\] Given \\(X\\) is an \\(n\\) x \\(k\\) matrix, \\((X^{T}X)^{-1}X^{T}X\\) can be simplified to? \\(I_k\\) the identity matrix \\(((X^{T}X)^{-1}X^{T})^{T} =\\) ? Recall our rule \\((AB)^T = B^TA^T\\) \\(X(X^TX)^{-1}\\) If \\(\\nu\\) is a constant, how does \\((X^{T}X)^{-1}X^{T} \\nu X(X^{T}X)^{-1}\\) simplify? We can pull it out front. \\[\\begin{align*} &amp;= \\nu(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\\\ &amp;= \\nu (X^{T}X)^{-1} \\end{align*}\\] If a matrix \\(P\\) is idempotent, \\(PP =\\) ? \\(P\\) from section 3.5.2 "],
["ols.html", "Section 4 Review of OLS", " Section 4 Review of OLS This section will provide a review of OLS. OLS is the workhorse of empirical political science. We will learn a lot beyond OLS, but OLS is often “good enough” and sometimes preferable for explaining the relationship between variables. That is to say, MLE will expand your toolkit, but OLS should remain a big part of your toolkit. I recommend that you review the following readings to familiarize yourself with regression. I will make note within this section where particular readings are most relevant. These readings are available on Canvas in the modules- Week 1 section. Wheelan, Charles. 2012. Naked Statistics. W.W. Norton. Chapter 11. This provides an accessible overview of regression and the interpretation of regression results. Gelman, Andrew, and Jennifer Hill. 2006. Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Chapter 3. This is a slightly more technical overview and includes some R code for running regressions. Building models and breaking models. (Optional) Fox, John. 2015. Applied Regression Analysis and Generalized Linear Models, 2nd Edition. Sage. Chapter 11. This reading describes diagnostic tests to probe whether the model is a good fit of the data. We won’t go into detail about this in this class, but is material classes focused on linear regression will generally cover. Messing, Solomon. “How to break regression.” Lenz, G., &amp; Sahn, A. (2020). “Achieving Statistical Significance with Control Variables and Without Transparency.” Political Analysis, 1-14. doi:10.1017/pan.2020.31. This paper talks about how to build a regression model, and in particular, why adding more and more controls isn’t always a good thing. "],
["introducing-ols-regression.html", "4.1 Introducing OLS Regression", " 4.1 Introducing OLS Regression The regression method describes how one variable depends on one or more other variables. Ordinary Least Squares regression is a linear model with the matrix representation: \\(Y = \\alpha + X\\beta + \\epsilon\\) Given values of variables in \\(X\\), the model predicts the average of an outcome variable \\(Y\\). For example, if \\(Y\\) is a measure of how wealthy a country is, \\(X\\) may contain measures related to the country’s natural resources and/or features of its institutions (things that we think might contribute to how wealthy a country is.) In this equation: \\(Y\\) is the outcome variable (\\(n \\times 1\\)).1 \\(\\alpha\\) is a parameter representing the intercept \\(\\beta\\) is a parameter representing the slope/marginal effect (\\(k \\times 1\\)), and \\(\\epsilon\\) is the error term (\\(n \\times 1\\)). In OLS, we estimate a line of best fit to predict \\(\\hat{Y}\\) values for different values of X: \\(\\hat{Y} = \\hat{\\alpha} + X\\hat{\\beta}\\). When you see a “\\(\\hat{hat}\\)” on top of a letter, that means it is an estimate of a parameter. As we will see in the next section, in multiple regression, sometimes this equation is represented as just \\(\\hat{Y} = X\\hat{\\beta}\\), where this generally means that \\(X\\) is a matrix that includes several variables and \\(\\hat \\beta\\) is a vector that includes several coefficients, including a coefficient representing the intercept \\(\\hat \\alpha\\) We interpret linear regression coefficients as describing how a dependent variable is expected to change when a particular independent variable changes by a certain amount. Specifically: “Associated with each one unit increase in a variable \\(x_1\\), there is a \\(\\hat{\\beta_1}\\) estimated expected average increase in \\(y\\).” If we have more than one explanatory variable (i.e., a multiple regression), we add the phrase “controlling on/ holding constant other observed factors included in the model.” We can think of the interpretation of a coefficient in multiple regression using an analogy to a set of light switches: We ask: How much does the light in the room change when we flip one switch, while holding constant the position of all the other switches? This would be a good place to review the Wheelan chapter and Gelman and Hill 3.1 and 3.2 to reinforce what a regression is and how to interpret regression results. Recall this notation means rows by columns, \\(Y\\) is a vector of length \\(n\\) (the number of observations), and since there is only 1 outcome measure, it is 1 column.↩︎ "],
["diving-deeper-into-ols-matrix-representation.html", "4.2 Diving Deeper into OLS Matrix Representation", " 4.2 Diving Deeper into OLS Matrix Representation In this section, we will review the matrix representation of the OLS regression in more detail and discuss how to derive the estimators for the regression coefficients.2 OLS in Matrix Form: Let \\(X\\) be an \\(n \\times k\\) matrix where we have observations on k independent variables for n observations. Since our model will usually contain a constant term, one of the columns in the X matrix will contain only ones. This column should be treated exactly the same as any other column in the X matrix. Let \\(Y\\) be an \\(n \\times 1\\) vector of observations on the dependent variable. Note: because \\(Y\\) is a vector (a matrix with just one column), sometimes it is written in lowercase notation as \\(\\mathbf y\\). Let \\(\\epsilon\\) be an \\(n \\times 1\\) vector of disturbances or errors. Let \\(\\beta\\) be an \\(k \\times 1\\) vector of unknown population parameters that we want to estimate. \\(\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ ... \\\\ y_n \\end{pmatrix}\\) = \\(\\begin{pmatrix} 1 &amp; x_{11} &amp; x_{12} &amp; x_{13} &amp; ... &amp; x_{1k}\\\\ 1 &amp; x_{21} &amp; x_{22} &amp; x_{23} &amp; ... &amp; x_{2k} \\\\ 1 &amp; x_{31} &amp; x_{32} &amp; x_{33} &amp; ... &amp; x_{3k}\\\\ 1 &amp; x_{41} &amp; x_{42} &amp; x_{43} &amp; ... &amp; x_{4k} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ... \\\\ 1 &amp; x_{n1} &amp; x_{n2} &amp; x_{n3} &amp; ... &amp; x_{nk}\\end{pmatrix}\\) X \\(\\begin{pmatrix} \\alpha \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ ... \\\\ \\beta_k \\end{pmatrix}\\) + \\(\\begin{pmatrix} \\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\epsilon_4 \\\\ ... \\\\ \\epsilon_n \\end{pmatrix}\\) Our estimates are then \\(\\mathbf{ \\hat y} = X\\hat \\beta\\). What are the dimensions of this quantity? Gelman and Hill Section 3.4, pg. 38 provides a nice visual of how this representation maps onto what a typical dataset may look like, where we will try to estimate a set of coefficients that map the relationship between the columns of \\(X\\) and \\(\\mathbf y\\): \\ This is a good place to review Gelman and Hill 3.4 on different notations for representing the regression model. 4.2.1 Estimating the Coefficients Models generally start with some goal. In OLS, our goal is to minimize the sum of squared “residuals.” Here is a video I created to explain why we can represent this as \\(\\mathbf{e&#39;}\\mathbf{e}\\). Note: at the end of the video it should read \\(X\\hat\\beta\\), not \\(\\hat X \\beta\\) What is a residual? It’s the difference between y and our estimate of y: \\(y - \\hat y\\). It represents the error in our prediction– how far off our estimate is of the outcome. We can write this in matrix notation in the following way where \\(\\mathbf e\\) is an \\(n \\times 1\\) vector of residuals– a residual for each observation in the data: \\[\\begin{align*} \\mathbf{e&#39;}\\mathbf{e} &amp;= (Y&#39; - \\hat{\\beta}&#39;X&#39;)(Y - X\\hat{\\beta})\\\\ &amp;=Y&#39;Y - \\hat{\\beta}&#39;X&#39;Y - Y&#39;X\\hat{\\beta} + \\hat{\\beta}&#39;X&#39;X\\hat{\\beta} \\\\ &amp;= Y&#39;Y - 2\\hat{\\beta}&#39;X&#39;Y + \\hat{\\beta}&#39;X&#39;X\\hat{\\beta} \\end{align*}\\] Recall we want a line that minimizes this quantity. We minimize the sum of squared residuals by taking the derivative with respect to \\(\\beta\\). (We want to identify the coefficients that help us achieve the goal of minimizing the squared error.) Because we are now deriving an estimate, we will use the hat over \\(\\beta\\): \\(\\frac{\\delta }{\\delta \\hat \\beta} = -2X&#39;Y + 2X&#39;X\\hat{\\beta}\\) So what is our estimate for \\(\\hat{\\beta}\\)? We take first order conditions \\[\\begin{align*} 0 &amp;=-2X&#39;Y + 2X&#39;X\\hat{\\beta}\\\\ \\hat{\\beta} &amp;= (X&#39;X)^{-1}X&#39;Y \\end{align*}\\] You may wonder how we got to these answers. Don’t worry, you will get your chance to solve this! The important thing to note for now, is that we have an analytic solution to our coefficient estimates. This video from Ben Lambert provides additional intuition for understanding OLS in a matrix form and how it can be useful.↩︎ "],
["ols-regression-in-r.html", "4.3 OLS Regression in R", " 4.3 OLS Regression in R To run a linear regression in R, we use the lm() function. The syntax is lm(y ~ x1, data = mydata) for a regression with y as the name of your dependent variable and there is one explanatory variable x1 where mydata is the name of your data frame. lm(y ~ x1 + x2 , data = mydata) is the syntax for a regression with two explanatory variables x1 and x2, where you would add additional variables for larger multivariate regressions. By default, R will include an intercept term in the regression. 4.3.1 Example: Predicting Current Election Votes from Past Election Votes In the American presidential election in 2000, there was an actual controversy in how ballots were cast in the state of Florida. Social scientists used data comparing the election results from 1996 in the state with 2000 as one way to help detect irregularities in the 2000 vote count. For more information on the background of this example, you can watch this video. We will use the data florida.csv available here: ## Load Data florida &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv&quot;) This data set includes several variables described below, where each row represents the voting information for a particular county in Florida. Name Description county county name Clinton96 Clinton’s votes in 1996 Dole96 Dole’s votes in 1996 Perot96 Perot’s votes in 1996 Bush00 Bush’s votes in 2000 Gore00 Gore’s votes in 2000 Buchanan00 Buchanan’s votes in 2000 In 2000, Buchanan was a third party candidate, similar to Perot in 1996. One might think that counties where Perot received a lot of votes in 1996 should also receive a lot in 2000. That is: with a one-vote increase in Perot’s vote, we might expect an average increase in Buchanan’s 2000 vote. We can translate that language into a regression equation: \\(Buchanan2000 = \\alpha + Perot1996 * \\beta + \\epsilon\\) In R, we run this regression the following way. We will save it as an object fit.1. You can name your regression objects anything you want. fit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida) summary(model) provides the summary statistics of the model. In particular, the following statistics are important Estimate: point estimate of each coefficient Std. Error: standard error of each estimate t value: indicates the \\(t\\)-statistic of each coefficient under the null hypothesis that it equals zero Pr(&gt;|t|): indicates the two-sided \\(p\\)-value corresponding to this \\(t\\)-statistic where asterisks indicate the level of statistical significance. Multiple R-squared: The coefficient of determination Adjusted R-squared: The coefficient of determination adjusting for the degrees of freedom We will say more to define these quantities in future sections. summary(fit.1) Call: lm(formula = Buchanan00 ~ Perot96, data = florida) Residuals: Min 1Q Median 3Q Max -612.74 -65.96 1.94 32.88 2301.66 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.34575 49.75931 0.027 0.979 Perot96 0.03592 0.00434 8.275 9.47e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 316.4 on 65 degrees of freedom Multiple R-squared: 0.513, Adjusted R-squared: 0.5055 F-statistic: 68.48 on 1 and 65 DF, p-value: 9.474e-12 R also allows several shortcuts for accessing particular elements of your regression results. Examples: ## Vector of the coefficient estimates only coef(fit.1) (Intercept) Perot96 1.34575212 0.03591504 ## Compute confidence intervals for these coefficients confint(fit.1) 2.5 % 97.5 % (Intercept) -98.03044506 100.72194929 Perot96 0.02724733 0.04458275 ## Table of coefficient results only summary(fit.1)$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.34575212 49.759306434 0.02704523 9.785065e-01 Perot96 0.03591504 0.004340068 8.27522567 9.473505e-12 ## Extract standard errors only summary(fit.1)$coefficients[,2] (Intercept) Perot96 49.759306434 0.004340068 ## Variance-Covariance matrix vcov(fit.1) (Intercept) Perot96 (Intercept) 2475.9885768 -1.360074e-01 Perot96 -0.1360074 1.883619e-05 ## Note that the square root of the diagonal of this matrix provides the standard errors sqrt(diag(vcov(fit.1))) (Intercept) Perot96 49.759306434 0.004340068 ## Degrees of freedom fit.1$df.residual [1] 65 4.3.2 Plotting Regression Results We often don’t want to hide our data under a bushel basket or in complicated regression models. Instead, we might also want to visualize data in R. The function plot() and the function ggplot() from the package ggplot2 are two terrific and flexible functions for visualizing data. We will use the plot() function to visualize the relationship between Perot and Buchanan votes. The example below provides a few arguments you can use within each of these functions, but they are capable of much more. At the core, plotting functions generally work as coordinate systems. You tell R specifically at which x and y coordinates you want your points to be located (e.g., by providing R with a vector of x values and a vector of y values). Then, each function has its own way of allowing you to add bells and whistles to your figure, such as labels (e.g., main, xlab, ylab), point styles ({}), additional lines and points and text (e.g., abline(), lines(), points(), text()), or x and y scales for the dimensions of your axes (e.g., xlim, ylim). You can create a plot without these additional features, but most of the time, you will add them to make your plots look good! and be informative! We will do a lot of plotting this semester. Note: feel free to use plot() or ggplot() or both. ggplot has similar capabilities as plot but relies on a different “grammar” of graphics. For example, see the subtle differences in the two plots below. ## Plot plot(x = florida$Perot96, # x-values y = florida$Buchanan00, # y-values main = &quot;Perot and Buchanan Votes&quot;, # label for main title ylab = &quot;Buchanan Votes&quot;, # y-axis label xlab = &quot;Perot Votes&quot;, # x-axis label pch = 20) # point type abline(fit.1, col = &quot;red&quot;) # adds a red regression line ## ggplot version library(ggplot2) ggplot(data = florida, # which data frame mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates geom_point() + # tells R we want a scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;red&quot;, data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line ggtitle(&quot;Perot and Buchanan Votes&quot;) + # main title labs(x = &quot;Perot Votes&quot;, y = &quot;Buchanan Votes&quot;) + # x and y labels theme_bw() # changes theme (e.g., color of background) `geom_smooth()` using formula &#39;y ~ x&#39; ## Note: data = florida, aes(x=Perot96, y=Buchanan00) in the geom_smooth line is not necessary if it is the same mapping at the first line. Required if data are different Tip: you might want to save your plots as .pdf or .png after you create it. You can do this straight from your R code. How you do it varies by function. The files will save to your working directory unless you specify a different file path. The code below is the same as above except it has additional lines for saving the plots: ## Plot pdf(file = &quot;myfirstmleplot.pdf&quot;, width = 7, height = 5) # play around with the dimensions plot(x = florida$Perot96, # x-values y = florida$Buchanan00, # y-values main = &quot;Perot and Buchanan Votes&quot;, # label for main title ylab = &quot;Buchanan Votes&quot;, # y-axis label xlab = &quot;Perot Votes&quot;, # x-axis label pch = 20) # point type abline(fit.1, col = &quot;red&quot;) # adds a red regression line dev.off() # this closes your pdf file ## ggplot version ggplot(data = florida, # which data frame mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates geom_point() + # tells R we want a scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;red&quot;, data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line ggtitle(&quot;Perot and Buchanan Votes&quot;) + # main title labs(x = &quot;Perot Votes&quot;, y = &quot;Buchanan Votes&quot;) + # x and y labels theme(plot.title = element_text(hjust = 0.5)) +# centers the title theme_bw() # changes theme (e.g., color of background) ggsave(&quot;myfirstmleggplot.png&quot;, device=&quot;png&quot;, width = 7, height = 5) # saves the last ggplot 4.3.3 Finding Coefficients without lm Let’s put our matrix algebra and R knowledge together. In the previous section, we found that \\(\\hat \\beta = (X&#39;X)^{-1}X&#39;Y\\). If we do that math directly in R, there is no need to use lm() to find those coefficients. To do so, we need \\(X\\) and \\(Y\\). Recall \\(Y\\) is an \\(n \\times 1\\) vector representing the outcome of our model. In this case, \\(Y\\) is Buchanan00. Y &lt;- florida$Buchanan00 Recall, \\(X\\) is a \\(n \\times k\\) matrix representing our independent variables and a column of 1’s for the intercept. Let’s build this matrix using cbind which was introduced in section 2. X &lt;- cbind(1, florida$Perot96) dim(X) [1] 67 2 Great, now we have \\(X\\) and \\(Y\\), so it’s just about a little math. Because \\(Y\\) is a vector, let’s make sure R knows to treat it like an \\(n \\times 1\\) matrix. Y &lt;- cbind(Y) dim(Y) [1] 67 1 Recall the solve() and t() functions take the inverse and transpose of matrices. betahat &lt;- solve(t(X) %*% X) %*% t(X) %*% Y Finally, let’s compare the results from our model using lm() with these results. betahat coef(fit.1) Y [1,] 1.34575212 [2,] 0.03591504 (Intercept) Perot96 1.34575212 0.03591504 We did it! In the problem set, you will get more experience using the analytic solutions to solve for quantities of interest instead of the built-in functions. 4.3.4 OLS Practice Problems Here are a couple of (ungraded) problems to modify the code above and gain additional practice with data wrangling and visualization in R. As you might have noticed in the example, there is a big outlier in the data. We will see how this observation affects the results. Using a linear regression examine the relationship between Perot and Buchanan votes, controlling for Bill Clinton’s 1996 votes. Provide a one sentence summary of the relationship between Perot and Buchanan’s votes. Is the relationship significant at the \\(p &lt; 0.05\\) level? What about the relationship between Clinton and Buchanan votes? What are the confidence intervals for the Perot coefficient results? What is the residual for the estimate for Palm Beach County– PalmBeach in the county variable? Let’s go back to the bivariate case. Subset the data to remove the county PalmBeach. Create a scatterplot of the relationship between Perot votes and Buchanan votes within this subset. This time make the points blue. Add a regression line based on this subset of data. Add a second regression line in a different color based on the initial bivariate regression we ran in the example, where all data were included. Describe the differences in the regression lines. 4.3.5 Code for solutions fit.multiple &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida) summary(fit.multiple) Call: lm(formula = Buchanan00 ~ Perot96 + Clinton96, data = florida) Residuals: Min 1Q Median 3Q Max -705.06 -49.17 -4.71 27.34 2254.89 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 14.110353 51.644141 0.273 0.78556 Perot96 0.027394 0.010095 2.714 0.00854 ** Clinton96 0.001283 0.001372 0.935 0.35325 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 316.7 on 64 degrees of freedom Multiple R-squared: 0.5196, Adjusted R-squared: 0.5046 F-statistic: 34.61 on 2 and 64 DF, p-value: 6.477e-11 confint(fit.multiple)[2,] 2.5 % 97.5 % 0.007228254 0.047560638 florida$res &lt;- residuals(fit.multiple) florida$res[florida$county == &quot;PalmBeach&quot;] [1] 2254.893 florida.pb &lt;- subset(florida, subset = (county != &quot;PalmBeach&quot;)) fit2 &lt;- lm(Buchanan00 ~ Perot96, data = florida.pb) ggplot(data = florida.pb, # which data frame mapping = aes(x = Perot96, y = Buchanan00)) + # x and y coordinates geom_point(color=&quot;blue&quot;) + # tells R we want a scatterplot geom_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;green&quot;, data = florida.pb, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line geom_smooth(method = &quot;lm&quot;, se = FALSE, colour = &quot;red&quot;, data = florida, aes(x=Perot96, y=Buchanan00)) + # adds lm regression line ggtitle(&quot;Perot and Buchanan Votes&quot;) + # main title labs(x = &quot;Perot Votes&quot;, y = &quot;Buchanan Votes&quot;) + # x and y labels theme(plot.title = element_text(hjust = 0.5)) +# centers the title theme_bw() # changes theme (e.g., color of background) `geom_smooth()` using formula &#39;y ~ x&#39; `geom_smooth()` using formula &#39;y ~ x&#39; "],
["week-1-thursday-tutorial.html", "4.4 Week 1 Thursday Tutorial", " 4.4 Week 1 Thursday Tutorial We may be using a tutorial today to get additional practice with linear regression and its assumptions. To load the tutorial you need to have the package devtools installed in your R. Then run the following lines of code. devtools::install_github(&quot;ktmccabe/interactivepack&quot;, dependencies = T) learnr::run_tutorial(&quot;lindep&quot;, package=&quot;interactivepack&quot;) "],
["uncertainty-and-regression.html", "4.5 Uncertainty and Regression", " 4.5 Uncertainty and Regression We have now gone through the process of minimizing the sum of squared errors (\\(\\mathbf{e&#39;e}\\)) and deriving estimates for the OLS coefficients \\(\\hat \\beta = (X&#39;X)^{-1}X&#39;Y\\). In this section, we will discuss how to generate estimates of the uncertainty around these estimates. Where we are going: In the last section, we visited an example related to the 2000 election in Florida. We regressed county returns for Buchanan in 2000 (Y) on county returns for Perot in 1996 (X). ## Load Data florida &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv&quot;) fit.1 &lt;- lm(Buchanan00 ~ Perot96, data = florida) summary(fit.1) Call: lm(formula = Buchanan00 ~ Perot96, data = florida) Residuals: Min 1Q Median 3Q Max -612.74 -65.96 1.94 32.88 2301.66 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 1.34575 49.75931 0.027 0.979 Perot96 0.03592 0.00434 8.275 9.47e-12 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 316.4 on 65 degrees of freedom Multiple R-squared: 0.513, Adjusted R-squared: 0.5055 F-statistic: 68.48 on 1 and 65 DF, p-value: 9.474e-12 The summary output of the model shows many different quantities in addition to the coefficient estimates. In particular, in the second column of the summary, we see the standard errors of the coefficients. Like many statistical software programs, the lm() function neatly places these right next to the coefficients. We will now discuss how we get to these values. 4.5.1 Variance of the Coefficients The standard error is the square root of the variance, representing the typical deviation we would expect to see between our estimates \\(\\hat \\beta\\) of the parameter \\(\\beta\\) across repeated samples. So to get to the standard error, we just need to get to an estimate of the variance. Let’s take the journey. First the math. As should start becoming familiar, we have our initial regression equation, which describes the relationship between the independent variables and dependent variables. Start with the model: \\(Y = X\\beta + \\epsilon\\) We want to generate uncertainty for our estimate of \\(\\hat \\beta =(X&#39;X)^{-1}X&#39;Y\\) Note: Conditional on fixed values of \\(X\\) (I say fixed values because this is our data. We know \\(X\\) from our dataset.), the only random component is \\(\\epsilon\\). What does that mean? Essentially, the random error term in our regression equation is what is giving us the uncertainty. If \\(Y\\) was a deterministic result of \\(X\\), we would have no need for it, but it’s not. The relationship is not exact, varies sample to sample, subject to random perturbations, represented by \\(\\epsilon\\). Below we go through how to arrive at the mathematical quantity representing the variance of \\(\\hat \\beta\\) which we will notate as \\(\\mathbf{V}(\\hat\\beta)\\). The first part of the math below is just substituting terms: \\[\\begin{align*} \\mathbf{V}(\\widehat{\\beta}) &amp;= \\mathbf{V}( (X^T X) ^{-1} X^T Y)) \\\\ &amp;= \\underbrace{\\mathbf{V}( (X^T X) ^{-1} X^T (X\\beta + \\epsilon))}_\\text{Sub in the expression for Y from above} \\\\ &amp;= \\underbrace{\\mathbf{V}((X^T X) ^{-1} X^T X \\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Distribute the term to the items in the parentheses} \\\\ &amp;= \\underbrace{\\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon)}_\\text{Using the rules of inverses, the two terms next to $\\beta$ canceled each other out} \\end{align*}\\] The next part of the math requires us to use knowledge of the definition of variance and the rules associated. We draw on two in particular: The variance of a constant is zero. When you have a constant multipled by a random variable, e.g., \\(\\mathbf{V}(4d)\\), it can come out of the variance operator, but must be squared: \\(16\\mathbf{V}(d)\\) Putting these together: \\(\\mathbf{V}(2 + 4d)= 16\\mathbf{V}(d)\\) Knowing these rules, we can proceed: \\[\\begin{align*} \\mathbf{V}(\\widehat{\\beta}) &amp;= \\mathbf{V}(\\beta + (X^T X) ^{-1} X^T \\epsilon) \\\\ &amp;=\\underbrace{ \\mathbf{V}((X^T X) ^{-1} X^T \\epsilon)}_\\text{$\\beta$ drops out because in a regression it is an unkown &quot;parameter&quot;-- it&#39;s constant, which means its variance is zero.}\\\\ &amp;= \\underbrace{(X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) ((X^T X)^{-1}X^T)^T}_\\text{We can move $(X^T X)^{-1}X^T$ out front because our data are fixed quantities, but in doing so, we have to &quot;square&quot; the matrix.}\\\\ &amp;= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1} \\end{align*}\\] The resulting quantity is our expression for the \\(\\mathbf{V}(\\hat \\beta)\\). However, in OLS, we make an additional assumption that allows us to further simplify the expression. We assume homoscedasticity aka “constant” or “equal error variance” which says that the variance of the errors are the same across observations: \\(\\mathbf{V}(\\epsilon) = \\sigma^2 I_n\\). If we assume homoscedastic errors, then Var\\((\\epsilon) = \\sigma^2 I_n\\) \\[\\begin{align*} \\mathbf{V}(\\widehat{\\beta}) &amp;= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\ &amp;= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\ &amp;= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.} \\\\ &amp;= \\sigma^2(X^T X)^{-1} \\end{align*}\\] All done! This expression: \\(\\sigma^2(X^T X)^{-1}\\) represents the variance of our coefficient estimates. Note its dimensions: \\(k \\times k\\). It has the same number of rows and columns as the number of our independent variables (plus the intercept). There is one catch, though. How do we know what \\(\\sigma^2\\) is? Well, we don’t. Just like the unknown parameter \\(\\beta\\), we have to estimate it in our regression model. Just like with the coefficients, we notate our estimate as \\(\\widehat{\\sigma}^2\\). Our estimate is based on the observed residual errors in the model and is as follows: \\(\\widehat{\\sigma}^2 = \\frac{1}{N-K}\\sum_{i=1}^N \\widehat{\\epsilon_i^2} = \\frac{1}{N-K} \\mathbf{e&#39;e}\\) That means our estimate of the variance of the coefficients is found within: \\(\\hat \\sigma^2(X^T X)^{-1}\\) Again, this is a \\(k \\times k\\) matrix and is often called the variance covariance matrix. We can extract this quantity from our linear models in R using vcov(). vcov(fit.1) (Intercept) Perot96 (Intercept) 2475.9885768 -1.360074e-01 Perot96 -0.1360074 1.883619e-05 This is the same that we would get if manually we took the residuals and multiplied it by our \\(X\\) matrix according to the formula above: X &lt;- cbind(1, florida$Perot96) e &lt;- cbind(residuals(fit.1)) sigmahat &lt;- ((t(e) %*% e) / (nrow(florida) -2)) ## tell r to stop treating sigmahat as a matrix sigmahat &lt;-as.numeric(sigmahat) XtX &lt;- solve(t(X) %*%X) sigmahat * XtX [,1] [,2] [1,] 2475.9885768 -1.360074e-01 [2,] -0.1360074 1.883619e-05 The terms on the diagonal represent the variance of a particular coefficient in the model.The standard error of a particular coefficient \\(k\\) is: s.e.(\\(\\hat{\\beta_k}) = \\sqrt{\\widehat{\\sigma}^2 (X&#39;X)^{-1}}_{kk}\\). The off-diagonal components represent the covariances between the coefficients. Recall that the standard error is just the square root of the variance. So, to get the nice standard errors we saw in the summary output, we can take the square root of the quantities on the diagonal of this matrix. sqrt(diag(vcov(fit.1))) (Intercept) Perot96 49.759306434 0.004340068 summary(fit.1)$coefficients[,2] (Intercept) Perot96 49.759306434 0.004340068 Why should I care? Well R actually doesn’t make it that easy to extract standard errors from the summary output. You can see above that the code for extracting the standard errors using what we know about them being the square root of the variance is about as efficient as extracting the second column of the coefficient component of the summary of the model. Sometimes, we may think that the assumption of equal error variance is not feasible and that we have unequal error variance or “heteroscedasticity.” Researchers have developed alternative expressions to model unequal error variance. Generally, what this means is they can no longer make that simplifying assumption, have to stop at the step with the uglier expression \\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\) and then assume something different about the structure of the errors in order to estimate the coefficients. These alternative variance estimators are generally what are referred to as “robust standard errors.” There are many different robust estimators, and you will likely come across them in your research. Some of you may have learned the formal, general definition for variance as defined in terms of expected value: \\(\\mathbb{E}[(\\widehat{m} - \\mathbb{E}(\\widehat{m}))^2 ]\\). We could also start the derivation there. This is not required for the course, but it is below if you find it useful. In particular, it can help show why we wend up needing to square a term when we move it outside the variance operator: \\[\\begin{align*} \\mathbf{V}(\\widehat{\\beta}) &amp;= \\mathbb{E}[(\\widehat{\\beta} - \\mathbb{E}(\\hat \\beta))^2)] \\\\ &amp;= \\mathbb{E}[(\\widehat{\\beta} - \\beta)^2]\\\\ &amp;= \\mathbb{E}[(\\widehat{\\beta} - \\beta)(\\widehat{\\beta} - \\beta)^T ] \\\\ &amp;= \\mathbb{E}[(X^T X) ^{-1} X^TY - \\beta)(X^T X) ^{-1} X^TY - \\beta)^T] \\\\ &amp;= \\mathbb{E}[(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)(X^T X) ^{-1} X^T(X\\beta + \\epsilon) - \\beta)^T]\\\\ &amp;= \\mathbb{E}[(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(X^T X) ^{-1} X^TX\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\ &amp;= \\mathbb{E}[(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)(\\beta + (X^T X) ^{-1} X^T\\epsilon - \\beta)^T]\\\\ &amp;= \\mathbb{E}[(X^T X) ^{-1} X^T\\epsilon)(X^T X) ^{-1} X^T\\epsilon)^T]\\\\ &amp;= (X^T X) ^{-1} X^T\\mathbb{E}(\\epsilon\\epsilon^T)X(X^T X) ^{-1}\\\\ &amp;= \\underbrace{(X^T X) ^{-1} X^T\\sigma^2I_nX(X^T X) ^{-1}}_\\text{Assume homoskedasticity}\\\\ &amp;= \\sigma^2(X^T X) ^{-1} X^TX(X^T X) ^{-1}\\\\ &amp;= \\sigma^2(X^T X) ^{-1} \\end{align*}\\] Note: Along the way, in writing \\(\\mathbb{E}(\\hat \\beta) = \\beta\\), we have implicitly assumed that \\(\\hat \\beta\\) is an “unbiased” estimator of \\(\\beta\\). This is not free. It depends on an assumption that the error term in the regression \\(\\epsilon\\) is independent of our independent variables. This can be violated in some situations, such as when we have omitted variable bias, which is discussed at the end of our OLS section. 4.5.2 Hypothesis Testing Most of the time in social science, we run a regression because we have some hypothesis about how a change in our independent variable affects the change in our outcome variable. In OLS, we can perform a hypothesis test for each independent variable in our data. The structure of the hypothesis test is: Null hypothesis: \\(\\beta_k = 0\\) This essentially means that we don’t expect a particular \\(x_k\\) independent variable to have a relationship with our outcome variable. Alternative hypothesis: \\(\\beta_k \\neq 0\\) We do expect a positive or negative relationship between a particular \\(x_k\\) and the dependent variable. We can use our estimates for \\(\\hat \\beta\\) coefficients and their standard errors to come to a conclusion about rejecting or failing to reject the null hypothesis of no relationship by using a t-test. In a t-test, we take our coefficient estimates and divide them by the standard error in order to “standardize” them on a scale that we can use to determine how likely it is we would have observed a value for \\(\\hat \\beta\\) as extreme or more extreme as the one we observed in a world where the true \\(\\beta = 0\\). This is just like a t-test you might have encountered before for a difference in means between groups, except this time our estimate is \\(\\hat \\beta\\). \\[\\begin{align*} t_{\\hat \\beta_k} &amp;= \\frac{\\hat \\beta_k}{s.e.(\\hat \\beta_k)} \\end{align*}\\] Generally speaking, when \\(t\\) is about +/-2 or greater in magnitude, the coefficient will be “significant” at conventional levels (i.e., \\(p &lt;0.05\\)), meaning that we are saying that it is really unlikely we would have observed a value as big as \\(\\hat \\beta_k\\) if the null hypothesis were true. Therefore, we can reject the null hypothesis. However, to get a specific quantity, we need to calculate the p-value, which depends on the t-statistic and the degrees of freedom in the model. The degrees of freedom in a regression model are \\(N-k\\), the number of observations in the model minus the number of independent variables plus the intercept. In R, we can calculate p-values using the pt() function. By default, most people use two-sided hypothesis tests for regression. So to do that, we are going to find the area on each side of the t values, or alternatively, multiply the area to the right of our positive t-value by 2. ## Let&#39;s say t was 2.05 and ## And there were 32 observations and 3 variables in the regression plus an intercept t &lt;- 2.05 df.t &lt;- 32 -4 p.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE)) p.value [1] 0.04983394 Let’s do this for the florida example. First, we can find t by dividing our coefficients by the standard errors. t &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1)))) t (Intercept) Perot96 0.02704523 8.27522567 ## Compare with output summary(fit.1)$coefficients[, 3] (Intercept) Perot96 0.02704523 8.27522567 We can then find the p-values. t &lt;- coef(fit.1) / (sqrt(diag(vcov(fit.1)))) df.t &lt;- fit.1$df.residual p.value &lt;- 2 * (pt(abs(t), df=df.t, lower.tail = FALSE)) p.value (Intercept) Perot96 9.785065e-01 9.473505e-12 summary(fit.1)$coefficients[, 4] (Intercept) Perot96 9.785065e-01 9.473505e-12 We see that the coefficient for Perot96 is significant. The p-value is tiny. In R, for small numbers, R automatically shifts to scientific notation. The 9.47e-12 means the p-value is essentially zero, with the stars in the summary output indicating the p-value is \\(p &lt; 0.001\\). R will also output a test of the significance of the intercept using the same formula as all other coefficients. This generally does not have much interpretive value, so you are usually safe to ignore it. Confidence Intervals Instead of representing the significance using p-values, sometimes it is helpful to report confidence intervals around the coefficients. This can be particularly useful when visualizing the coefficients. The 95% confidence interval represents roughly 2 standard errors above and below the coefficient. The key thing to look for is whether it overlaps with zero (not significant) or does not (in which case the coefficient is significant). The precise formula is \\(\\widehat{\\beta}_k\\) Confidence intervals: \\(\\widehat{\\beta}_k - t_{crit.value} \\times s.e._{\\widehat{\\beta}_k}, \\widehat{\\beta}_k + t_{crit.value} \\times s.e_{\\widehat{\\beta}_k}\\) In R, we can use qt() to get the specific critical value associated with a 95% confidence interval. This will be around 2, but fluctuates depending on the degrees of freedom in your model (which are function of your sample size and how many variables you have in the model.) R also has a shortcut confint() function to extract the coefficients from the model. Below we do this for the Perot96 coefficient. ## Critical values from t distribution at .95 level qt(.975, df = fit.1$df.residual) # n- k degrees of freedom [1] 1.997138 ## Shortcut confint(fit.1)[2,] 2.5 % 97.5 % 0.02724733 0.04458275 ## By hand coef(fit.1)[2] - qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2] Perot96 0.02724733 coef(fit.1)[2] + qt(.975, df = fit.1$df.residual)*sqrt(diag(vcov(fit.1)))[2] Perot96 0.04458275 4.5.3 Goodness of Fit A last noteworthy component to the standard regression output is the goodness of fit statistics. For this class, we can put less attention on these, though there will be some analogues when we get into likelihood. These are measures of how much of the total variation in our outcome measure can be explained by our model, as well as how far off are our estimates from the truth. For the first two measures R-squared and Adjusted R-squared, we draw on three quantities: Total Sum of Squares–how much variance in \\(Y_i\\) is there to explain? \\(TSS: \\sum_{i=1}^N (Y_i -\\overline Y_i)^2\\) Estimated Sum of Squares–how much of this variance do we explain? \\(ESS: \\sum_{i=1}^N (\\widehat Y_i -\\overline Y_i)^2\\) Residual Sum of Squares–how much variance is unexplained? \\(RSS: \\sum_{i=1}^N ( Y_i -\\widehat Y_i)^2\\) \\(TSS = ESS + RSS\\) Multiple R-squared: \\(\\frac{ESS}{TSS}\\) This is a value from 0 to 1, representing the proportion of the variance in the outcome that can be explained by the model. Higher values are generally considered better, but there are many factors that can affect R-squared values. In most social science tasks where the goal is to engage in hypothesis testing of coefficients, this measure is of less value. Adjusted R-squared: \\(1 - \\frac{\\frac{RSS}{n - k}}{\\frac{TSS}{n - 1}}\\) This is essentially a penalized version of R-squared. When you add additional predictors to a model, the R-squared value can never decrease, even if the predictors are useless. The Adjusted R-squared adds a consideration for the degrees of freedom into the equation, creating a penalty for adding more and more predictors. Residual standard error aka root mean squared error aka square root of the mean squared residual: \\(r.s.e = \\sqrt{\\frac{RSS}{n-k}}\\) This represents the typical deviation of an estimate of the outcome from the actual outcome. This quantity is often used to assess the quality of prediction exercises. It is used less often in social science tasks where the goal is hypothesis testing of the relationship between one or more independent variables and the outcome. F-Statistic So far we have conducted hypothesis tests for each individual coefficient. We can also conduct a global hypothesis test, where the null hypothesis is that all coefficients are zero, with the alternative being that at least one coefficient is nonzero. This is the test represented by the F-statistic in the regression output. The F-statistic helps us test the null hypothesis that all of the regression slopes are 0: \\(H_0 = \\beta_1 = \\beta_2 = \\dots = \\beta_k = 0\\) \\(F_0 = \\frac{ESS/(k - 1)}{RSS/(n - k)}\\) The F-Statistic has two separate degrees of freedom. The model sum of squares degrees of freedom (ESS) are \\(k - 1\\). The residual error degrees of freedom (RSS) are \\(n - k\\). In a regression output, the model degrees of freedom are generally the first presented: “F-statistic: 3.595 on \\((k - 1) = 1\\) and \\((n - k) = 48\\) DF.” Note: This test is different from our separate hypothesis tests that a \\(k\\) regression slope is 0. For that, we use the t-tests discussed above. "],
["generating-predictions-from-regression-models.html", "4.6 Generating predictions from regression models", " 4.6 Generating predictions from regression models The regression coefficients tell us how much \\(Y\\) is expected to change for a one-unit change in \\(x_k\\). It does not immediately tell us the values we estimate our outcome (\\(\\hat Y\\)) to take conditional on particular values of \\(x_k\\). While often knowing our independent variables have a significant effect on the outcome and the size of the coefficient is sufficient for testing our hypotheses, it can be helpful for interpretation’s sake, to see the estimated values for the outcome. This is going to be particularly important once we get into models like logistic regression, where the coefficients won’t be immediately interpretable. Recall that our equation for estimating values of our outcomes is: \\(\\hat Y = X\\hat \\beta\\) This can also be written out in long form for any particular observation \\(i\\): \\(\\hat y_i = \\hat \\alpha + \\hat \\beta_1*x_1i + \\hat \\beta_2*x_2i + ... \\hat\\beta_k*x_ki\\) The estimated values of our regression \\(\\hat Y\\) are often called the “fitted values.” In R, you can identify the estimated values for each observation using the fitted() command. ## Y hat for the first observation in the data fitted(fit.1)[1] 1 291.252 Again, this is just the multiplication of the matrix \\(X\\) and \\(\\hat \\beta\\). If we have already run a regression model in R, one shortcut for getting the \\(X\\) matrix, is to use the model.matrix command. We can get \\(\\hat \\beta\\) using the coef() command. X &lt;- model.matrix(fit.1) head(X) # head() shows about the first six values of an object (Intercept) Perot96 1 1 8072 2 1 667 3 1 5922 4 1 819 5 1 25249 6 1 38964 betahat &lt;- coef(fit.1) Our fitted values are then just yhat &lt;- X %*% betahat head(yhat) [,1] 1 291.25196 2 25.30108 3 214.03462 4 30.76017 5 908.16461 6 1400.73939 If I want to generate an estimate for any particular observation, I could also just extract its specific value for Perot96. florida$Perot96[1] [1] 8072 Let’s estimate the Buchanan 2000 votes for the first county in the data with Perot 96 votes of 8072. We can write it out as \\(\\hat Buchanan00_1 =\\hat \\alpha + \\hat \\beta*Perot96_1\\) buch00hat &lt;- coef(fit.1)[1] + coef(fit.1)[2]*florida$Perot96[1] buch00hat (Intercept) 291.252 What is useful about this is that now we have the coefficient estimates, we can apply them to any values of \\(X\\) we wish in order to generate estimates/predictions of the values \\(Y\\) will take given particular values of our independent variables. One function that is useful for this (as a shortcut) is the predict(fit, newdata=newdataframe) function in R. It allows you to enter in “newdata”– meaning values of the \\(X\\) variables for which you want to generate estimates of \\(Y\\) based on the coefficient estimates of your regression model. For example, let’s repeat the calculation from above for Perot96 = 8072. predict(fit.1, newdata = data.frame(Perot96 = 8072)) 1 291.252 We can also generate confidence intervals around these estimates by adding interval = \"confidence\" in the command. predict(fit.1, newdata = data.frame(Perot96 = 8072), interval=&quot;confidence&quot;) fit lwr upr 1 291.252 213.7075 368.7964 We can also simultaneously generate multiple predictions by supplying a vector of values in the predict() command. For example, let’s see the estimated Buchanan votes for when the Perot 1996 votes took values of 1000 to 10,000 by intervals of 1,000. predict(fit.1, newdata = data.frame(Perot96 = c(1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000))) 1 2 3 4 5 6 7 8 37.26079 73.17583 109.09087 145.00591 180.92095 216.83599 252.75104 288.66608 9 10 324.58112 360.49616 The important thing to note about the predict() command is that if you have multiple independent variables, you have to specify the values you want each of them to take when generating the estimated values of y. fit.2 &lt;- lm(Buchanan00 ~ Perot96 + Clinton96, data = florida) For example, let’s build a second model with Clinton96 as an additional predictor. In order to generate the same prediction for different values of Perot 1996 votes, we need to tell R at what values we should “hold constant” Clinton96. I.e., we want to see how hypothetical changes in Perot96 votes influence changes in Buchanan 2000 votes while also leaving the Clinton votes identical. This is that lightswitch metaphor– flipping one switch, while keeping the rest untouched. There are two common approaches to doing this. 1) We can hold constant Clinton96 votes at its mean value in the data 2) We can keep Clinton96 at its observed values in the data. In linear regression, it’s not going to matter which approach you take. In other models we will talk about later, this distinction may matter more substantially because of how our quantities of interest change across different values of \\(X\\hat \\beta\\). The first approach is easily implemented in predict. predict(fit.2, newdata = data.frame(Perot96 = 8072, Clinton96 = mean(florida$Clinton96))) 1 283.997 For the second approach, what we will do is generate an estimate for Buchanan’s votes in 2000 when Perot96 takes 8072 votes, and we keep Clinton96’s votes at whatever value it currently is in the data. That is, we will generate \\(n\\) estimates for Buchanan’s votes when Perot takes 8072. Then, we will take the mean of this as our “average estimate” of Buchanan’s votes in 2000 based on Perot’s votes at a level of 8072. We can do this in one of two ways: ## Manipulating the X matrix X &lt;- model.matrix(fit.2) ## Replace Perot96 column with all 8072 values X[, &quot;Perot96&quot;] &lt;- 8072 head(X) #take a peek (Intercept) Perot96 Clinton96 1 1 8072 40144 2 1 8072 2273 3 1 8072 17020 4 1 8072 3356 5 1 8072 80416 6 1 8072 320736 ## Generate yhat yhats &lt;-X %*% coef(fit.2) ## take the mean mean(yhats) [1] 283.997 ## Use predict yhats &lt;- predict(fit.2, newdata = data.frame(Perot96=8072, Clinton96=florida$Clinton96)) mean(yhats) [1] 283.997 Now often, after we generate these predicted values, we want to display them for the whole world to see. You will get a chance to visualize values like this using the plotting functions in the problem sets. We have already seen one example of this in the simple bivariate case, when R plotted the bivariate regression line in section 4.3.2. However, the predict function extends are capabilities to plot very specific values of \\(X\\) and \\(\\hat Y\\) for bivariate or multiple regressions. The predict() function is also very relevant when we move to logistic, probit, etc. regressions. This is just the start of a beautiful friendship between you and predict() and associated functions. "],
["wrapping-up-ols.html", "4.7 Wrapping up OLS", " 4.7 Wrapping up OLS Linear regression is a great way to explain the relationship between one or more independent variables and an outcome variables. However, there is no free lunch. We have already mentioned a couple of assumptions along the way. Below we will summarize these and other assumptions. These are things you should be mindful of when you use linear regression in your own work. Some conditions that generate violations of these assumptions can also motivate why we will seek out alternative methods, such as those that rely on maximum likelihood estimation. This is a good place to review Gelman section 3.6. Exogeneity. This one we haven’t discussed yet, but is an important assumption for letting us interpret our coefficients \\(\\hat \\beta\\) as “unbiased” estimates of the true parameters \\(\\beta\\). We assume that the random error in the regression model \\(\\epsilon\\) is indeed random, and uncorrelated with and independent of our independent variables \\(X\\). Formally: \\(\\mathbb{E}(\\epsilon| X) = \\mathbb{E}(\\epsilon) = 0\\). This can be violated, for example, when we suffer from Omitted Variable Bias due to having an “endogenous explanatory variable” that is correlated with some unobserved or unaccounted for factor. This bias comes from a situation where there is some variable that we have left out of the model (\\(Z\\)), and is therefore a part of the unobserved error term. Moreover this variable which is correlated with–and a pre-cursor of– our independent variables and is a cause of our dependent variable. A failure to account for omitted variables can create bias in our coefficient estimates. Concerns about omitted variable bias often prompt people to raise their hands in seminars and ask questions like, “Well have you accounted for this? Have you accounted for that? How do you know it is \\(X\\) driving your results and not \\(Z\\)?” If we omit important covariates, we may wrongly attribute an effect to \\(X\\) when it was really the result of our omitted factor \\(Z\\). Messing discusses this here. This is a really tough assumption. The only real way to guarantee the independence of your error term and the independent variables is if you have randomly assigned values to the independent variables (such as what you do when you randomly assign people to different treatment conditions in an experiment). Beyond random assignment, you have to rely on theory to understand what variables you need to account for in the regression model to be able to plausibly claim your estimate of the relationship between a given independent variable and the dependent variable is unbiased. Failing to control for important factors can lead to misleading results, such as what happens in Simpson’s paradox, referenced in the Messing piece. Danger Note 1: The danger here, though, is that the motivation for avoiding omitted variable bias might be to keep adding control after control after control into the regression model. However, model building in this way can sometimes be atheoretical and result in arbitrary fluctuations in the size of your coefficients and their significance. At its worse, it can lead to “p-hacking” where researchers keep changing their models until they find the results they like. The Lenz and Sahn article on Canvas talks more about the dangers of arbitrarily adding controls to the model. Danger Note 2: We also want to avoid adding “bad controls” to the model. Messing talks about this in the medium article as it relates to collider bias. We want to avoid adding controls to our model, say \\(W\\) that are actually causes of \\(Y\\) and causes of \\(X\\) instead of the other way around. Model building is a delicate enterprise that depends a lot on having a solid theory that guides the choice of variables. Homoscedasticity. We saw this when defining the variance estimator for the OLS coefficients. We assume constant error variance. This can be violated when we think observations at certain values of our independent variables may have different magnitudes of error than observations at other values of our independent variables. No correlation in the errors. The error terms are not correlated with each other. This can be violated in time series models (where we might think past, present, and future errors are correlated) or in cases where our observations are nested in some hierarchical structures (e.g., perhaps students in a school) and the errors are correlated. No perfect collinearity. The \\(X\\) matrix must be full rank: We cannot have linear dependence between columns in our X matrix. We saw this in the tutorial when we tried to add the dummy variables for all of our racial groups into a regression at once. When there is perfect collinearity between variables, our regression will fail. We should also avoid situations where we have severe multicollinearity. This can happen when we include two or more variables in a regression model that are highly correlated (just not perfectly correlated). While the regression will still run in this case, it can inflate the standard errors of the coefficients, making it harder to detect significant effects. This is particularly problematic in smaller samples. Linearity. The relationship between the independent and dependent variables needs to be linear in the parameters. It should be modeled as the addition of constants or parameters multiplied by the independent variables. If instead the model requires the multiplication of parameters, this is no longer linear (e.g., \\(\\beta^2\\)). Linearity also often refers to the shape of the model. Our coefficients tell us how much change we expect in the outcome for each one-unit change in an independent variable. We might think some relationships are nonlinear– meaning this rate of change varies across values of the independent variables. If that is the case, we need to shift the way our model is specified to account for this or change modeling approaches. For example, perhaps as people get older (one-unit changes in age), they become more politically engaged, but at some age level, their political engagement starts to decline. This would mean the slope (that expected change in political engagement for each one-unit change in age) is not constant across all levels of age. There, we might be violating linearity in the curvature of the relationship between the independent and dependent variables. This is sometimes why you might see \\(age^2\\) or other nonlinear terms in regression equations to better model this curvature. Likewise, perhaps each additional level of education doesn’t result in the same average increase in \\(y\\). If not, you could consider including categorical dummy variables for different levels of education instead of treating education as a numeric variable. Normality. We assume that the errors are normally distributed. As Gelman 3.6 notes, this is a less important assumption and is generally not required. OLS Properties Why we like OLS. When we meet our assumptions, OLS produces the best linear unbiased estimates (BLUE). A discussion of this here. We have linearity in our parameters (e.g., \\(\\beta\\) and not \\(\\beta^2\\) for example). The unbiasedness means that the expected value (aka the average over repeated samples) of our estimates \\(\\mathbb{E}(\\hat \\beta)= \\beta\\) is the true value. Our estimates are also efficient, which has to do with the variance, not only are our estimates true in expectation, but we also have lower variance than an alternative linear unbiased estimator could get us. If our assumptions fail, then we might no longer have BLUE. OLS estimates are also consistent, meaning that as the sample gets larger and larger, the estimates start converging to the truth. Now, a final hidden assumption in all of this is that the sample of our data is representative of the population we are trying to make inferences about. If that is not the case, then we may no longer be making unbiased observations to that population level. Further adjustments may be required (e.g., analyses of survey data sometimes use weights to adjust estimates to be more representative). When we violate these assumptions, OLS may no longer be best, and we may opt for other approaches. More soon! 4.7.1 Practice Problems Let’s use the florida data. Run a regression according to the following formula: \\(Buchanan00_i = \\alpha + \\beta_1*Perot96_i + \\beta_2*Dole96 + \\beta_3*Gore00 + \\epsilon\\) Report the coefficient for Perot96. What do you conclude about the null hypothesis that there is no relationship between 1996 Perot votes and 2000 Buchanan votes? What is the confidence interval for the Perot96 coefficient estimate? When Perot 1996 vote is 5500, what is the expected 2000 Buchanan vote? 4.7.2 Practice Problem Code for Solutions fit.practice &lt;- lm(Buchanan00 ~ Perot96 + Dole96 + Gore00, data = florida) coef(fit.practice)[&quot;Perot96&quot;] Perot96 0.02878927 confint(fit.practice)[&quot;Perot96&quot;, ] 2.5 % 97.5 % 0.004316382 0.053262150 expbuch &lt;- model.matrix(fit.practice) expbuch[,&quot;Perot96&quot;] &lt;- 5500 mean(expbuch %*% as.matrix(coef(fit.practice))) [1] 211.1386 "],
["week-2-thursday-example.html", "4.8 Week 2 Thursday Example", " 4.8 Week 2 Thursday Example This example is based on Dancygier, Rafaela; Egami, Naoki; Jamal, Amaney; Rischke, Ramona, 2020, “Hate Crimes and Gender Imbalances: Fears over Mate Competition and Violence against Refugees”, published in the American Journal of Political Science. Replication data is available here. We will draw on the survey portion of the article and replicate Table 1 in the paper. The pre-print is available here. The abstract is: As the number of refugees rises across the world, anti-refugee violence has become a pressing concern. What explains the incidence and support of such hate crime? We argue that fears among native men that refugees pose a threat in the competition for female partners is a critical but understudied factor driving hate crime. Employing a comprehensive dataset on the incidence of hate crime across Germany, we first demonstrate that hate crime rises where men face disadvantages in local mating markets. Next, we complement this ecological evidence with original survey measures and confirm that individual-level support for hate crime increases when men fear that the inflow of refugees makes it more difficult to find female partners. Mate competition concerns remain a robust predictor even when controlling for antirefugee views, perceived job competition, general frustration, and aggressiveness. We conclude that a more complete understanding of hate crime and immigrant conflict must incorporate marriage markets and mate competition. The authors summarize their hypotheses as, “the notion that male refugees are engaged in romantic relationships with German women has received considerable media attention from a variety of sources, with coverage ranging from the curious to the outright hostile. We argue that the prospect of refugee-native mate competition can trigger or compound resentment against refugees, including support for hate crime” pg. 14 library(foreign) dat_use &lt;- read.dta(&quot;https://github.com/ktmccabe/teachingdata/blob/main/dat_use.dta?raw=true&quot;) The data include wave 4 of an online survey fielded in Germany through Respondi from September 2016 to December 2017). Each wave was designed to be nationally representative on age (starting at 18), gender, and state (Bundesland) with a sample of about 3,000 respondents in each wave. Key variables include hate_violence_means representing respondents’ agreement or disagreement to the Only Means question: “When it comes to the refugee problem, violence is sometimes the only means that citizens have to get the attention of German politicians.” from (1) disagree strongly to (4) agree strongly. MateComp_cont, Mate Competition. “The inflow of refugees makes it more difficult for native men to find female partners.” from (1) disagree strongly to (4) agree strongly. The data include several other variables related to the demographics of the respondents and measures representing potential alternative explanations, such as JobComp_cont (agreement with “the inflow of young male refugees makes it more difficult for young native men to find apprenticeships and jobs”) and LifeSatis_cont (0-10 scale, ranging from extremely dissatisfied to extremely satisfied). Let’s pause here to ask a few questions about research design. What is the outcome? What is the independent variable of interest? How would we write out the bivariate regression model? Why OLS? (e.g., why not experiment?) What types of alternative explanations might exist? Ok let’s move to replication of the first two regression models in the table: Try to code these on your own, then click for the solution lm1 &lt;- lm(hate_violence_means ~ MateComp_cont, data=dat_use) lm2 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data=dat_use) Now, let’s compare the summary output of each output. Try on your own, then click for the solution summary(lm1) Call: lm(formula = hate_violence_means ~ MateComp_cont, data = dat_use) Residuals: Min 1Q Median 3Q Max -1.6804 -0.3694 -0.3694 0.6306 2.6306 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.93235 0.03302 28.24 &lt;2e-16 *** MateComp_cont 0.43702 0.01635 26.73 &lt;2e-16 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.7993 on 3017 degrees of freedom Multiple R-squared: 0.1915, Adjusted R-squared: 0.1912 F-statistic: 714.6 on 1 and 3017 DF, p-value: &lt; 2.2e-16 summary(lm2) Call: lm(formula = hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont, data = dat_use) Residuals: Min 1Q Median 3Q Max -1.8275 -0.4783 -0.1842 0.3171 2.8452 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.788623 0.057849 13.63 &lt;2e-16 *** MateComp_cont 0.263437 0.020261 13.00 &lt;2e-16 *** JobComp_cont 0.249956 0.018672 13.39 &lt;2e-16 *** LifeSatis_cont -0.014725 0.006292 -2.34 0.0193 * --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Residual standard error: 0.7751 on 3015 degrees of freedom Multiple R-squared: 0.2403, Adjusted R-squared: 0.2395 F-statistic: 317.9 on 3 and 3015 DF, p-value: &lt; 2.2e-16 Questions about the output How should we interpret the coefficients? Do they support the researchers’ hypotheses? How would we extract confidence intervals from the coefficients? How should we interpret the goodness of fit statistics at the bottom of the output? Additional Models We can also run regressions with even more covariates, as the authors do in models 3-6 in the paper. Click to reveal regression code below. lm3 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + factor(age_group) + # age group factor(gender) + # gender factor(state) + # state factor(citizenship) + # german citizen factor(marital) + # marital status factor(religion) + # religious affiliation eduyrs + # education factor(occupation) + # main activity factor(income) + # income factor(household_size) + # household size factor(self_econ), # subjective social status data=dat_use) lm4 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + factor(age_group) + # age group factor(gender) + # gender factor(state) + # state factor(citizenship) + # german citizen factor(marital) + # marital status factor(religion) + # religious affiliation eduyrs + # education factor(occupation) + # main activity factor(income) + # income factor(household_size) + # household size factor(self_econ) + # subjective social status factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + factor(ref_economy) + factor(ref_crime) + factor(ref_terror), data=dat_use) lm5 &lt;- lm(hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + factor(age_group) + # age group factor(gender) + # gender factor(state) + # state factor(citizenship) + # german citizen factor(marital) + # marital status factor(religion) + # religious affiliation eduyrs + # education factor(occupation) + # main activity factor(income) + # income factor(household_size) + # household size factor(self_econ) + # subjective social status factor(ref_integrating) + # Refugee Index (National-level; Q73) 8 in total factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + factor(ref_loc_services) + # Refugee Index (Local, Q75) factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife), ## end data=dat_use) formula.5 &lt;- as.character(&quot;hate_violence_means ~ MateComp_cont + JobComp_cont + LifeSatis_cont + factor(age_group) + factor(gender) + factor(state) + factor(citizenship) + factor(marital) + factor(religion) + eduyrs + factor(occupation) + factor(income) + factor(household_size) + factor(self_econ) + factor(ref_integrating) + factor(ref_citizenship) + factor(ref_reduce) + factor(ref_moredone) + factor(ref_cultgiveup) + factor(ref_economy) + factor(ref_crime) + factor(ref_terror) + factor(ref_loc_services) + factor(ref_loc_economy) + factor(ref_loc_crime) + factor(ref_loc_culture) + factor(ref_loc_islam) + factor(ref_loc_schools) + factor(ref_loc_housing) + factor(ref_loc_wayoflife)&quot;) formula.6 &lt;- paste(formula.5, &quot;factor(distance_ref) + factor(settle_ref)&quot;, &quot;lrscale + afd + muslim_ind + afd_ind + contact_ind&quot;, sep=&quot;+&quot;, collapse=&quot;+&quot;) lm6 &lt;- lm(as.formula(formula.6), data=dat_use) Table 1: Mate Competition Predicts Support for Hate Crime. Model 1 Model 2 Model 3 Model 4 Model 5 Model 6 (Intercept) 0.9323*** 0.7886*** 1.3982*** 1.4437*** 1.4372*** 1.3878*** (0.0330) (0.0578) (0.2293) (0.2296) (0.2388) (0.2372) MateComp_cont 0.4370*** 0.2634*** 0.2361*** 0.2064*** 0.1848*** 0.1550*** (0.0163) (0.0203) (0.0206) (0.0194) (0.0195) (0.0189) JobComp_cont 0.2500*** 0.2358*** 0.0772*** 0.0650*** 0.0559*** (0.0187) (0.0189) (0.0195) (0.0196) (0.0189) LifeSatis_cont -0.0147** -0.0136* -0.0034 -0.0020 -0.0001 (0.0063) (0.0070) (0.0065) (0.0065) (0.0062) factor(age_group)30-39 -0.1323** -0.1800*** -0.1821*** -0.1957*** (0.0525) (0.0489) (0.0488) (0.0471) factor(age_group)40-49 -0.2088*** -0.2771*** -0.2709*** -0.2808*** (0.0525) (0.0490) (0.0490) (0.0474) factor(age_group)50-59 -0.2876*** -0.3621*** -0.3480*** -0.3580*** (0.0535) (0.0501) (0.0502) (0.0486) factor(age_group)60 and older -0.3362*** -0.3427*** -0.3199*** -0.3073*** (0.0678) (0.0631) (0.0631) (0.0610) factor(gender)Female -0.0247 -0.0528* -0.0451 -0.0233 (0.0299) (0.0281) (0.0282) (0.0272) factor(state)Bayern 0.0097 -0.0168 -0.0148 -0.0229 (0.0531) (0.0494) (0.0491) (0.0474) factor(state)Berlin 0.0106 -0.0023 -0.0259 0.0037 (0.0776) (0.0722) (0.0720) (0.0706) factor(state)Brandenburg -0.1572* -0.1023 -0.0949 -0.1082 (0.0896) (0.0833) (0.0834) (0.0805) factor(state)Bremen -0.1266 -0.1252 -0.1750 -0.0508 (0.1531) (0.1423) (0.1415) (0.1365) factor(state)Hamburg -0.0208 -0.0140 -0.0255 -0.0269 (0.1016) (0.0946) (0.0941) (0.0914) factor(state)Hessen -0.1207* -0.0931 -0.0766 -0.0853 (0.0647) (0.0604) (0.0601) (0.0578) factor(state)Mecklenburg-Vorpommern -0.0849 -0.1008 -0.1015 -0.1572* (0.1035) (0.0961) (0.0959) (0.0928) factor(state)Niedersachsen -0.0993 -0.1052* -0.1055* -0.1190** (0.0607) (0.0564) (0.0561) (0.0543) factor(state)Nordrhein-Westfalen -0.0299 -0.0277 -0.0414 -0.0414 (0.0501) (0.0465) (0.0465) (0.0450) factor(state)Rheinland-Pfalz -0.1178 -0.1137 -0.1089 -0.1407** (0.0750) (0.0700) (0.0697) (0.0675) factor(state)Saarland -0.0264 0.0227 0.0353 -0.0250 (0.1293) (0.1203) (0.1199) (0.1162) factor(state)Sachsen -0.0357 -0.0813 -0.1118 -0.1470** (0.0734) (0.0683) (0.0684) (0.0662) factor(state)Sachsen-Anhalt -0.0193 -0.0811 -0.0765 -0.1024 (0.0927) (0.0862) (0.0863) (0.0836) factor(state)Schleswig-Holstein -0.2402*** -0.1693** -0.1725** -0.1839** (0.0862) (0.0806) (0.0802) (0.0773) factor(state)Thringen 0.0090 -0.0076 -0.0081 -0.0654 (0.0957) (0.0889) (0.0887) (0.0858) factor(citizenship)1 -0.0621 -0.0831 -0.0739 -0.0314 (0.1064) (0.0990) (0.0983) (0.0946) factor(marital)With partner, not living together 0.0825 0.0323 0.0145 0.0099 (0.0572) (0.0532) (0.0529) (0.0509) factor(marital)With partner, living together 0.0968* 0.0570 0.0582 0.0342 (0.0562) (0.0524) (0.0521) (0.0501) factor(marital)Married 0.0884 0.0487 0.0509 0.0165 (0.0538) (0.0500) (0.0497) (0.0479) factor(marital)Registered partnership 0.0982 0.1345 0.1601 0.1976 (0.1880) (0.1753) (0.1744) (0.1679) factor(marital)Divorced / separated 0.1150* 0.0938 0.0853 0.0877 (0.0616) (0.0573) (0.0569) (0.0549) factor(marital)Widowed 0.1612* 0.1556* 0.1309 0.1243 (0.0920) (0.0855) (0.0855) (0.0824) factor(religion)Roman Catholic -0.0034 -0.0300 -0.0333 -0.0713* (0.0407) (0.0378) (0.0377) (0.0364) factor(religion)Protestant -0.0640* -0.0396 -0.0249 -0.0556* (0.0375) (0.0348) (0.0349) (0.0337) factor(religion)Protestant Free Church -0.0225 -0.0240 -0.0170 -0.0780 (0.1022) (0.0951) (0.0945) (0.0911) factor(religion)Other Protestant 0.7822** 0.9286*** 0.9234*** 0.8768** (0.3855) (0.3587) (0.3565) (0.3441) factor(religion)Eastern Orthodox 0.2751 0.1666 0.1350 0.1455 (0.1869) (0.1744) (0.1735) (0.1672) factor(religion)Other Christian -0.0119 0.0406 0.0645 0.0954 (0.1633) (0.1518) (0.1514) (0.1458) factor(religion)Jewish 0.0827 -0.1329 -0.0855 -0.2074 (0.3460) (0.3217) (0.3202) (0.3081) factor(religion)Muslim -0.0578 0.0586 0.0046 0.0906 (0.1667) (0.1554) (0.1549) (0.1509) factor(religion)Eastern religion (Buddhism, Hinduism, Sikhism, Shinto, Tao, etc.) -0.0026 0.0043 0.0289 0.0138 (0.1215) (0.1130) (0.1129) (0.1086) factor(religion)Other non-Christian religion 0.3904* 0.4759** 0.4412** 0.2675 (0.2333) (0.2175) (0.2168) (0.2089) factor(religion)Christian, but not close to a particular religious community -0.0270 0.0102 0.0083 -0.0177 (0.0611) (0.0567) (0.0566) (0.0544) factor(religion)No answer 0.1273 0.2257** 0.2106** 0.1926* (0.1116) (0.1039) (0.1036) (0.0999) eduyrs -0.0179*** -0.0139*** -0.0121*** -0.0088** (0.0042) (0.0039) (0.0039) (0.0038) factor(occupation)Parental leave 0.1940 0.1368 0.1319 0.1606 (0.1589) (0.1478) (0.1473) (0.1417) factor(occupation)In schooling / vocational training, student -0.0690 -0.1196 -0.1279 -0.1244 (0.0926) (0.0862) (0.0859) (0.0828) factor(occupation)Unemployed / seeking work 0.0088 -0.0339 -0.0420 -0.0451 (0.1080) (0.1004) (0.0999) (0.0961) factor(occupation)Retired 0.1055 0.0493 0.0470 0.0298 (0.0816) (0.0758) (0.0755) (0.0726) factor(occupation)Permanently sick or disabled -0.1574 -0.1490 -0.1458 -0.1507 (0.1287) (0.1198) (0.1194) (0.1149) factor(occupation)Unskilled worker 0.1800* 0.0893 0.0642 0.0221 (0.0934) (0.0870) (0.0865) (0.0833) factor(occupation)Skilled worker 0.2026** 0.1379* 0.1393* 0.1073 (0.0871) (0.0812) (0.0808) (0.0778) factor(occupation)Employee in low / medium position 0.1065 0.0560 0.0524 0.0653 (0.0740) (0.0688) (0.0685) (0.0659) factor(occupation)Employee in high position 0.0536 -0.0116 -0.0165 -0.0292 (0.0824) (0.0766) (0.0763) (0.0735) factor(occupation)Civil servant -0.0009 -0.1061 -0.1342 -0.1687 (0.1307) (0.1214) (0.1209) (0.1166) factor(occupation)Senior civil servant 0.0173 0.0085 0.0262 -0.0256 (0.1481) (0.1380) (0.1372) (0.1325) factor(occupation)Senior civil servant &lt;96&gt; highest level -0.0083 -0.1016 -0.0793 -0.0611 (0.1262) (0.1174) (0.1169) (0.1127) factor(occupation)Self-employed / freelancer 0.1171 0.0323 0.0396 0.0693 (0.0889) (0.0828) (0.0824) (0.0794) factor(occupation)Other 0.2269 0.0467 0.0232 0.0080 (0.1724) (0.1607) (0.1601) (0.1540) factor(income)500 to below 1,000 &lt;80&gt; 0.0260 0.0768 0.0750 -0.0028 (0.1024) (0.0953) (0.0948) (0.0914) factor(income)1,000 to below 1,500 &lt;80&gt; 0.0677 0.0714 0.0642 -0.0274 (0.1011) (0.0943) (0.0937) (0.0904) factor(income)1,500 to below 2,000 &lt;80&gt; 0.1360 0.1289 0.1319 0.0564 (0.1024) (0.0955) (0.0949) (0.0914) factor(income)2,000 to below 2,500 &lt;80&gt; 0.1320 0.1155 0.1146 0.0028 (0.1045) (0.0974) (0.0969) (0.0935) factor(income)2,500 to below 3,000 &lt;80&gt; 0.0479 0.0606 0.0615 -0.0466 (0.1071) (0.0998) (0.0992) (0.0956) factor(income)3,000 to below 3,500 &lt;80&gt; 0.1659 0.1531 0.1557 0.0384 (0.1108) (0.1031) (0.1025) (0.0989) factor(income)3,500 to below 4,000 &lt;80&gt; 0.2256** 0.2133** 0.2101** 0.0785 (0.1138) (0.1059) (0.1054) (0.1017) factor(income)4,000 to below 4,500 &lt;80&gt; 0.0770 0.0396 0.0271 -0.0996 (0.1211) (0.1127) (0.1121) (0.1081) factor(income)4,500 to below 5,000 &lt;80&gt; 0.2446* 0.1782 0.1755 0.0431 (0.1274) (0.1188) (0.1182) (0.1140) factor(income)5,000 or more &lt;80&gt; 0.2017 0.1350 0.1128 0.0250 (0.1227) (0.1143) (0.1136) (0.1095) factor(income)No answer 0.0325 0.0498 0.0501 -0.0453 (0.1062) (0.0990) (0.0984) (0.0949) factor(household_size)2 0.0362 0.0390 0.0316 0.0617 (0.0511) (0.0475) (0.0473) (0.0457) factor(household_size)3 0.0404 0.0374 0.0403 0.0675 (0.0574) (0.0535) (0.0533) (0.0515) factor(household_size)4 0.0289 0.0129 0.0114 0.0516 (0.0647) (0.0602) (0.0599) (0.0578) factor(household_size)5 0.0161 0.0255 0.0347 0.0345 (0.1046) (0.0972) (0.0968) (0.0934) factor(household_size)6 0.3162* 0.3629** 0.4030** 0.3646** (0.1793) (0.1666) (0.1658) (0.1599) factor(household_size)7 0.0387 0.0311 0.0495 -0.0145 (0.3181) (0.2957) (0.2939) (0.2838) factor(household_size)8 0.7654** 0.9534*** 0.8352** 0.7004** (0.3876) (0.3615) (0.3615) (0.3479) factor(household_size)12 -0.0289 0.0946 -0.0435 -0.1011 (0.7761) (0.7217) (0.7188) (0.6915) factor(self_econ)2 -0.0271 -0.1166 -0.1141 -0.0368 (0.1629) (0.1517) (0.1511) (0.1457) factor(self_econ)3 -0.2019 -0.2069 -0.2058 -0.1859 (0.1513) (0.1408) (0.1405) (0.1353) factor(self_econ)4 -0.1501 -0.1394 -0.1358 -0.1343 (0.1510) (0.1405) (0.1403) (0.1351) factor(self_econ)5 -0.2279 -0.1700 -0.1705 -0.1569 (0.1491) (0.1389) (0.1386) (0.1335) factor(self_econ)6 -0.2812* -0.2191 -0.2186 -0.2051 (0.1503) (0.1400) (0.1397) (0.1345) factor(self_econ)7 -0.3444** -0.2527* -0.2484* -0.2383* (0.1518) (0.1415) (0.1414) (0.1360) factor(self_econ)8 -0.2107 -0.1598 -0.1765 -0.1973 (0.1573) (0.1466) (0.1462) (0.1407) factor(self_econ)9 -0.1747 -0.0476 -0.0684 -0.0658 (0.1933) (0.1804) (0.1801) (0.1731) factor(self_econ)10 ( TOP ) 0.3679 0.2960 0.2701 0.2253 (0.2349) (0.2192) (0.2183) (0.2100) factor(self_econ)0 ( BOTTOM ) -0.0023 -0.0450 -0.0278 0.0017 (0.2077) (0.1933) (0.1925) (0.1853) factor(ref_integrating)2 -0.0585 -0.0390 -0.0304 (0.0913) (0.0924) (0.0890) factor(ref_integrating)3 -0.0921 -0.0692 -0.0772 (0.0935) (0.0951) (0.0918) factor(ref_integrating)4 0.0787 0.0928 0.0585 (0.0998) (0.1015) (0.0980) factor(ref_citizenship)2 0.0020 -0.0202 -0.0245 (0.0444) (0.0447) (0.0429) factor(ref_citizenship)3 0.0893* 0.0720 0.0349 (0.0493) (0.0497) (0.0480) factor(ref_citizenship)4 0.1626*** 0.1425** 0.1000* (0.0571) (0.0581) (0.0561) factor(ref_reduce)2 -0.0253 -0.0103 -0.0041 (0.0595) (0.0599) (0.0576) factor(ref_reduce)3 0.0162 0.0326 -0.0106 (0.0636) (0.0644) (0.0623) factor(ref_reduce)4 0.0354 0.0465 -0.1045 (0.0744) (0.0754) (0.0736) factor(ref_moredone)2 0.0930* 0.0802* 0.0559 (0.0482) (0.0484) (0.0467) factor(ref_moredone)3 0.1947*** 0.1834*** 0.0920* (0.0528) (0.0532) (0.0520) factor(ref_moredone)4 0.3050*** 0.2874*** 0.1561** (0.0618) (0.0623) (0.0609) factor(ref_cultgiveup)2 -0.0125 -0.0212 -0.0309 (0.0586) (0.0590) (0.0569) factor(ref_cultgiveup)3 0.0145 -0.0054 -0.0505 (0.0582) (0.0587) (0.0568) factor(ref_cultgiveup)4 0.1507** 0.1281* 0.0733 (0.0652) (0.0656) (0.0636) factor(ref_economy)2 0.0237 0.0456 0.0510 (0.0549) (0.0582) (0.0561) factor(ref_economy)3 -0.0004 0.0343 0.0145 (0.0606) (0.0663) (0.0638) factor(ref_economy)4 0.1657** 0.2379*** 0.1524** (0.0702) (0.0778) (0.0750) factor(ref_crime)2 0.0183 -0.0033 -0.0037 (0.0606) (0.0645) (0.0620) factor(ref_crime)3 0.0794 -0.0061 -0.0290 (0.0661) (0.0715) (0.0688) factor(ref_crime)4 0.2506*** 0.1343 0.0431 (0.0774) (0.0835) (0.0806) factor(ref_terror)2 -0.0689 -0.0975* -0.1060* (0.0568) (0.0578) (0.0556) factor(ref_terror)3 -0.0330 -0.0818 -0.1054* (0.0608) (0.0617) (0.0595) factor(ref_terror)4 -0.0338 -0.0865 -0.1144* (0.0707) (0.0715) (0.0689) factor(ref_loc_services)2 0.0852 0.0888 (0.0680) (0.0653) factor(ref_loc_services)3 0.0765 0.0788 (0.0681) (0.0655) factor(ref_loc_services)4 0.0577 0.0699 (0.0761) (0.0732) factor(ref_loc_economy)2 -0.1186* -0.1209* (0.0718) (0.0690) factor(ref_loc_economy)3 -0.1420* -0.1562** (0.0767) (0.0738) factor(ref_loc_economy)4 -0.2892*** -0.2972*** (0.0864) (0.0832) factor(ref_loc_crime)2 0.0813 0.0727 (0.0582) (0.0560) factor(ref_loc_crime)3 0.2474*** 0.2050*** (0.0662) (0.0640) factor(ref_loc_crime)4 0.3110*** 0.2766*** (0.0800) (0.0774) factor(ref_loc_culture)2 0.0051 0.0068 (0.0524) (0.0507) factor(ref_loc_culture)3 0.0297 0.0078 (0.0612) (0.0594) factor(ref_loc_culture)4 0.1232* 0.0013 (0.0737) (0.0718) factor(ref_loc_islam)2 -0.0128 -0.0085 (0.0531) (0.0511) factor(ref_loc_islam)3 -0.0085 -0.0453 (0.0550) (0.0531) factor(ref_loc_islam)4 -0.0281 -0.1068* (0.0660) (0.0638) factor(ref_loc_schools)2 0.1254 0.1084 (0.0887) (0.0853) factor(ref_loc_schools)3 0.0552 0.0573 (0.0832) (0.0801) factor(ref_loc_schools)4 -0.0806 -0.0809 (0.0845) (0.0814) factor(ref_loc_housing)2 0.0134 0.0095 (0.0586) (0.0566) factor(ref_loc_housing)3 0.0068 0.0008 (0.0564) (0.0547) factor(ref_loc_housing)4 0.0432 0.0433 (0.0608) (0.0590) factor(ref_loc_wayoflife)2 -0.0586 -0.0653 (0.0621) (0.0597) factor(ref_loc_wayoflife)3 -0.0341 -0.0515 (0.0609) (0.0586) factor(ref_loc_wayoflife)4 0.0694 0.0550 (0.0707) (0.0682) factor(distance_ref)3-5 kilometers -0.0375 (0.0362) factor(distance_ref)6-10 kilometers 0.0256 (0.0405) factor(distance_ref)11-20 kilometers 0.0165 (0.0494) factor(distance_ref)21-50 kilometers 0.0644 (0.0534) factor(distance_ref)More than 50 kilometer 0.0568 (0.0794) factor(distance_ref)Don’t know -0.0389 (0.0418) factor(settle_ref)1 &lt;96&gt; 49 -0.0133 (0.0668) factor(settle_ref)50 &lt;96&gt; 249 -0.0178 (0.0661) factor(settle_ref)250 &lt;96&gt; 499 -0.0455 (0.0694) factor(settle_ref)500 &lt;96&gt; 999 -0.0213 (0.0732) factor(settle_ref)1000 and more -0.0536 (0.0687) lrscale 0.0235*** (0.0078) afd 0.0044*** (0.0006) muslim_ind 0.3152*** (0.0701) afd_ind 0.3390*** (0.0489) contact_ind 0.0741 (0.0522) R2 0.1915 0.2403 0.2883 0.3942 0.4097 0.4592 Adj. R2 0.1912 0.2395 0.2673 0.3712 0.3821 0.4308 Num. obs. 3019 3019 3008 3008 3008 3008 p &lt; 0.01; p &lt; 0.05; p &lt; 0.1 Final Questions Even with all these covariates accounted for, the authors still engage in a discussion about possible violations of the OLS assumptions that could bias their results, as well as potential alternative modelling strategies. Is their survey representative? They replicate using another polling firm. Are there even more alternative explanations? Is OLS the right choice? Validity (discussed in Gelman and Hill). Does the outcome accurately measure the concept? They consider alternative outcomes and visualize the coefficient results in Figure 4. Message: Attacks against refugee homes are sometimes necessary to make it clear to politicians that we have a refugee problem. Justified : Hostility against refugees is sometimes justified, even when it ends up in violence. Prevent : Xenophobic acts of violence are defensible if they result in fewer refugees settling in town. Condemn: Politicians should condemn attacks against refugees more forcefully. Additional Practice Questions. Find the average expected level of “Only Means” agreement at each level of mate competition. Plot the results. Base these results on lm2. Fit lm2 using the generalized linear model glm approach (with a normal distribution) instead of the lm What are some of the conceptual differences between ordinary least squares and maximum likelihood estimation? "],
["mle.html", "Section 5 Introduction to MLE", " Section 5 Introduction to MLE This section will provide an overview of MLE. There are a few general connections we can make between the things we know (the linear least squares model) and the concepts introduced here. Practical Uses: Going from nice continuous outcome data to outcome data generated differently Estimation: Going from minimizing squared error to maximizing likelihood. Both involve optimization. In likelihood, we are trying to find the optimal values of parameters for a distribution given observed data. Formulation: Going from Linear Model to Generalized Linear Model Mechanics in R: Going from lm() to glm() and its friends. The first sections will focus on drawing out these connections. We will then get into the details on the derivations for common methods. This video is an overview of some of the concepts we discuss. Don’t worry about the mathematical details in the video. For a given model, we will go into greater depth in the future. Focus on the general concepts and process. The notes to follow in sections 5.1 and 5.2 elaborate on the concepts in the video. I also encourage you to watch this video from StatQuest to get an initial understanding of likelihood and what it means to “choose the maximum likelihood” using a visual example. StatQuest video These concepts are also addressed in: King, Gary. 1998. Unifying political methodology: The likelihood theory of statistical inference. University of Michigan Press. Note: Available as an electronic resource through the Rutgers library. Chapters 1, 2, 3, 4.6-4.8. (Available online through Rutgers University) "],
["what-is-likelihood.html", "5.1 What is likelihood?", " 5.1 What is likelihood? Just like the derivation of the OLS coefficient estimators \\(\\hat \\beta\\) for the \\(\\beta\\) parameters started with a goal (minimizing least squared errors) in describing the relationship between variables, with likelihood we also start with a goal. The “likelihood” is going to ask the question: What values of the unknown parameters make the data we see least surprising? When we get into likelihood, we will be drawing more directly on concepts within probability. We start by making a choice about what type of data generating process best describes our outcome data. Eventually, our likelihood function represents the “probability density of the data given the parameters and predictors.” (Definition taken from Gelman et al. 2020, pg. 105). In MLE, we are going to choose parameter estimates \\(\\widehat{\\theta}\\) for \\(\\theta\\) that maximize the likelihood that our data came from the particular distribution. Already, we are placing a lot of emphasis on the nature of our outcome data. The nature of our likelihood will change depending on if the data are dichotomous (e.g, similar to a set of coin flips that could be head or tails) or a count (e.g., similar to the number of events expected to occur over a certain interval) or more of a continuous numeric distribution with (e.g., where the probabilities of certain levels can be visualized similar to a bell curve). Each of these sets of data are generated through a different process, which is described by a particular probability function. Example This introduction is based on Ben Lambert’s video. I highly recommend watching this 8-9 minute video. Below we highlight a few of the key concepts and definitions. Take the UK population of 70 million. We have a sample of this, and in our sample some observations are male and some female. How can we use what we have, a sample, to estimate the probability that an individual is male?3 First, we can make a judgment about the data generating process. We can suppose there is some probability distribution function that determines the probability is a male or female. A probability density function (PDF for continuous data or PMF for discrete data) tells you the relative probability or likelihood of observing a particular value given the parameters of the distribution. Let’s call this \\(f(y_i | p)\\) where \\(y_i = 1\\) if male, 0 if female. We will say \\(p\\) is the probability an individual is male. \\(p^{y_i}(1 - p)^{1-y_i}\\) We are going to treat this like a toss of a coin, which has a Bernouilli distribution (every probability distribution we are dealing with is going to have an associated formula. You don’t need to memorize these. You can look them up on the internet when needed.) So, \\(f()\\) tells us the probability we would have gotten the value of the observation if we think of our observation as this Bernouilli toss of coin. This is the likelihood for a single observation. For example, we can now plug this into our Bernoulli function for the two possible cases of values for \\(y_i\\). \\(f(1|p) = p^1(1-p)^{1-1} = p\\) probability an individual is male \\(f(0 |p) p ^0(1-p)^{1-0} = 1-p\\) probability individual is female Now we want an estimate using our entire sample of observations, not just a single \\(i\\). What if we have \\(n\\) observations? \\(f(y_1, y_2, ... y_n | p)\\). We can write the joint probability as all individual probabilities multiplied together if we assume our observations are independent. This will now represent the likelihood for all observations. \\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)= \\prod_{i=1}^n p^y_i(1 - p)^{1-y_i}\\) This answers what is the probability that \\(Y_1\\) took on the particular value \\(y_1\\) Ok, now we have the statement \\(L = P(Y_1 = y_1, Y_2=y_2, ..., Y_n = y_n)\\). This joint probability is the likelihood (Technically it is proportionate to the likelihood, but this detail will not matter for us. What this means is there is a hidden constant \\(k(y)\\) multiplied by the joint probability. Because likelihood is a relative concept, this constant can fall out.) Generally, we don’t know \\(p\\). We are trying to estimate it. What we want to do is choose the \\(\\hat p\\) to maximize the likelihood that we would have gotten this set of observations given that \\(Y_i\\) has a probability distribution as specified. We have used a buzz word: “maximize.” Just as in OLS, that should be our signal that a derivative should be taken so that we can find the quantities that represent the maximum. We differentiate L with respect to p, set it to 0, to give us \\(\\hat{p}\\). Our issue (or at least one of our issues) is that products are tough to differentiate. A chain rule disaster. Instead, we use a trick of taking the log of the likelihood: log \\(\\prod ()\\).4 Benefit: it turns it into a sum, much easier. \\(\\log ab = \\log a + \\log b\\). So we will actually differentiate the \\(\\log\\) of the likelihood. Yes, this is why we had logs as part of Section 3. 5.1.1 Summarizing Steps for Maximum Likelihood Initial Setup What is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\). Define the likelihood for a single observation Define the likelihood for all observations Find the log-likelihood Then, the derivation begins! Yes, we’ve only just begun, but that first step of deciding the data generating process is huge. Example: Count Data Let’s say we are trying to understand the relationship between an independent variable and the number of news articles reported on a topic. This is a count of data. It goes from 0 to some positive number, never extending below 0. A distribution that is a good fit for this is the Poisson. We start by specifying this as our data generating process and looking up the Poisson probability density function, which has the parameter \\(\\lambda\\). Data Generating Process and probability density function. \\[\\begin{align*} &amp;Y_i \\stackrel{\\rm i.i.d.}{\\sim} Pois(\\lambda)\\Rightarrow\\\\ &amp;\\Pr(Y=Y_i|\\lambda)=\\lambda \\frac{exp(-\\lambda) \\lambda^{Y_i}}{Y_i!} \\end{align*}\\] Note: we assume our observations are iid (independently and identically distributed (For definition.) This assumption can be relaxed. What is the likelihood for a single observation? \\[\\begin{align*} \\mathcal L(\\lambda|Y_i)=\\Pr(Y=Y_i|\\lambda) \\end{align*}\\] What is the likelihood for all observations? \\[\\begin{align*} \\mathcal L(\\lambda|Y)&amp;=\\mathcal L(\\lambda|Y_1)\\times\\mathcal L(\\lambda|Y_2)\\times \\ldots \\times \\mathcal L(\\lambda|Y_{N})\\\\ \\mathcal L(\\lambda|Y)&amp;=\\prod_{i=1}^N\\mathcal L(\\lambda|Y_i)\\\\ \\end{align*}\\] Easier to work with log-likelihood \\[\\begin{align*} \\ell(\\lambda|Y)&amp;=\\sum_{i=1}^N\\mathcal \\log(\\mathcal L(\\lambda|Y_i))\\\\ \\end{align*}\\] Given observed data \\(Y\\), what is the likelihood it was generated from \\(\\lambda\\)? We will be choosing estimates of the parameters that maximize the likelihood we would have seen these data. Generally, we will also consider parameters like \\(\\lambda\\) to be functions of our covariates– the things we think help explain our otucome. For additonal practice, try to write down the likelihood of a single observation, the likelihood for all observations, and the log likelihood for an outcome we believe is normally distributed. We have \\(Y_i \\sim N(\\mu, \\sigma^2)\\). Our PDF is: \\[\\begin{align*} f(Y_i | \\theta) &amp;= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}} \\end{align*}\\] You can take it from here. Try on your own, then expand for the solution. We have the PDF. Let’s write the likelihood for a single observation. \\[\\begin{align*} L(\\theta | Y_i) = L(\\mu, \\sigma^2 | Y_i) &amp;= \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}} \\end{align*}\\] Let’s write the likelihood for all observations. \\[\\begin{align*} L(\\mu, \\sigma^2 | Y) &amp;= \\prod_{i=1}^{N} \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}} \\end{align*}\\] Let’s write the log likelihood. \\[\\begin{align*} \\ell(\\mu, \\sigma^2 | Y) &amp;= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\ &amp;= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\ &amp;= \\underbrace{\\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\ &amp;= \\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i&#39;\\beta)^2}{2\\sigma^2} \\end{align*}\\] Note how we usually will sub in \\(X\\beta\\) or \\(\\mathbf{x_i&#39;\\beta}\\) for the parameter because we think these will vary according to covariates in our data. The \\(e\\) in the Normal PDF is the base of the natural log. It is a mathematical constant. Sometimes you might see this written as \\(exp\\) instead of \\(e\\). In R, you can use exp() to get this constant. For example, \\(\\log e^2\\) in R would be log(exp(2)). In the future, we may want to know the probability of turning out to vote, or of going to war, or voting “yes” on a particular policy question, etc.↩︎ Why does this work? It has to do with the shape of the log (always increasing). Details are beyond the scope.↩︎ "],
["generalized-linear-models.html", "5.2 Generalized Linear Models", " 5.2 Generalized Linear Models Before we get into the details of deriving the estimators, we are going to discuss another connection between linear models and the types of models we will work with when we are using common maximum likelihood estimators. Recall our linear model: \\(y_i = \\beta_o + \\beta_1x_{i1} + ... \\beta_kx_{ik} + \\epsilon\\) \\(Y\\) is modelled by a linear function of explanatory variables \\(X\\) \\(\\hat \\beta\\) is our estimate of how much \\(X\\) influences \\(Y\\) (the slope of the line) On average, a one-unit change in \\(X_{ik}\\) is associated with a \\(\\hat \\beta_{k}\\) change in \\(Y_i\\) Slope/rate of change is linear, does not depend on where you are in \\(X\\). Every one-unit change has the same expected increase or decrease Sometimes we are dealing with outcome data that are restricted or “limited” in some way such that this standard linear predictor will no longer make sense. If we keep changing \\(X\\) we may eventually generate estimates of \\(\\hat y\\) that extend above or below the plausible range of values for our actual observed outcomes. The generalized linear model framework helps address this problem by adding two components: a nonlinear transformation and a probability model. This allows us to make predictions of our outcomes that retain the desired bounded qualities of our observed data. Generalized linear models include linear regression as a special case (a case where no nonlinear transformation is required), but as its name suggests, is much more general and can be applied to many different outcome structures. 5.2.1 GLM Model. In a GLM, we still have a “linear predictor”: \\(\\eta_i = \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik}\\) But our \\(Y_i\\) might be restricted in some way (e.g., might be binary). So, now we require a “link” function which tells us how \\(Y\\) depends on the linear predictor. This is the key to making sure our linear predictor, when transformed, will map into sensible units of Y. Our \\(Y_i\\) will also now be expressed in terms of a probability model, and it is this probability distribution that generates the randomness (the stochastic component of the model). For example, when we have binary outcome data, such as \\(y_i =\\) 1 or 0 for someone turning out to vote or not, we may try to estimate the probability that someone turns out to vote given certain explanatory variables. We can write this as \\(Pr(Y_i = 1 | x_i\\)). In a GLM, we need a way to transform our linear predictor such that as we shift in values of \\(X\\hat \\beta\\), we stay within plausible probability ranges. To do so we use a “link” function that is used to model the data. For example, in logistic regression, our link function will be the “logit”: \\[\\begin{align*} Pr(Y_i = 1 | x_i) &amp;= \\pi_i\\\\ \\eta_i &amp;= \\text{logit}(\\pi_i) = \\log \\frac{\\pi_i}{1-\\pi_i} &amp;= \\beta_o + \\beta_1x_{i1} + ... + \\beta_kx_{ik} \\end{align*}\\] One practical implication of this is that when we generate our coefficient estimates \\(\\hat \\beta\\), these will no longer be in units if \\(y_i\\) or even in units of probability. Instead, they will be in units as specified by the link function. In logistic regression, this means they will be in “logits.” For every one-unit change in \\(x_k\\), we get a \\(\\hat \\beta_k\\) change in logits of \\(y\\) However, the nice thing is that because we know the link function, with a little bit of work, we can use the “response” function to transform our estimates back into the units of \\(y_i\\) that we care about. \\[\\begin{align*} Pr(Y_i = 1 | x_i) &amp;= \\pi_i = g^{-1}(\\eta_i) \\\\ &amp;= \\text{logit}^{-1}(\\pi_i) \\\\ &amp;= \\frac{exp^{x_i&#39;\\beta}}{1 + exp^{x_i&#39;\\beta}} \\end{align*}\\] 5.2.2 Linking likelihood and the GLM Let’s use \\(\\theta\\) to represent the parameters of the pdf/pmf that we have deemed appropriate for our outcome data. As discussed before, we can write the likelihood for an observation as a probability statement. \\(\\mathcal L (\\theta | Y_i) = \\Pr(Y=Y_i | \\theta)\\) In social science, instead of thinking of these parameters as just constants (e.g., \\(p\\) or \\(\\mu\\)), we generally believe that they vary according to our explanatory variables in \\(X\\). We think \\(Y_i\\) is distributed according to a particular probability function and that the parameters that shape that distribution are a function of the covariates. \\(Y_i \\sim f(y_i | \\theta_i)\\) and \\(\\theta_i = g(X_i, \\beta)\\) Each type of model we come across–guided by the structure of the dependent variable– is just going to have different formulas for each of these components. Examples Model PDF \\(\\theta_i\\) ; Link\\(^{-1}\\) \\(\\eta_i\\) Linear \\(Y_i \\sim \\mathcal{N}(\\mu_i,\\sigma^2)\\) \\(\\mu_i = X_i^\\prime\\beta\\) \\(\\mu_i\\) Logit \\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\) \\(\\pi_i=\\frac{\\exp(X_i^\\prime\\beta)}{(1+\\exp(X_i^\\prime\\beta))}\\) logit\\((\\pi_i)\\) Probit \\(Y_i \\sim \\rm{Bernoulli}(\\pi_i)\\) \\(\\pi_i = \\Phi(X_i^\\prime\\beta)\\) \\(\\Phi^{-1}(\\pi_i)\\) These generalized linear models are then fit through maximum likelihood estimation, through an approach discussed in the next section where we use algorithms to choose the most likely values of the \\(\\beta\\) parameters given the observed data. Note: not all ML estimators can be written as generalized linear models, though many we use in political science are indeed GLMs. To be a GLM, the distribution we specify for the data generating process has to be a part of the exponential family of probability distributions (fortunately the gaussian normal, poisson, bernouilli, binomial, gamma, and negative binomial are), and after that, we need the linear predictor and link function. 5.2.3 GLM in R The way generalized linear models work in R is very similar to lm. Below is a simple example where we will specify a linear model in lm() and glm() to compare. ## Load Data florida &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/florida.csv&quot;) fit.lm &lt;- lm(Buchanan00 ~ Perot96, data=florida) fit.glm &lt;- glm(Buchanan00 ~ Perot96, data=florida, family=gaussian(link = &quot;identity&quot;)) For the glm, we just need to tell R the family of distributions we are using and the appropriate link function. In this example, we are going to use the normal gaussian distribution to describe the data generating process for Buchanan00. This is appropriate for nice numeric continuous data, even if it isn’t perfectly normal. The normal model has a link function, but it is the special case where the link function is just the identity. There is no nonlinear transformation that takes place. Therefore, we can still interpret the \\(\\hat \\beta\\) results in units of \\(Y\\) (votes in this case). In this special case, the \\(\\hat \\beta\\) estimates from lm() and glm() will be the same. coef(fit.lm) (Intercept) Perot96 1.34575212 0.03591504 coef(fit.glm) (Intercept) Perot96 1.34575212 0.03591504 There are some differences in the mechanics of how we get to the results in each case, but we will explore those more in the next section. I.e., these coefficients do not come out of thin air. Just like in OLS, we have to work for them. "],
["mle-estimation.html", "5.3 MLE Estimation", " 5.3 MLE Estimation This section will discuss the general process for deriving maximum likelihood estimators. It’s all very exciting. It builds on the resources from the previous sections. In the next section, we will go through this process for a binary dependent variable. Here, we lay out the overview. 5.3.1 Deriving Estimators Recall, we’ve already gone through a few steps of maximum likelihood estimation. Initial Setup What is the data generating process? This means think about the structure of the dependent variable. Is it continuous, is it a count, is it binary, is it ordered? Based on this, describe the probability distribution for \\(Y_i\\). Define the likelihood for a single observation Define the likelihood for all observations Find the log-likelihood Now we add steps building on the log-likelihood. Maximize the function with respect to (wrt) \\(\\theta\\) Take the derivative wrt \\(\\theta\\). We call this the “score” Set \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible) If not possible (often the case), we use an optimization algorithm to maximize the log likelihood. Take the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates. 5.3.2 Score function The first derivative of the log-likelihood is called the score function: \\(\\frac{\\delta \\ell}{\\delta \\theta} = S(\\theta)\\). This will tell us how steep the slope of the log likelihood is given certain values of the parameters. What we are looking for as we sift through possible values of the parameters, is the set of values that will make the slope zero, signalling that the function has reached a peak (maximizing the likelihood.) We set the \\(S(\\theta) = 0\\) and solve for \\(\\hat \\theta\\) (if possible). \\(\\hat \\theta\\) are the slopes/gradient, which we use as estimates (e.g., \\(\\hat \\beta\\)). We can interpret the sign and significance just as we do in OLS. But, unlike OLS, most of the time, these are not linear changes in units of \\(Y\\) We have to transform them into interpretable quantities Example: Normally distributed outcome Start with the log-likelihood \\[\\begin{align*} \\ell(\\theta | Y) &amp;= \\sum_{i = 1}^N \\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}\\Bigg)\\\\ &amp;= \\sum_{i = 1}^N \\underbrace{\\log \\Bigg( \\frac{1}{\\sigma\\sqrt{2\\pi}}\\Bigg) + \\log e^{\\frac{-(Y_i-\\mu)^2}{2\\sigma^2}}}_\\text{Using the rule $\\log ab = \\log a + \\log b$}\\\\ &amp;= \\underbrace{\\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mu)^2}{2\\sigma^2}}_\\text{The second term was of the form $\\log e ^ a$, we can re-write as $a * \\log e$. $\\log e$ cancels to 1, leaving us with just $a$.}\\\\ &amp;= \\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i&#39;\\beta)^2}{2\\sigma^2} \\end{align*}\\] Note: when you see \\(\\mathbf{x}_i&#39;\\beta\\), usually that is the representation of the multiplication of \\(k\\) covariates (a \\(1 \\times k\\) vector) for a particular observation \\(i\\) by \\(k \\times 1\\) coefficient values \\(\\beta\\). You can contrast this with \\(X\\beta\\), which represents \\(n \\times k\\) rows of observations with \\(k\\) covariates multiplied by the \\(k \\times 1\\) coefficients. You will see both notations depending on if notation is indexed by \\(i\\) or represented fully in matrix form. The \\(\\mathbf{x_i&#39;}\\) representation tends to come up more when we are dealing with likelihood equations. Here is a short video relating these notations. Take the derivative wrt \\(\\theta\\). Note: we have to take two derivatives- one for \\(\\mu\\) (\\(\\beta\\)) and one for \\(\\sigma^2\\). For this example we will focus only on the derivative wrt to \\(\\beta\\), as that it what gets us the coefficient estimates.5 Note: Below, we can simplify the expression of the log likelihood for taking the derivative with respect to \\(\\beta\\) because any term (i.e., the first term in the log likelihood in this case) that does not have a \\(\\beta\\) will fall out of the derivative expression. This is because when we take the derivative with respect to \\(\\beta\\) we treat all other terms as constants, and the slope of a constant (the rate of change of a constant) is zero. The curly \\(\\delta\\) in the expression below means “the derivative of …” with respect to \\(\\beta\\). \\[\\begin{align*} \\delta_\\beta \\ell(\\theta | Y) &amp;= -\\frac{1}{2\\sigma^2}\\sum_{i = 1}^N \\delta_\\beta (Y_i-\\mathbf{x}_i&#39;\\hat \\beta)^2 \\end{align*}\\] The right term should look familiar! It is the same derivative we take when we are minimizing the least squares. Therefore, we will end up with \\(S(\\hat \\theta)_\\beta = \\frac{1}{\\sigma^2}X&#39;(Y - X\\hat \\beta)\\). We set this equal to 0. \\[\\begin{align*} \\frac{1}{\\sigma^2}X&#39;(Y - X\\hat \\beta) &amp;= 0\\\\ \\frac{1}{\\sigma^2}X&#39;Y &amp;= \\frac{1}{\\sigma^2}X&#39;X\\hat \\beta \\\\ (X&#39;X)^{-1}X&#39;Y = \\hat \\beta \\end{align*}\\] 5.3.3 Hessian and Information Matrix The second derivative of the log-likelihood is the Hessian \\((H(\\theta))\\). The second derivative is a measure of the curvature of the likelihood function. This will help us confirm that we are at a maximum, and it will also help us calculate the uncertainty. The more curved (i.e., the steeper the curve), the more certainty we have. The \\(I\\) stands for the information matrix. The \\(H\\) stands for Hessian. \\(I(\\theta) = - \\mathbb{E}(H)\\) \\(var(\\theta) = [I(\\theta)]^{-1} = ( - \\mathbb{E}(H))^{-1}\\) Standard errors are the square roots of the diagonals of this \\(k \\times k\\) matrix (like vcov() in OLS) Example: Normal Start with the log-likelihood \\[\\begin{align*} \\ell(\\theta | Y) &amp;= \\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-x_i&#39;\\beta)^2}{2\\sigma^2} \\end{align*}\\] Because our \\(\\theta\\) has two parameters, the Hessian actually has four components. For this example, we will focus on one: the first and second derivatives wrt \\(\\beta\\). Recall the first derivative = \\(\\frac{1}{\\sigma^2}X&#39;(Y - X\\hat \\beta)\\). We now take the second derivative with respect to \\(\\hat \\beta\\) \\[\\begin{align*} \\frac{\\delta^2}{\\delta \\hat \\beta} \\frac{1}{\\sigma^2}X&#39;(Y - X\\hat \\beta)&amp;= -\\frac{1}{\\sigma^2}X&#39;X \\end{align*}\\] To get our variance, we take the inverse of the negative (-) of this: \\(\\sigma^2(X&#39;X)^{-1}\\) Should look familiar! With this example, we can start to see why lm and glm for a normally distributed outcome generate the same estimates. The maximum likelihood estimator is the same as the least squares estimator. 5.3.4 MLE Estimation Algorithm Suppose we are interested in finding the true probability \\(p\\) that a comment made on twitter is toxic, and we have a small sample of hand-coded data. Let’s say we have \\(n=8\\) observations where we could observe a \\(y_i = 1\\) or \\(0\\). For example, let’s say we read an online sample of tweets and we classified tweets as “toxic=1” or “nontoxic=0.” In our sample of \\(n=8\\), we coded 6 of them as toxic and 2 as nontoxic. We can write down the likelihood for a single observation using the Bernouilli pmf: \\(L(p | y_i) = p^{y_i}*(1-p)^{(1-y_i)}\\) We could then write out the likelihood for all 8 observations as follows: Where the equation simplifies to \\(p\\) for observations where \\(y_i\\) = 1 and (1-p) for observations where \\(y_i\\) = 0. For simplicity, let’s say \\(i=1\\) to \\(6\\) were toxic, and \\(i=7\\) to \\(8\\) were nontoxic. \\(L(p | \\mathbf{y}) = p * p * p * p * p * p * (1-p) * (1-p)\\) Now a naive way to maximize the likelihood would be to just try out different quantities for \\(p\\) and see which give us the maximum. ## Let&#39;s try this for different p&#39;s p &lt;- seq(.1, .9, .05) L &lt;- p * p * p * p * p * p * (1-p) * (1-p) We can then visualize the likelihood results and figure out about at which value for \\(\\hat p\\) we have maximized the likelihood. plot(x=p, y=L, type=&quot;b&quot;, xaxt=&quot;n&quot;) axis(1, p, p) When we have more complicated models, we are taking a similar approach–trying out different values and comparing the likelihood (or log likelihood), but we will rely on a specific algorithm(s) that will help us get to the maximum a bit faster than a naive search would allow. Don’t worry the built-in functions in R will do this for you (e.g., what happens under the hood of glm()), but if you were to need to develop your own custom likelihood function for some reason, you could directly solve it through an optimization algorithm if no such built-in function is appropriate. You can skip the details below if you wish and jump to the MLE Properties section. This content will only be involved in problem sets as extra credit, as you may not have to use optim in your own research. The optim function in R provides one such approach. For this optimization approach, we will need to. Derive the likelihood and/or log likelihood function and score Create an R function for the quantity to want to optimize (often the log likelihood) where given we provide the function certain values, the function returns the resulting quantity. (Kind of like when we supply the function mean() with a set of values, it returns the average of the values by computing the average under the hood of the function.) Use optim() to maximize optim(par, fn, ..., gr, method, control, hessian,...), where par: initial values of the parameters fn: function to be maximized (minimized) gr: optional argument, can include the gradient to help with optimization ...: (specify other variables in fn) method: optimization algorithm control: parameters to fine-tune optimization hessian: returns the Hessian matrix if TRUE By default, optim performs minimization. Make sure to set control = list(fnscale=-1) for maximization For starting values par, least squares estimates are often used. More sensible starting values help your optimize more quickly. You may need to adjust the maxit control parameter to make sure the optimization converges. A commonly used method is BFGS (a variant of Newton-Raphson), similar to what glm() uses, but there are other methods available. Example 1: estimating p Let’s take our relatively simple example about toxic tweets above and optimize the likelihood. First, we create a function for the likelihood that will calculate the likelihood for the values supplied. In the future, our models will be complicated enough, we will stick with the log likelihood, which allows us to take a sum instead of a product. One benefit of R is that you can write your own functions, just like mean() is a built-in function in R. For more information on writing functions, you can review Imai QSS Chapter 1 pg. 19.. lik.p &lt;- function(p){ lh &lt;- p * p * p * p * p * p * (1-p) * (1-p) return(lh) } Ok, now that we have our likelihood function, we can optimize. We just have to tell R a starting parameter for \\(\\hat p\\). Let’s give it a (relatively) bad one just to show how it works (i.e., can optim find the sensible .75 value. If you give the function too bad of a value, it might not converge before it maxes out and instead return a local min/max instead of a global one. startphat &lt;- .25 opt.fit &lt;- optim(par = startphat, fn=lik.p, method=&quot;BFGS&quot;, control=list(fnscale=-1)) ## This should match our plot opt.fit$par [1] 0.7500035 ## you should check convergence. Want this to be 0 to make sure it converged opt.fit$convergence [1] 0 Example 2: Linear Model We can use optim to find a solution for a linear model by supplying R with our log likelihood function. For the MLE of the normal linear model, our log likelihood equation is: \\[\\begin{align*} \\ell(\\theta | Y) &amp;= \\sum_{i = 1}^N \\log \\frac{1}{\\sigma\\sqrt{2\\pi}} - \\frac{(Y_i-\\mathbf{x}_i&#39;\\beta)^2}{2\\sigma^2} \\end{align*}\\] Now that we have our log likelihood, we can write a function that for a given set of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) parameter values, \\(X\\), and \\(Y\\), it will return the log likelihood. Below we indicate we will supply an argument par (an arbitrary name) that will inclue our estimates for the parameters: \\(k\\) values for the set of \\(\\hat \\beta\\) estimates and a \\(k + 1\\) value for the \\(\\hat \\sigma^2\\) estimate. Many models with only have one set of parameters. This is actually a slightly more tricky example. The lt line is the translation of the equation above into R code ## Log Likelihood function for the normal model l_lm &lt;- function(par, Y, X){ k &lt;- ncol(X) beta &lt;- par[1:k] sigma2 &lt;- par[(k+1)] lt &lt;- sum(log(1/(sqrt(sigma2)*sqrt(2*pi))) - ((Y - X %*% beta)^2/(2*sigma2))) return(lt) } Now that we have our function, we can apply it to a problem. Let’s use an example with a sample of Democrats from the 2016 American National Election Study dataset. This example is based on the article “Hostile Sexism, Racial Resentment, and Political Mobilization” by Kevin K. Banda and Erin C. Cassese published in Political Behavior in 2020. We are not replicating their article precisely, but we use similar data and study similar relationships. The researchers were interested in how cross-pressures influence the political participation of different partisan groups. In particular, they hypothesized that Democrats in the U.S. who held more sexist views would be demobilized from political participation in 2016, a year in which Hillary Clinton ran for the presidency. The data we are using are available anesdems.csv and represent a subset of the data for Democrats (including people who lean toward the Democratic party). We have a few variables of interest participation: a 0 to 8 variable indicating the extent of a respondent’s political participation female: a 0 or 1 variable indicating if the respondent is female edu: a numeric variable indicating a respondent’s education level age: a numeric variable indicating a respondent’s age. sexism: a numeric variable indicating a respondent’s score on a battery of questions designed to assess hostile sexism, where higher values indicate more hostile sexism. Let’s regress participation on these variables and estimate it using OLS, GLM, and optim. Note, OLS and GLM fit through their functions in R will automatically drop any observations that have missing data on these variables. To make it comparable with optim, we will manually eliminate missing data. anes &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv&quot;) ## choose variables we will use anes &lt;- subset(anes, select=c(&quot;participation&quot;, &quot;age&quot;, &quot;edu&quot;, &quot;sexism&quot;, &quot;female&quot;)) ## omit observations with missing data on these variables anes &lt;- na.omit(anes) ## OLS and GLM regression fit &lt;- lm(participation ~ female + edu + age + sexism, data=anes) fit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes, family=gaussian(link=&quot;identity&quot;)) Now we will build our data for optim. We need \\(X\\), \\(Y\\), and a set of starting \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\) values. ## X and Y data X.anes &lt;- model.matrix(fit) Y.anes &lt;- as.matrix(anes$participation) ## make sure dimensions are the same nrow(X.anes) [1] 1585 nrow(Y.anes) [1] 1585 ## Pick starting values for parameters startbetas &lt;- coef(fit) ## Recall our estimate for sigma-squared based on the residuals k &lt;- ncol(X.anes) startsigma &lt;- sum(fit$residuals^2) / (nrow(X.anes) - k ) startpar &lt;- c(startbetas, startsigma) ## Fit model ## But let&#39;s make it harder on the optimization by providing arbitrary starting values ## (normally you wouldn&#39;t do this) startpar &lt;- c(1,1,1,1,1,1) opt.fit &lt;- optim(par = startpar, fn=l_lm, X = X.anes, Y=Y.anes, method=&quot;BFGS&quot;, control=list(fnscale=-1), hessian=TRUE) We can compare this optimization approach to the output in glm(). We can first compare the log likelihoods logLik(fit.glm) &#39;log Lik.&#39; -2661.428 (df=6) opt.fit$value [1] -2661.428 We can compare the coefficients. ## Coefficients round(coef(fit), digits=4) round(coef(fit.glm), digits=4) round(opt.fit$par, digits=4)[1:k] (Intercept) female edu age sexism 0.9293 -0.2175 0.1668 0.0088 -0.9818 (Intercept) female edu age sexism 0.9293 -0.2175 0.1668 0.0088 -0.9818 [1] 0.9294 -0.2175 0.1668 0.0088 -0.9819 We can add the gradient of the log likelihood to help improve optimization. This requires specifying the first derivative (the score) of the parameters. Unfortunately this means taking the derivative of that ugly normal log likelihood above. Again, with the normal model, we have two scores because of \\(\\hat \\beta\\) and \\(\\hat \\sigma^2\\). For others, we may just have one. ## first derivative function score_lm &lt;- function(par, Y, X){ k &lt;- ncol(X) beta &lt;- as.matrix(par[1:k]) scorebeta &lt;- (1/par[k+1]) * (t(X) %*% (Y - X %*% beta)) scoresigma &lt;- -nrow(X)/(par[k+1]*2) + sum((Y - X %*% beta)^2)/(2 * par[k+1]^2) return(c(scorebeta, scoresigma)) } ## Fit model opt.fit &lt;- optim(par = startpar, fn=l_lm, gr=score_lm, X = X.anes, Y=Y.anes, method=&quot;BFGS&quot;, control=list(fnscale=-1), hessian=TRUE) In addition to using optim, we can program our own Newton-Raphson algorithm, which is a method that continually updates the coefficient estimates \\(\\hat \\beta\\) until it converges on a set of estimates. We will see this in a future section. The general algorithm involves the components we’ve seen before: values for \\(\\hat \\beta\\), the score, and the Hessian. Newton-Raphson: \\(\\hat \\beta_{new} = \\hat \\beta_{old} - H(\\beta_{old})^{-1}S(\\hat \\beta_{old})\\) Essentially, you need to take derivatives with respect to each of the parameters. Some models we use will have only one parameter, which is easier.↩︎ "],
["mle-properties.html", "5.4 MLE Properties", " 5.4 MLE Properties Just like OLS had certain properties (BLUE) that made it worthwhile, models using MLE also have desirable features under certain assumptions and regularity conditions. Large sample properties MLE is consistent: \\(p\\lim \\hat \\theta^{ML} = \\theta\\) It is also asymptotically normal: \\(\\hat \\theta^{ML} \\sim N(\\theta, [I(\\theta)]^{-1})\\) This will allow us to use the normal approximation to calculate z-scores and p-values And it is asymptotically efficient. “In other words, compared to any other consistent and uniformly asymptotically Normal estimator, the ML estimator has a smaller asymptotic variance” (King 1998, 80). Note on consistency What does it mean to say an estimator is consistent? As samples get larger and larger, we converge to the truth. Consistency: \\(p\\lim \\hat{\\theta} =\\beta\\) As \\(n \\rightarrow \\infty P(\\hat{\\theta} - \\theta&gt; e) \\rightarrow 0\\). Convergence in probability: the probability that the absolute difference between the estimate and parameter being larger than \\(e\\) goes to zero as \\(n\\) gets bigger. Note that bias and consistency are different: Consistency means that as the sample size (\\(n\\)) gets large the estimate gets closer to the true value. Unbiasedness is not affected by sample size. An estimate is unbiased if over repeated samples, its expected value (average) is the true parameter. It is possible for an estimator to be unbiased and consistent, biased and not consistent, or consistent yet biased. ; Images taken from here What is a practical takeaway from this? The desirable properties of MLE kick in with larger samples. When you have a very small sample, you might use caution with your estimates. No Free Lunch We have hinted at some of the assumptions required, but below we can state them more formally. First, we assume a particular data generating process: the probability model. We are generally assuming that observations are independent and identically distributed (allowing us to write the likelihood as a product)– unless we explicitly write the likelihood in a way that takes this into account. When we have complicated structures to our data, this assumption may be violated, such as data that is clustered in particular hierarchical entities. We assume the model (i.e., the choice of covariates and how they are modeled) is correctly specified. (e.g., no omitted variables.) We have to meet certain technical regularity conditions–meaning that our problem is a “regular” one. These, in the words of Gary King are “obviously quite technical” (1998, 75). We will not get into the details, but you can see pg. 75 of Unifying Political Methodology for the formal mathematical statements. In short, our paramaters have to be identifiable and within the parameter space of possible values (this identifiability can be violated, for example, when we have too many parameters relative to the number of observations in the sample), we have to be able to differentiate the log-likelihood (in fact, it needs to be twice continuously differentiable) along the support (the range of values) in the data. The information matrix, which we get through the second derivative, must be positive definite and finitely bounded. This helps us know we are at a maximum, and the maximum exists and is finite. You can visualize this as a smooth function, that is not too sharp (which would make it non differentiable), but has a peak (a maximum) that we can identify. 5.4.1 Hypothesis Tests We can apply the same hypothesis testing framework to our estimates here as we did in linear regression. First, we can standardize our coefficient estimates by dividing them by the standard error. This will generate a “z score.” Just like when we had the t value in OLS, we can use the z score to calculate the p-value and make assessments about the null hypothesis that a given \\(\\hat \\beta_k\\) = 0. \\[\\begin{align*} z &amp;= \\frac{\\hat \\theta_k}{\\sqrt{Var(\\hat \\theta)_k}} \\sim N(0,1) \\end{align*}\\] Note: to get p-values, we typically now use, 2 * pnorm(abs(z), lower.tail=F) instead of pt() and our critical values are based on qnorm() instead of qt(). R will follow the same in most circumstances. In large samples, these converge to the same quantities. 5.4.2 Model Output in R As discussed, we can fit a GLM in R using the glm function: glm(formula, data, family = XXX(link = \"XXX\", ...), ...) formula: The model written in the form similar to lm() data: Data frame family: Name of PDF for \\(Y_i\\) (e.g. binomial, gaussian) link: Name of the link function (e.g. logit, `probit, identity, log) ## Load Data fit.glm &lt;- glm(participation ~ female + edu + age + sexism, data=anes, family=gaussian(link=&quot;identity&quot;)) We’ve already discussed the coefficient output. Like lm(), GLM wiil also display the standard errors, z-scores / t-statistics, and p-values of the model in the model summary. summary(fit.glm)$coefficients Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.929302597 0.154033186 6.033132 1.999280e-09 female -0.217453631 0.067415200 -3.225588 1.282859e-03 edu 0.166837062 0.022402469 7.447262 1.559790e-13 age 0.008795138 0.001874559 4.691843 2.941204e-06 sexism -0.981808431 0.154664950 -6.347970 2.844067e-10 For this example, R reverts to the t-value instead of the z-score given that we are using the linear model. In other examples, you may see z in place of t. There are only small differences in these approximations because as your sample size gets larger, the degrees of freedom (used in the calculation of p-calues for the t distribution) are big enough that the t distribution converges to the normal distribution. Goodness of fit The glm() model has a lot of summary output. summary(fit.glm) Call: glm(formula = participation ~ female + edu + age + sexism, family = gaussian(link = &quot;identity&quot;), data = anes) Deviance Residuals: Min 1Q Median 3Q Max -2.3001 -0.8343 -0.3013 0.3651 7.2887 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 0.929303 0.154033 6.033 2.00e-09 *** female -0.217454 0.067415 -3.226 0.00128 ** edu 0.166837 0.022402 7.447 1.56e-13 *** age 0.008795 0.001875 4.692 2.94e-06 *** sexism -0.981808 0.154665 -6.348 2.84e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for gaussian family taken to be 1.688012) Null deviance: 2969.3 on 1584 degrees of freedom Residual deviance: 2667.1 on 1580 degrees of freedom AIC: 5334.9 Number of Fisher Scoring iterations: 2 Some of the output represents measures of the goodness of fit of the model. However, their values are not directly interpretable from a single model. Larger (less negative) likelihood, the better the model fits the data. (logLik(mod)). The becomes relevant when comparing two or more models. Deviance is calculated from the likelihood. This is a measure of discrepancy between observed and fitted values. (Smaller values, better fit.) Null deviance: how well the outcome is predicted by a model that includes only the intercept. (\\(df = n - 1\\)) Residual deviance: how well the outcome is predicted by a model with our parameters. (\\(df = n-k\\)) AIC- used for model comparison. Smaller values indicate a more parsimonious model. Accounts for the number of parameters (\\(K\\)) in the model (like Adjusted R-squared, but without the ease of interpretation). Sometimes used as a criteria in prediction exercises (using a model on training data to predict test data). For more information on how AIC can be used in prediction exercises, see here. Likelihood Ratio Test The likelihood ratio test compares the fit of two models, with the null hypothesis being that the full model does not add more explanatory power to the reduced model. Note: You should only compare models if they have the same number of observations. This type of test is used in a similar way that people compare R-squared values across models. fit.glm2 &lt;- glm(participation ~ female + edu + age + sexism, data=anes, family=gaussian(link=&quot;identity&quot;)) fit.glm1 &lt;- glm(participation ~ female + edu + age, data=anes, family=gaussian(link = &quot;identity&quot;)) anova(fit.glm1, fit.glm2, test = &quot;Chisq&quot;) # reject the null Analysis of Deviance Table Model 1: participation ~ female + edu + age Model 2: participation ~ female + edu + age + sexism Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) 1 1581 2735.1 2 1580 2667.1 1 68.021 2.182e-10 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Pseudo-R-squared We don’t have an exact equivalent to the R-squared in OLS, but people have developed “pseudo” measures. Example: McFadden’s R-squared \\(PR^2 = 1 - \\frac{\\ell(M)}{\\ell(N)}\\) where \\(\\ell(M)\\) is the log-likelihood for your fitted model and \\(\\ell(N)\\) is the log-likelihood for a model with only the intercept Recall, greater (less negative) values of the log-likelihood indicate better fit McFadden’s values range from 0 to close to 1 # install.packages(&quot;pscl&quot;) library(pscl) Warning: package &#39;pscl&#39; was built under R version 4.0.2 Classes and Methods for R developed in the Political Science Computational Laboratory Department of Political Science Stanford University Simon Jackman hurdle and zeroinfl functions by Achim Zeileis fit.glm.null &lt;- glm(participation ~ 1, anes, family = gaussian(link = &quot;identity&quot;)) pr &lt;- pR2(fit.glm1) fitting null model for pseudo-r2 pr[&quot;McFadden&quot;] McFadden 0.02370539 ## Or, by hand: 1 - (logLik(fit.glm1)/logLik(fit.glm.null)) &#39;log Lik.&#39; 0.02370539 (df=5) "],
["binary.html", "Section 6 Binary Dependent Variables", " Section 6 Binary Dependent Variables In this section, we review models specifically designed for estimating the relationships between our independent variables and a dichotomous dependent variable. This is where “logit” and “probit” regression models become relevant. Resources These concepts are discussed in King, Gary. 1998. Unifying political methodology: The likelihood theory of statistical inference. University of Michigan Press. 5.1-5.3 (available through Rutgers online library) The 2020 Gelman et al. book referenced in the syllabus also has a chapter devoted to logistic regression. Key limitations of OLS in the binary case are discussed here, and will be referenced at the end of the section. "],
["data-generating-process.html", "6.1 Data Generating Process", " 6.1 Data Generating Process Let’s say \\(Y_i\\) is a set of 0’s and 1’s for whether two states have experienced a dispute, an outcome common in IR studies. \\[\\begin{gather*} Y_i = \\begin{cases}1, \\;\\text{a dispute happened}\\\\ 0,\\; \\text{a dispute did not happen}\\end{cases} \\end{gather*}\\] Outside of political science, for example, in the field of higher education, we might instead think about an outcome related to whether a student has (= 1) or has not (= 0) had a negative advising experience. # Example of first 20 observations (Y1, Y2, ..., Y20) 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 We need to align these data with a data generating process and distribution. For each \\(Y_i\\), it is like a single trial, where you have a dispute with some probability \\((\\pi)\\) This sounds like the Bernoulli distribution! \\(Y_i \\sim Bernouli(\\pi)\\) 6.1.1 MLE Estimation Let’s do the steps we saw in the previous section. What is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\). Note: if you are using a function like glm() you can proceed directly there after this step. However, let’s work under the hood for a bit. \\[\\begin{align*} Y_i \\sim f(Y_i | \\pi) &amp;= Pr(Y_i = y_i |\\pi_i) = \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{pmf for Bernoulli} \\end{align*}\\] for \\(y = 0,1\\) Define the likelihood for a single observation Define the likelihood for all observations Find the log-likelihood \\[\\begin{align*} \\mathcal L( \\pi | Y_i) &amp;= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\ \\mathcal L( \\pi | Y) &amp;= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\ \\ell( \\pi | Y) &amp;= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood}\\\\ \\hat \\pi &amp;= \\text{Next step: arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$} \\end{align*}\\] Add step: nonlinear transformation to \\(X\\beta\\) Note that because we are likely using covariates, we need to express our parameter as a function of \\(X\\beta\\). Why? Because we don’t think there is a constant probability for a dispute. Instead, we think the probability of a dispute varies according to different independent variables, which are included in the \\(X\\) matrix and everntually will each have their own \\(\\beta_k\\) relationship with the probability of dispute. Now that we are outside of linear territory, we cannot just simply replace \\(\\pi\\) with \\(X\\beta\\) in the equation. Instead, \\(\\pi\\) is a function of \\(X\\beta\\). \\(\\pi = g(X_i, \\beta) \\neq \\mathbf{x}_i&#39;\\beta\\) This is because we need a transformation, such as the logit or probit to map our linear predictor into the outcome to make sure the linear predictor can be transformed back into sensible units of the outcome. The logit is one variety, as is the probit. Like two roads diverged in a yellow wood, this is the point in the process where we choose the transformation. For this first example, let’s apply a logit transformation, which restricts our estimates to between 0 and 1 (a good thing for probability!) where: \\(\\pi_i = \\text{logit}^{-1}(\\eta_i) = \\frac{exp^{\\eta_i}}{1 + exp^{\\eta_i}} = \\frac{exp^{\\mathbf{x}_i&#39;\\beta}}{1 + exp^{\\mathbf{x}_i&#39;\\beta}}\\) \\(\\eta_i = \\text{logit}(\\pi_i) = \\log\\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x}_i&#39;\\beta\\) Maximize the function with respect to (wrt) \\(\\theta\\) Where \\(\\pi_i = \\frac{exp^{\\mathbf{x}_i&#39;\\beta}}{1 + exp^{\\mathbf{x}_i&#39;\\beta}}\\) \\[\\begin{align*} \\hat \\pi &amp;= \\text{arg max } \\ell( \\pi | Y) \\text{ wrt $\\pi$} \\\\ &amp;= \\text{arg max} \\sum_{i=1}^n\\log \\pi_i^{y_i}(1 -\\pi_i)^{(1-y_i)}\\\\ &amp;= \\text{arg max} \\sum_{i=1}^n \\underbrace{y_i \\log \\Big( \\frac{exp^{\\mathbf{x}_i&#39;\\beta}}{1 + exp^{\\mathbf{x}_i&#39;\\beta}}\\Big) + (1-y_i)\\log \\Big(1-\\frac{exp^{\\mathbf{x}_i&#39;\\beta}}{1 + exp^{\\mathbf{x}_i&#39;\\beta}}\\Big)}_\\text{We replaced $\\pi_i$ and used the rule $\\log a^b = b \\log a$ to bring down the $y_i$ terms.} \\end{align*}\\] At this point, we take the derivative with respect to \\(\\beta\\). You can try this on your own, and, if you’re lucky, it may show up on a problem set near you. With a bit of work, we should get something that looks like the below, which we can represent in terms of a sum or in matrix notation. \\[\\begin{align*} S(\\theta) &amp;= \\sum_{i=1}^n (Y_i - \\pi_i)\\mathbf{x}^T_i\\\\ &amp;= X^T(Y - \\mathbf{\\pi}) \\end{align*}\\] You can note that the matrix notation retains the dimensions \\(k \\times 1\\), which we would expect because we want to choose a set of \\(k\\) coefficients in our \\(k \\times 1\\) vector \\(\\beta\\). The score, as written in summation notation, also has length \\(k\\) but here, we use a convention as writing \\(\\mathbf{x}_i&#39;\\) in row vector representation instead of a column vector. You could instead represent this as multiplied by \\(\\mathbf{x}_i\\), which would give us the \\(k \\times 1\\) dimensions. Either way we have \\(k\\) coefficients. These are just different notations. Take the second derivative of the log likelihood to get the “hessian” and help estimate the uncertainty of the estimates. Again, we can represent this as a sum or in matrix notation, which might be easier when translating this into R code where our data is more naturally inside a matrix. \\[\\begin{align*} H(\\theta) &amp;= - \\sum_{i=1}^n \\mathbf{x}_i\\mathbf{x}^T_i(\\pi_i)(1 - \\pi_i)\\\\ &amp;= -X^TVX \\end{align*}\\] where \\(V\\) is \\(n \\times n\\) diagonal matrix with weights that are the ith element of \\((\\pi)(1 - \\pi)\\) Once we have these quantities, we can optimize the function with an algorithm or go to glm in R, which will do that for us. "],
["r-code-for-fitting-logistic-regression.html", "6.2 R code for fitting logistic regression", " 6.2 R code for fitting logistic regression We can fit logistic regressions in R through glm(). Let’s build on the ANES example from section 5.3 and analyze a dichotomized measure of participation where 1=participated in at least some form and 0=did not participate. anes &lt;- read.csv(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/anesdems.csv&quot;) anes$partbinary &lt;- ifelse(anes$participation &gt; 0, 1, 0) We can then fit using glm where family = binomial(link=\"logit\") out.logit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes, family = binomial(link=&quot;logit&quot;)) The summary output includes the logit coefficients, standard errors, z-scores, and p-values. summary(out.logit) Call: glm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = &quot;logit&quot;), data = anes) Deviance Residuals: Min 1Q Median 3Q Max -2.5668 0.3328 0.4475 0.6287 1.2936 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 1.016734 0.334656 3.038 0.00238 ** female -0.382087 0.151516 -2.522 0.01168 * edu 0.321190 0.050945 6.305 2.89e-10 *** age 0.008682 0.004046 2.146 0.03188 * sexism -1.593694 0.336373 -4.738 2.16e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1361.5 on 1584 degrees of freedom Residual deviance: 1252.9 on 1580 degrees of freedom (355 observations deleted due to missingness) AIC: 1262.9 Number of Fisher Scoring iterations: 5 6.2.1 Writing down the regression model In the articles you write, you will describe the methods you use in detail, including the variables in the model and the type of regression (e.g., logistic regression). Sometimes you may want to go a step further and be very explicit about the model that you ran. We’ve already seen the regression equations for linear models. For the GLMs, they will look very similar, but we need to make the link/response function an explicit part of the equation. For example, for logistic regression we have a few ways of writing it, including: \\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i&#39;}\\beta\\), or alternatively \\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i&#39;\\beta) = \\frac{exp(\\mathbf{x_i&#39;}\\beta)}{(1 + exp(\\mathbf{x_i&#39;}\\beta)}\\) (You can also write out the individual variable names.) There is a new R package equatiomatic that can also be used to help write the equations from regression models. It’s not perfect, but should get you there for most basic models. ## First time, you need to install one of these #remotes::install_github(&quot;datalorax/equatiomatic&quot;) #install.packages(&quot;equatiomatic&quot;) ## Each time after, run library library(equatiomatic) ## Will output in latex code, though see package for details on options extract_eq(out.logit, wrap = TRUE, terms_per_line = 3) \\[ \\begin{aligned} \\log\\left[ \\frac { P( \\operatorname{partbinary} = \\operatorname{1} ) }{ 1 - P( \\operatorname{partbinary} = \\operatorname{1} ) } \\right] &amp;= \\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\ &amp;\\quad \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism}) \\end{aligned} \\] "],
["probit-regression.html", "6.3 Probit Regression", " 6.3 Probit Regression Probit regression is very similar to logit except we use a different link function to map the linear predictor into the outcome. Both the logit and probit links are suitable for binary outcomes with a Bernoulli distribution. If we apply a probit transformation, this also restricts our estimates to between 0 and 1. \\(\\pi_i = Pr(Y_i = 1| X_i) = \\Phi(\\mathbf{x}_i&#39;\\beta)\\) \\(\\eta_i = \\Phi^{-1}(\\pi_i) = \\mathbf{x}_i&#39;\\beta\\) Here, our coefficients \\(\\hat \\beta\\) represent changes in “probits” or changes “z-score” units. We use the Normal CDF (\\(\\Phi()\\)) aka pnorm() in R to transform them back into probabilities, specifically, the probability that \\(Y_i\\) is 1. Let’s fit our binary model with probit. We just need to change the link function. We can then fit using glm where family = binomial(link=\"probit\") out.probit &lt;- glm(partbinary ~ female + edu + age + sexism, data=anes, family = binomial(link=&quot;probit&quot;)) Let’s apply the equation tool to this: ## Each time after, run library library(equatiomatic) ## Will output in latex code, though see package for details on options extract_eq(out.probit, wrap = TRUE, terms_per_line = 3) \\[ \\begin{aligned} P( \\operatorname{partbinary} = \\operatorname{1} ) &amp;= \\Phi[\\alpha + \\beta_{1}(\\operatorname{female}) + \\beta_{2}(\\operatorname{edu})\\ + \\\\ &amp;\\qquad\\ \\beta_{3}(\\operatorname{age}) + \\beta_{4}(\\operatorname{sexism})] \\end{aligned} \\] The summary output includes the probit coefficients, standard errors, z-scores, and p-values. summary(out.probit) Call: glm(formula = partbinary ~ female + edu + age + sexism, family = binomial(link = &quot;probit&quot;), data = anes) Deviance Residuals: Min 1Q Median 3Q Max -2.6343 0.3188 0.4470 0.6361 1.2477 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.603661 0.184864 3.265 0.00109 ** female -0.202300 0.083407 -2.425 0.01529 * edu 0.179264 0.027611 6.493 8.44e-11 *** age 0.005145 0.002257 2.280 0.02261 * sexism -0.898871 0.186443 -4.821 1.43e-06 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1361.5 on 1584 degrees of freedom Residual deviance: 1250.6 on 1580 degrees of freedom (355 observations deleted due to missingness) AIC: 1260.6 Number of Fisher Scoring iterations: 5 We can interepret the sign and significance of the coefficients similarly to OLS. They just aren’t in units of \\(Y\\). In the section next week, we will discuss in detail how to generate quantities of interest from this output. "],
["to-logit-or-to-probit.html", "6.4 To logit or to probit?", " 6.4 To logit or to probit? Both approaches produce a monotonically increasing S-curve in probability between 0 and 1, which vary according to the linear predictor (\\(\\mathbf{x_i}^T\\beta\\)). In this way, either approach satisfies the need to keep our estimates, when transformed, within the plausible range of \\(Y\\). Image from Kosuke Imai. Both also start with \\(Y_i\\) as bernoulli Both produce the same function of the log-likelihood BUT define \\(\\pi_i\\) and link function differently Results–in terms of sign and significance of coefficients– are very similar Logit coefficients are roughly 1.6*probit coefficients Results–in terms of predicted probabilities– are very similar Exception– at extreme probabilities– Logit has “thicker tails”, gets to 0 and 1 more slowly Sometimes useful–Logit can also be transformed into “odds ratios” By convention, logit slightly more typically used in political science but easy enough to find examples of either Note on Odds Ratios in Logistic Regression Coefficients are in “logits” or changes in “log-odds” (\\(\\log \\frac{\\pi_i}{1 - \\pi}\\)). Some disciplines like to report “odds ratios” Odds ratio: \\(\\frac{\\pi_i(x1)/(1 - \\pi(x1))}{\\pi_i(x0)/(1 - \\pi(x0))}\\) (at a value of x1 vs. x0) If \\(\\log \\frac{\\pi_i}{1 - \\pi} = logodds\\); \\(\\exp(logodds) = \\frac{\\pi_i}{1 - \\pi}\\) Therefore, if we exponentiate our coefficients, this represents an odds ratio: the odds of \\(Y_i = 1\\) increase by a factor of (\\(\\exp(\\hat \\beta_k)\\)) due to 1-unit change in X ## odds ratio for the 4th coefficient exp(coef(out.logit)[4]) age 1.00872 ## CI for odds ratios exp(confint(out.logit)[4, ]) Waiting for profiling to be done... 2.5 % 97.5 % 1.000790 1.016801 In political science, we usually opt to present predicted probabilities instead of odds ratios, but ultimately you should do whatever you think is best. "],
["latent-propensity-representation.html", "6.5 Latent propensity representation", " 6.5 Latent propensity representation Sometimes you will see the binary outcome problem represented as a latent propensity where \\(Y^*_i\\) is a continuous variable that represents an unobserved propensity (e.g., to have a dispute, to be a toxic tweet, to participate), where \\[\\begin{gather*} Y_i = \\begin{cases}1, \\; y^*_i &gt; \\tau \\\\ 0,\\; y^*_i \\leq \\tau \\end{cases} \\end{gather*}\\] and \\(\\tau\\) is some threshold after which a the event (e.g., dispute) occurs. This becomes particularly relevant when the goal is to classify outcome estimates given certain \\(X\\) features. This type of threshold will also be relevant when we move into ordinal outcome variables where we want to estimate the probability an outcome belongs to a specific category. "],
["linear-probability-models.html", "6.6 Linear Probability Models", " 6.6 Linear Probability Models People (who me? yes, I admit, me) will sometimes still use a linear OLS model when we have dichotomous outcomes. In that case, we interpret the results as a “linear probability model” where a one-unit change in \\(x\\) is associated with a \\(\\hat \\beta\\) change in the probability that \\(Y_i = 1\\). This may sound like a disaster because linear models are generally meant for nice continuous outcomes, and there is no way to prevent extreme values of \\(X\\beta\\) from extending above 1 or below 0. This is not to mention the heteroskedasticity issues that come from binary outcome because the error terms depend on the values of \\(X\\). This website has a good overview of the potential problems with linear regression with binary outcomes. \\ Image from Chelsea Parlett-Pelleriti @ChelseaParlett on Twitter However, we can address some of these potential issues: 1) we can use robust standard errors to account for non-constant error variance , 2) if you look at the S-curve in the previous section, you will note that a large part of the curve is pretty linear over a wide range of \\(X\\beta\\) values. For many applications, the estimates transformed from a logit or probit into probability will look similar to the estimates from a linear probability model (i.e., OLS). 3) Linear probability models are easier to interpret, and there is no need to transform coefficients. LPM vs. logit/probit has spurred a lot of debate throughout the years. Reviewers disagree, twitter users disagree, some people just like to stir the pot, etc. This is just something to be aware of as you choose modeling approaches. Particularly when it comes to experiments and other causal inference approaches, there is a non-trivial push among active scholars to stick with linear probability models when your key independent variable is a discrete treatment ind icator variable. See this new article from Robin Gomilla who lays out the considerations for using LPM, particularly in experimental settings, as well as follow up discussion from Andrew Gelman. That said, even if you run with an LPM and cite the Gomilla article, a reviewer may still ask you to do a logit/probit. And there are certainly circumstances where LPM will fall short. So what’s the upshot? Probably try both, and then choose your own adventure. "],
["week-3-tutorial.html", "6.7 Week 3 Tutorial", " 6.7 Week 3 Tutorial This week’s example, we will replicate a portion of “The Effectiveness of a Racialized Counterstrategy” by Antoine Banks and Heather Hicks, published in the American Journal of Political Science in 2018. The replication data are here. Abstract: Our article examines whether a politician charging a political candidate’s implicit racial campaign appeal as racist is an effective political strategy. According to the racial priming theory, this racialized counterstrategy should deactivate racism, thereby decreasing racially conservative whites’ support for the candidate engaged in race baiting. We propose an alternative theory in which racial liberals, and not racially conservative whites, are persuaded by this strategy. To test our theory, we focused on the 2016 presidential election. We ran an experiment varying the politician (by party and race) calling an implicit racial appeal by Donald Trump racist. We find that charging Trump’s campaign appeal as racist does not persuade racially conservative whites to decrease support for Trump. Rather, it causes racially liberal whites to evaluate Trump more unfavorably. Our results hold up when attentiveness, old-fashioned racism, and partisanship are taken into account. We also reproduce our findings in two replication studies. We will replicate the analysis in Table 1 of the paper, based on an experiment the authors conducted through SSI. They exposed white survey respondents to either a news story about a Trump ad that includes an “implicit racial cue” or conditions that add to this with “explicitly racial” responses from different partisan actors calling out the ad as racist. Drawing on racial priming theory, racially prejudiced whites should be less supportive of Trump after the racial cues are made explicit. The authors test this hypothesis against their own hypothesis that this effect should be more pronounced among “racially liberal” whites. We are going to focus on a secondary outcome related to whether respondents believed the ad to be about race: “We also suspect that whites should provide a justification for either maintaining or decreasing their support for the candidate alleged to be playing the race card. Our results support these expectations. For example, racial liberals who read about a politician calling Trump’s implicit ad racist are more likely than those in the implicit condition to believe Trump’s ad is about race. On the other hand, pointing out the racial nature of the ad does not cause resentful whites to be any more likely to believe the ad is about race. Racially resentful whites deny that Trump’s subtle racial appeal on crime is racially motivated, which provides them with the evidence they need to maintain their support for his presidency” (320). 6.7.1 Loading data and fitting glm Let’s load the data. library(rio) study &lt;- import(&quot;https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true&quot;) The data include several key variables abtrace1: 1= if the respondent thought the ad was about race. 0= otherwise condition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions. racresent: a 0 to 1 numeric variable measuring racial resentment oldfash: a 0 to 1 numeric variable measuring “old-fashioned racism” trumvote: 1= respondent has vote preference for Trump 0=otherwise Let’s try to replicate column 5 in Table 1 using probit regression, as the authors do. Write down the equation for the regression. Use glm to run the regression. Compare the output to the table, column 5. Try on your own, then expand for the solution. We are fitting a probit regression. ## Column 5 fit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent + factor(condition2)*oldfash, data=study, family=binomial(link = &quot;probit&quot;)) summary(fit.probit5) Call: glm(formula = abtrace1 ~ factor(condition2) * racresent + factor(condition2) * oldfash, family = binomial(link = &quot;probit&quot;), data = study) Deviance Residuals: Min 1Q Median 3Q Max -2.2659 -0.9371 -0.5194 1.0015 2.0563 Coefficients: Estimate Std. Error z value Pr(&gt;|z|) (Intercept) 0.6320 0.2431 2.600 0.009323 ** factor(condition2)2 0.9685 0.2797 3.462 0.000535 *** racresent -1.9206 0.4174 -4.601 4.2e-06 *** oldfash 0.5265 0.3907 1.348 0.177772 factor(condition2)2:racresent -0.8513 0.4728 -1.801 0.071777 . factor(condition2)2:oldfash -0.4197 0.4376 -0.959 0.337476 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for binomial family taken to be 1) Null deviance: 1376.9 on 994 degrees of freedom Residual deviance: 1148.7 on 989 degrees of freedom (25 observations deleted due to missingness) AIC: 1160.7 Number of Fisher Scoring iterations: 4 We can write the regression as: \\[\\begin{align*} Pr(Y_i = 1 | X) &amp;= \\\\ \\Phi(\\alpha + \\text{Explicit Politician Condition}_i*\\beta_1 +\\\\ \\text{Racial Resentment}_i *\\beta_2 +\\\\ \\text{Old Fashioned Racism}_i*\\beta_3 +\\\\ \\text{Explicit Politician Condition}_i*\\text{Racial Resentment}_i*\\beta_4 +\\\\ \\text{Explicit Politician Condition}_i* \\text{Old Fashioned Racism}_i*\\beta_5) \\end{align*}\\] library(equatiomatic) extract_eq(fit.probit5, wrap = TRUE, terms_per_line = 3) \\[ \\begin{aligned} P( \\operatorname{abtrace1} = \\operatorname{1} ) &amp;= \\Phi[\\alpha + \\beta_{1}(\\operatorname{factor(condition2)}_{\\operatorname{2}}) + \\beta_{2}(\\operatorname{racresent})\\ + \\\\ &amp;\\qquad\\ \\beta_{3}(\\operatorname{oldfash}) + \\beta_{4}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{racresent}) + \\beta_{5}(\\operatorname{factor(condition2)}_{\\operatorname{2}} \\times \\operatorname{oldfash})] \\end{aligned} \\] A few questions: How should we interpret the coefficients? How do the interactions affect this interpretation? 6.7.2 Numeric Optimization Let’s repeat our replication of column 5, but this time, let’s use numeric optimization. We first need to make an X and Y matrix from our data. Because we have already run the models, let’s use a trick below: X &lt;- model.matrix(fit.probit5) Y &lt;- as.matrix(fit.probit5$y) The next thing we need to do is make a function for the log likelihood. Let’s recall the log likelihood for a Bernoulli random variable from a previous section: \\[\\begin{align*} \\mathcal L( \\pi | Y_i) &amp;= \\underbrace{\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for single observation}\\\\ \\mathcal L( \\pi | Y) &amp;= \\underbrace{\\prod_{i=1}^n\\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Likelihood for all observations}\\\\ \\ell( \\pi | Y) &amp;= \\underbrace{\\sum_{i=1}^n\\log \\pi^{y_i}(1 -\\pi)^{(1-y_i)}}_\\text{Log likelihood} \\end{align*}\\] Now we just change the definition of \\(\\pi\\) to be the transformation for a probit, which is \\(\\Phi(X\\beta)\\). We code this up as a function below. Try to make the connection between the log likelihood equation above and the last line of the function. Note: in R, we can use pnorm() to express \\(\\Phi(X\\beta)\\), this is the CDF of the normal distribution. llik.probit &lt;- function(par, Y, X){ beta &lt;- as.matrix(par) link &lt;- pnorm(X %*% beta) like &lt;- sum(Y*log(link) + (1 - Y)*log(1 - link)) return(like) } Let’s generate some starting values for the optimization. ## Starting values ls.lm &lt;- lm(Y ~ X[, -1]) start.par &lt;- ls.lm$coef Finally, let’s use optim to find our parameter estimates. ## optim() out.opt &lt;- optim(start.par, llik.probit, Y=Y, X=X, control=list(fnscale=-1), method=&quot;BFGS&quot;, hessian = TRUE) We can compare the estimates recovered from glm and optim. ## Log likelihood logLik(fit.probit5) out.opt$value &#39;log Lik.&#39; -574.3267 (df=6) [1] -574.3267 ## Coefficients cbind(round(coef(fit.probit5), digits=4), round(out.opt$par, digits = 4)) [,1] [,2] (Intercept) 0.6320 0.6321 factor(condition2)2 0.9685 0.9684 racresent -1.9206 -1.9207 oldfash 0.5265 0.5265 factor(condition2)2:racresent -0.8513 -0.8512 factor(condition2)2:oldfash -0.4197 -0.4196 How could we get the standard errors? 6.7.3 Predicted Probabilities One thing we could do for our interpretations is to push this further to generate “quantities of interest.” We will do one example of this but will talk in more detail about this next week. Let’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions. For this example, we are going to hold “old-fashioned racism” at its mean value. This is slightly different from what the authors do but should generate similar results. The authors hold old-fashioned racism at its “observed values.” We can rely on the predict function like we did with OLS, but here, we need to set type = response to put the results on the response scale instead of the scale of the linear predictor. What this does is apply our function \\(\\Phi(\\mathbf{x_i}&#39; \\hat \\beta)\\) for our designated values of \\(X\\) and estimates for \\(\\hat \\beta\\). predvals.imp &lt;- predict(fit.probit5, newdata = data.frame(condition2=1, racresent = seq(0, 1, .0625), oldfash = mean(study$oldfash, na.rm=T)), type=&quot;response&quot;) predvals.exp &lt;- predict(fit.probit5, newdata = data.frame(condition2=2, racresent = seq(0, 1,.0625), oldfash = mean(study$oldfash, na.rm=T)), type=&quot;response&quot;) ## Plot results plot(x=seq(0, 1, .0625), y=predvals.imp, type=&quot;l&quot;, ylim = c(0, 1), lty=2, ylab = &quot;Predicted Probability&quot;, xlab = &quot;Racial Resentment&quot;, main = &quot;Predicted Probability of Viewing the Ad as about Race&quot;, cex.main = .7) legend(&quot;bottomleft&quot;, lty= c(2,1), c(&quot;Implicit&quot;, &quot;Explicit&quot;)) points(x=seq(0, 1, .0625), y=predvals.exp, type=&quot;l&quot;) Additional Questions For extra practice, you can try replicating column 4 in the model. You can also try replicating the results in the figure with a logit model. Are the predicted probabilities similar? "],
["qoi.html", "Section 7 Quantities of Interest", " Section 7 Quantities of Interest This section will provide information about calculating quantities of interest. Often, in linear regression, our coefficients might directly represent the quantities we are interested in interpreting. However, in other models, we might need to do a bit of work to make it easier on our readers (and ourselves). The quantities of interest you generate should be directly related to your research question and hypotheses. For example, if my hypothesis was about how the competitiveness of a state is related to the probability that someone is contacted by a campaign, then my quantities of interest would involve a comparison of the predicted probability of campaign contact for those in more vs. less competitive states. In contrast, if my hypothesis was about the number of bills passed in a legislature under unified vs. divided government, I would want to express my quantities of interest as a different in counts of bills or rate of bills pasesed. We will discuss computing quantities of interest, uncertainty for these quantities, and visualization. We can take something that looks like column 5 in the table from Antoine Banks and Heather Hicks example from the previous section and move it into a figure, as the authors did. Here are a few external resources that relate to the concepts discussed in this section. Michael J. Hanmer and Kerem Ozan Kalkan. Behind the Curve: Clarifying the Best Approach to Calculating Predicted Probabilities and Marginal Effects from limited Dependent Variable Models. American Journal of Political Science. Why plot results? Jonathan P. Kastellec and Eduardo L. Leoni. Using Graphs Instead of Tables in Political Science. Short overviews of the bootstrap method for calculating uncertainty: Pezullo, John. The Bootstrap Method for Standard Errors and Confidence Intervals. Banks, David. Lecture from Duke University. Overview of simulation approach for calculating uncertainty from King, Tomz, and Wittenberg 2000. R packages helpful for quantities of interest Thomas Leeper: prediction and margins Gary King and colleagues: Zelig "],
["using-the-response-functions-to-generate-quantities-of-interest.html", "7.1 Using the response functions to generate quantities of interest", " 7.1 Using the response functions to generate quantities of interest Recall, in linear regression, to get our estimated values \\(\\hat Y\\) we said \\(\\hat Y = X\\hat\\beta\\). In glm’s, we can do the same to get our estimated values on the scale of the linear predictor \\(\\hat \\eta = X\\hat\\beta\\). We then use our \\(Link^{-1}\\) response function to transform these values into the quantity of interest. E.g., in logistic regression we want \\(\\hat{\\pi} = \\frac{\\exp(X\\hat\\beta)}{1 + \\exp(X\\hat\\beta)}\\). E.g., in probit regression we want \\(\\hat{\\pi} = \\Phi(X \\hat \\beta)\\). These represent the predicted probability of \\(Y_i = 1\\) given our coefficient estimates and designated values of the covariates Let’s use a subset of the MIDs mids.txt data available here. This dataset has variables related to whether a dyad of states is engaged in a militarized interstate dispute between the two countries in a given year. The variable that will be our outcome of interest is Conflict which takes the values 0 or 1. We will also look at the relationship between a few independent variables and the propensity for conflict. Data are at Dyad Level. whether the pair of countries include a major power (MajorPower, 1=yes, 0=otherwise), are contiguous ( Contiguity, 1=yes, 0=otherwise), are allies (Allies, 1=yes, 0=otherwise), and/or have similar foreign policy portfolios (ForeignPolicy, 1=yes, 0=otherwise) BalanceofPower: balance of military power YearsSince: the number of years since the last dispute ## Load data mids &lt;- read.table(&quot;https://raw.githubusercontent.com/ktmccabe/teachingdata/main/midsshort.txt&quot;) table(mids$Conflict) 0 1 99657 343 As can be seen from the table above, conflicts (fortunately) are relatively rare in our data. This means are predicted probabilities are likely going to be pretty small in this example. We will run a logistic regression with a few covariates. out.logit &lt;-glm(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = mids) Our logistic regression equation is: \\(\\log \\frac{\\pi_i}{1-\\pi_i} = \\mathbf{x_i&#39;}\\beta\\), or alternatively \\(Pr(Y_i = 1 | \\mathbf{x}_i) = logit^{-1}(\\mathbf{x}_i&#39;\\beta) = \\frac{exp(\\mathbf{x_i&#39;}\\beta)}{1 + exp(\\mathbf{x_i&#39;}\\beta)}\\) Our coefficients are in the “logit” aka log-odds scale of the linear predictor, so we use the response function to put them into probability estimates. For logistic regression, we can generate predicted probabilities for each \\(Y_i\\) using the predict(model, type=\"response\") function, using the plogis function with \\(X\\hat \\beta\\), or Question: What would be the equivalent function for probit? manually writing down the response function \\(\\frac{exp(\\mathbf{x_i&#39;}\\beta)}{1 + exp(\\mathbf{x_i&#39;}\\beta)}\\). We use the predict function exactly the same way as before, but the key argument we need to specify is type. If you have type = link this generates answers that are still on the log-odds linear predictor scale. It is switching this to response that goes into probability in the logit case or probit case. Example First, let’s just generate predicted probabilities for each observation in our data, without specifiying any designated values for \\(X\\)– just keeping the values as they are in the data aka “as observed.” ## Method with predict() ## When you don&#39;t specify newdata, R assumes you want the X data from the model pp &lt;- predict(out.logit, type = &quot;response&quot;) ## Manual way #1 X &lt;- model.matrix(out.logit) bh &lt;- coef(out.logit) pplogis &lt;- plogis(X %*% bh) ## Manual way # 2 ppexp &lt;- exp(X %*% bh)/(1 + exp(X %*% bh)) ## Compare the first five rows of each to see they are the same cbind(pplogis, ppexp, pp)[1:5,] pp 78627 0.0004204501 0.0004204501 0.0004204501 295818 0.0001035526 0.0001035526 0.0001035526 251841 0.0006211178 0.0006211178 0.0006211178 98068 0.0004270812 0.0004270812 0.0004270812 209797 0.0001005396 0.0001005396 0.0001005396 The code above generates a predicted probability associated with each observation in the data. This is similar to generating a fitted value \\(\\hat y\\) for each observation in OLS. "],
["qoi-at-designated-values.html", "7.2 QOI at Designated Values", " 7.2 QOI at Designated Values Usually in social science we have hypotheses about how the predicted probabilities change as one or more of our independent variables change. We will now turn to calculating predicted responses according to specific values of the independent variables. Recall, sometimes in linear regression, we wanted to calculate a specific estimated value of \\(\\hat Y_i\\) for when we set \\(X\\) at particular values. (e.g., What value do we estimate for \\(Y\\) when \\(X1 = 2\\) and \\(X2=4\\)?) In OLS, this would be \\(\\hat Y_i = \\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2\\) Here, we can do the same for GLMs by setting specific values for \\(X\\) when we apply the \\(Link^{-1}\\) response function. E.g., What is the predicted probability of \\(Y_i = 1\\) when \\(X1 = 2\\) and \\(X2=4\\)? In logistic regression, \\(\\hat{\\pi_i} = \\frac{\\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}{1 + \\exp(\\hat \\alpha + 2*\\hat \\beta_1 + 4*\\hat \\beta_2)}\\) Example 1 Example using the Conflict data using different approaches in R. ## Predicted probability when Allies = 1, and all other covariates = 0 allies1 &lt;- predict(out.logit, newdata = data.frame( MajorPower = 0, Contiguity = 0, Allies = 1, ForeignPolicy = 0,BalanceOfPower = 0, YearsSince = 0), type = &quot;response&quot;) allies1 1 0.002632504 ## for allies = 1, careful of the order to make same as coefficients X &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) Bh &lt;- coef(out.logit) ## Approach 1 plogis(X %*% bh) [,1] [1,] 0.002632504 ## Approach 2 exp(X%*% bh)/(1 + exp(X%*% Bh)) [,1] [1,] 0.002632504 Example 2 Second example keeping X at observed values. Here the manual approach is easier given the limitations of predict (at least until we learn a new package). Now we are estimating \\(N\\) predicted probabilities, so we take the mean to get the average estimate. ## for allies = 1careful of the order to make same as coefficients X &lt;- model.matrix(out.logit) X[, &quot;Allies&quot;] &lt;- 1 # change Allies to 1, leave everything else as is Bh &lt;- coef(out.logit) ## Approach 1 mean(plogis(X %*% bh)) [1] 0.002759902 ## Approach 2 mean(exp(X%*% bh)/(1 + exp(X%*% Bh))) [1] 0.002759902 This is the average predicted probability of having a dispute when the dyad states are Allies, holding other covariates at their observed values. Here is a brief video with a second example of the process above, leading into the discussion of marginal effects below. It uses the anes data from Banda and Cassese in section 6. 7.2.1 Marginal Effects Recall, in linear regression a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) change in \\(Y\\) no matter where we are in the domain of \\(X_k\\). (The slope of a line is constant!) The catch for glm’s, again, is that our linear predictor (\\(\\eta\\)) is often not in the units of \\(Y\\) that we want. E.g., In logistic regression, a one-unit change in \\(X_k\\) is associated with a \\(\\hat \\beta_k\\) logits change Recall, for logit and probit, this takes us into an S-curve for \\(Pr(Y_i = 1)\\) instead of a line Well, the slope of an S-curve is not constant. Depending on where we are in \\(X\\), it will influence how much change we have in the predicted probability of \\(Y_i = 1\\). Therefore, to understand the marginal effect in glm’s we have to set \\(X\\) to particular values and be careful about the values we select. By “careful,” this means choosing sensible, theoretically informed values of interest. You can generate predictions based on any values. Here are three common approaches for understanding the marginal effect of a particular variable \\(X_k\\). Marginal effects at the mean Average marginal effects Marginal effects at representative values Wait, what do we mean by marginal effects? For a discrete (categorical/factor) variable (\\(X_k\\)) this will be the change in predicted probability associated with a one-unit change in (\\(X_k\\)). For continuous variables (\\(X_k\\)), this technically is the instantaneous rate of change (change in probability associated with a very small change in \\(X\\)). Usually instead of estimating this (what is a very small change anyway?) we will do this by hand instead, and set the specific amount of change). Often, this is called “discrete change” or “first difference” effect. 7.2.2 Marginal effects at the mean In this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at their mean values. This gives us 1 estimate for the difference in predicted probability When can this be problematic? (think categorical variables) 7.2.3 Marginal effects at representative values In this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_k\\), we set all other covariates \\(X_j\\) for \\(j \\neq k\\) at values that are of theoretical interest. This could be the mean value, modal vale, or some other value that makes sense for our research question. This gives us 1 estimate for the difference in predicted probability Depending on your research question, there may/may not be a particularly interesting set of representative values on all of your covariates. The example above where we held all other covariates at zero would be an example of calculating marginal effects at representative values. 7.2.4 Average marginal effects In this approach, when we calculate the difference in predicted probability resulting from a one-unit change in \\(X_{ik}\\), we hold all covariates \\(X_{ij}\\) for \\(j \\neq k\\) at their observed values. This gives us \\(N\\) estimates for the difference in predicted probability We report the average of these estimates Here is an example for average marginal effects. Let’s sat we were interested in the difference in probability of a dispute for Allies vs. non-Allies, when all other covariates are zero. We can do this manually or in predict. ## Extract beta coefficients Bh &lt;- coef(out.logit) ## Set Allies to 1, hold all other covariates as observed X1 &lt;- model.matrix(out.logit) X1[, &quot;Allies&quot;] &lt;- 1 ## Set Allies to 0, hold all other covariates as observed X0 &lt;- model.matrix(out.logit) X0[, &quot;Allies&quot;] &lt;- 0 pp1 &lt;- mean(plogis(X1 %*% Bh)) pp0 &lt;- mean(plogis(X0 %*% Bh)) pp1 - pp0 [1] -0.0009506303 This represents the average difference in predicted probability of having a dispute for Dyads that are Allies vs. not Allies. 7.2.5 prediction and margins packages. There are functions that can make this easier so long as you understand what they are doing. One package developed by Dr. Thomas Leeper is prediction. A second is margins. Documentation available here and here. It is always important to understand what’s going on in a package because, for one, it’s possible that the package will stop being updated, and you will have to find an alternative solution. We will focus on the prediction package first. The prediction function generates specific quantities of interest. An advantage it has over the built-in predict function is that it makes it easier to “hold all other variables at observed values.” In the prediction function, you specify the designated values for particular variables, and then by default, it assumes you want to hold all other variables at observed values. Here is an example of generating predicted probabilities for Allies = 1 and Allies = 0. It will generate the summary means of these two predictions. ## install.packages(&quot;prediction&quot;) library(prediction) ## By default, allows covariates to stay at observed values unless specified prediction(out.logit, at = list(Allies = c(0, 1)), type = &quot;response&quot;) Data frame with 200000 predictions from glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = mids) with average predictions: Allies x 0 0.003711 1 0.002760 ## compare with the manual calculated values above pp0 [1] 0.003710532 pp1 [1] 0.002759902 7.2.6 QOI Practice Problems Conduct the following regression using glm \\(Pr(Conflict_i = 1 | X) = logit^{-1}(\\alpha + \\beta_1 * Allies_i + \\beta_2 * MajorPower_i + \\beta_3 * ForeignPolicy_i)\\) What is the predicted probability of entering a dispute when the dyad includes a major power, holding all covariates at observed values? Repeat the previous exercise, but now use probit. How similar/different are the predicted probability estimates? Try on your own, then expand for the solution. ## Problem 1 out.logit2 &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids, family = binomial(link = &quot;logit&quot;)) ## Problem 2 library(prediction) prediction(out.logit, at = list(MajorPower = 1), type = &quot;response&quot;) Data frame with 100000 predictions from glm(formula = Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = mids) with average prediction: MajorPower x 1 0.007745 ## Problem 3 out.probit &lt;- glm(Conflict ~ Allies + MajorPower + ForeignPolicy, data=mids, family = binomial(link = &quot;probit&quot;)) prediction(out.probit, at = list(MajorPower = 1), type = &quot;response&quot;) Data frame with 100000 predictions from glm(formula = Conflict ~ Allies + MajorPower + ForeignPolicy, family = binomial(link = &quot;probit&quot;), data = mids) with average prediction: MajorPower x 1 0.01451 ## Manual approach X &lt;- model.matrix(out.probit) X[, &quot;MajorPower&quot;] &lt;- 1 Bhat &lt;- coef(out.probit) mean(pnorm(X %*% Bhat)) [1] 0.01450613 "],
["uncertainty.html", "7.3 Uncertainty", " 7.3 Uncertainty Usually, we want to report a confidence interval around our predicted probabilities, average predicted probabilities, or around the difference in our predicted probabilities or difference in our average predicted probabilities. This is different from the uncertainty of a coefficient, which we already have from our glm output. Here, if we say there is a 0.01 probability of a dispute, that is just an estimate, it is going to vary over repeated samples. We want to generate a confidence interval that represents this variability in \\(\\hat \\pi\\). We have already discussed using the predict function in lm to generate confidence intervals for OLS estimates. In a limited set of cases, we can also use this shortcut for glm by taking advantage of the distribution being approximately normal on the scale of the linear predictor. When we are estimating confidence intervals around 1) one or multiple single quantities of interest (a predicted probability, as opposed to a difference in predicted probability) 2) where the \\(X\\) values are set at specific values (and not at their observed values) then, we can plug this into the predict function in the following way: Generate the prediction and standard errors of the prediction on the link linear predictor scale. On the scale of the linear predictor, the standard errors of the prediction are calculated as \\(\\sqrt{\\mathbb{x&#39;}_c \\text{vcov(fit)} \\mathbb{x}_c}\\) using the delta method. Calculate the CI on the linear predictor scale: \\(CI(\\hat \\theta) = \\hat \\theta - z_{crit}*se_{\\hat \\theta}\\) ; \\(\\hat \\theta + z_{crit}*se_{\\hat \\theta}\\) \\(z_{crit}\\) for the 95% confidence interval is 1.96 (so this is saying +/- about 2 standard errors). We get this by using qnorm(). Convert the prediction and confidence intervals to the response scale. Here is an example: ## Predicted probability when Allies = 1 and all other covariates = 0 ## Note type = &quot;link&quot; allies1.link &lt;- predict(out.logit, newdata = data.frame( MajorPower = 0, Contiguity = 0, Allies = 1, ForeignPolicy = 0,BalanceOfPower = 0, YearsSince = 0), type = &quot;link&quot;, se = T) allies1 &lt;- plogis(allies1.link$fit) allies1.lb &lt;- plogis(allies1.link$fit - qnorm(.975)*allies1.link$se.fit) allies1.ub &lt;- plogis(allies1.link$fit + qnorm(.975)*allies1.link$se.fit) ## Confidence interval c(allies1, allies1.lb, allies1.ub) 1 1 1 0.002632504 0.001302711 0.005312514 ## By hand (using x as a k x 1 vector) x.c &lt;- rbind(1, 0, 0, 1, 0, 0, 0) se.hand &lt;- sqrt(t(x.c) %*% vcov(out.logit) %*% x.c) p.hand &lt;- t(x.c) %*% coef(out.logit) allies1.hand &lt;- plogis(p.hand) allies1.hand.lb &lt;- plogis(p.hand- qnorm(.975)*se.hand) allies1.hand.ub &lt;- plogis(p.hand + qnorm(.975)*se.hand) c(allies1.hand, allies1.hand.lb, allies1.hand.ub) [1] 0.002632504 0.001302711 0.005312514 Beyond this simple case, there are three general approaches to calculating the uncertainty of the quantities of interest. Here is a video with an overview of these three processes. The course notes contain additional detail below. It continues with the anes data example from Banda and Cassese in section 6, as did the other video in this section. Delta Method (based on calculus, first order Taylor Expansion approximation) Bootstrapping (very flexible, common, computationally demanding) Quasi-bayesian (flexible, less computationally demanding), also described as simulation/Monte carlo simulation. For now, we will focus on the second two methods, but some statistical software programs will report uncertainty estimates based on the Delta method. Here is more information on this method and the deltamethod function in R. 7.3.1 Bootstrapping Bootstrapping simulates the idea of conducting repeated samples to generate a distribution of estimates of your quantity of interests. We “resample” from our existing data to generate thousands of new datasets, and use each dataset to generate a slightly different quantity of interest. This distribution is then used to construct the confidence interval. Process: Sample from the data to generate new data frame Run the model: this gives new coefficient estimates and new covariate matrices Use new coefficient and covariate estimates to compute quantity of interest Replicate the previous process for about 1000 iterations to get 1000 estimates of quantity of interest Use the distribution of these estimates to calculate confidence intervals Why? How does this work? It is simulating the exercise of hypothetical repeated samples Similar to Law of Large Numbers- with sufficient iterations, the empirical “bootstrap” distribution is a good approximation of the true distribution (will get closer and closer to the truth) It won’t help us correct a bad estimate– have to work from the data we have. The logic is we think the distribution of \\(\\bar x\\) sample estimate is centered on \\(\\mu\\) (the truth), and then we assume the distribution of \\(\\bar x*\\) (the bootstrapped estimate) is centered on \\(\\bar x\\) This would be a good place to review the Bootstrap resources at the front of the section: Pezullo, John. The Bootstrap Method for Standard Errors and Confidence Intervals. Banks, David. Lecture from Duke University. How do we implement this procedure? Example Find the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values ## Original regression out.logit &lt;-glm(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = mids) ## We need to build our bootstrap procedure ## Let&#39;s assume we just want 1 iteration ## Step 1: sample to generate new data ## this selects N row numbers from mids, with replacement wrows &lt;- sample(x =1:nrow(mids), size = nrow(mids), replace = T) ## Create subset of data based on these rows subdata &lt;- mids[wrows, ] ## Step 2: run your regression model with the new data boot.logit &lt;-glm(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = subdata) ## Step 3: generate average predicted probability Xboot &lt;- model.matrix(boot.logit) Xboot[, &quot;Allies&quot;] &lt;- 1 Bh &lt;- coef(boot.logit) p.boot &lt;- mean(plogis(Xboot %*% Bh)) Expand below for more details on what the sample function does. Let’s say we have a dataframe of different colors and shapes. somedata &lt;- data.frame(colors = c(&quot;red&quot;, &quot;blue&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;orange&quot;, &quot;black&quot;), shapes = c(&quot;circle&quot;, &quot;square&quot;, &quot;triangle&quot;, &quot;rectangle&quot;, &quot;diamond&quot;, &quot;line&quot;, &quot;sphere&quot;)) somedata colors shapes 1 red circle 2 blue square 3 yellow triangle 4 green rectangle 5 purple diamond 6 orange line 7 black sphere I could generate a new “resampled” dataset with the sample function. We tell the function three things: 1) choose from the row numbers in my dataframe (1:nrow(somedata)), 2) pick \\(N\\) row numbers in total (nrow(somedata)), 3) Each time you pick a given row number \\(i\\), put it back in the data, allowing the possibility that you may randomly sample it again (replace = TRUE). sample(1:nrow(somedata), nrow(somedata), replace = TRUE) [1] 5 5 6 1 5 1 5 sample(1:nrow(somedata), nrow(somedata), replace = TRUE) [1] 2 5 5 4 4 2 6 sample(1:nrow(somedata), nrow(somedata), replace = TRUE) [1] 7 2 6 4 2 6 4 What happened is the function generated a set of row numbers. Note how it is possible for the same row number to be picked multiple times. Each time we run the sample function, we get slightly different row numbers. We can subset our data based on these row indices. ## store row indices wrows &lt;- sample(1:nrow(somedata), nrow(somedata), replace = TRUE) wrows [1] 3 6 5 6 2 4 4 ## subset data to include rows sampled ## note if row indices are in wrows more than once, they will also be in the subset more than once subdata &lt;- somedata[wrows,] subdata colors shapes 3 yellow triangle 6 orange line 5 purple diamond 6.1 orange line 2 blue square 4 green rectangle 4.1 green rectangle Given that each time the sample function runs, we get slightly different random samples of the data, that’s how we end up with a distribution of slightly different estimates of our quantities of interest. Each time the regression is run with a slightly different dataset. This gives us one estimate of the average predicted probability stored in p.boot. However, the idea of a bootstrap is that we repeat this procedure at least 1000 times to generate a distribution of estimates of the quantity of interest, the average predicted probability in this case. We could literally repeat that code chunk 1000 times…. but, we have better things to do than that much copy/paste. Instead, we will create a function that will do this automatically. To do so, we are going to wrap our procedure above inside the syntax for creating functions in R. In R, to create a function, We first name the function. (Let’s call this myboot. You could call yours anything.) The next syntax is always myboot &lt;- function(){}. Inside the function() part, you tell R what you are going to supply the function each time you want it to run. Sometimes functions only have one input, others like lm have multiple inputs. For example, in the function mean(x), we always supply that function with a vector of values. For this bootstrap example, we are going to write the function as one where we will supply the function with a dataframe. Let’s call this df. The inside part of the function, between the {} is the procedure from above. All we do is Instead of writing mids, we keep it generic by writing df. We add a final line that tells R what we want it to return() as the output of the function. Here, we want it to return the average predicted probability. ## We need to build our bootstrap function ## Step 4: Let&#39;s wrap our current steps into a function that we can replicate ## Note: all we need as an input is our data.frame mids ## I will label it something generic to show how a function can work myboot &lt;- function(df){ wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T) ## Create subset of data based on these rows subdata &lt;- df[wrows, ] ## Step 2: run your regression model with the new data boot.logit &lt;-glm(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, family = binomial(link = &quot;logit&quot;), data = subdata) ## Step 3: generate average predicted probability Xboot &lt;- model.matrix(boot.logit) Xboot[, &quot;Allies&quot;] &lt;- 1 Bh &lt;- coef(boot.logit) p.boot &lt;- mean(plogis(Xboot %*% Bh)) return(p.boot) } Note: here, our quantity of interest is the predicted probability of a dispute when the dyad are Allies. Let’s say, instead, we wanted the difference in predicted probability of a dispute between Allies and Non-Allies. Well, we would just adjust our function to calculate the mean probabilities for Allies and Non-Allies and return the difference in these means as the quantity of interest. We would then get 10000 estimates of this difference in probabilties. Now that we have the function from above, instead of copying/pasting this 1000 times, we will use the function called replicate which will do this for us. We indicate the number of estimates we want and then indicate which function (and in our case, which dataframe inside the function) we want to replicate. ## This may take a minute to run. ## We will do just 50, Normally you will want this to be more like 1000 set.seed(1234) # this helps us get the same results each time, good for reproducibility myestimates &lt;- replicate(50, myboot(mids)) The bootstrapping approach is very computationally demanding given it has to repeat an operation several (thousand) times. After you hit “run,” just sit back, relax and wait for the water to run dry. For a troubleshooting tip, expand. If you get an error message at the replicate(1000, myboot(mids)) stage, it is best to see if your function runs at all. Try just the below to see if it generates output: myboot(mids) [1] 0.002250429 If you get the error here, then it means there is a bug within the function code, not the replicate code. Each time we replicate the function, it will generate slightly different results because the sample functions is randoming sampling rows of data each time. We can plot the distribution of estimates to show this. library(ggplot2) ggplot(data.frame(x = myestimates), aes(x = myestimates)) + geom_histogram(aes(y=..density..)) + geom_density(color=&quot;red&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The final step after generating the bootstrap distribution of estimates is to use it to construct a confidence interval for the quantity of interest. There are a few ways to do this. Normal approximation Percentile Bias correction In each approach, we take our original “point estimate” from the computation of the quantity of interest from our original data and use the bootstrap estimates for the lower and upper bounds of the confidence interval. Here we will assume we want a 95% confidence interval. ## Find the original point estimate Bh &lt;- coef(out.logit) X1 &lt;- model.matrix(out.logit) X1[, &quot;Allies&quot;] &lt;- 1 pe1 &lt;- mean(plogis(X1 %*% Bh)) ## Normal c((pe1 - qnorm(.975)*sqrt(var(myestimates))),(pe1 + qnorm(.975)*sqrt(var(myestimates)))) [1] 0.002174802 0.003345002 ## Percentile quantile(myestimates, c(0.025, .975)) 2.5% 97.5% 0.002275356 0.003431285 ## Bias correction bc &lt;- 2*pe1 - myestimates quantile(bc, c(0.025, .975)) 2.5% 97.5% 0.002088520 0.003244449 Each of these is pretty commonly used, but they may generate slightly different results. 7.3.2 Simulated Confidence Intervals Quasi-Bayesian or simulated confidence intervals take advantage of the large sample properties of our estimates \\(\\hat \\beta\\) having a Normal sampling distribution due to the Central Limit Theorem. Like the bootstrap, the simulation procedure also generates hypothetical new samples. However, here, we are sampling new \\(\\hat \\beta\\) estimates each time instead of sampling a new underlying dataset each time. This allows use to skip the step of generating a new dataset and running the regression 1000 times. Here, we just run the regression model once. The simulation process takes place after this step. Process Estimate your model (e.g., with optim or glm) Sample \\(\\sim\\) 1000 new estimates of the vector of \\(\\hat \\beta\\) by using the vcov of \\(\\hat \\beta\\) to generate the uncertainty For each of these new \\(\\hat \\beta_c\\), calculate \\(\\hat \\theta_c\\), in this case, the predicted probabilities. Estimate “fundamental uncertainty” by drawing new y’s based on these parameters Only necessary in some cases. Depends on Jensen’s Inequality discussed in the Gary King resources and in the details on the use of rbinom function below. We are going to average over this, which means we are calculating “expected values.” Use this distribution to compute the CI’s This would be a good place to review the resources from Gary King: Overview of simulation approach for calculating uncertainty from King, Tomz, and Wittenberg 2000. Lecture video from Gary King on simulating quantities of interest Example The code for this approach will more simple in a case where we are computing quantities of interest when covariates are held at means or representative values (cases where we get just one predicted probability associated with each set of \\(X\\) values). It will look a little more complex in cases where we want to hold covariates at observed values and calculate the average predicted probability. First, let’s find the point estimate and 95% CI for the predicted probability of conflict when the dyad are Allies, and all other covariates are held at zero. ## install.packages(&quot;mvtnorm&quot;) library(mvtnorm) ## Step 2: Sample 1000 new Bhs (we will use 50 for this example) ## This uses the multivariate normal distribution for resampling set.seed(1234) numsims &lt;- 50 qb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) ## This generates numsims X k coefficients matrix ## Step 3: Iterate through the estimates ## Create an empty vector to store 1000 quantities of interest qestimatessimple &lt;- rep(NA, numsims) ## Here, our covariate matrix stays the same each time ## We have a 1 for intercept and 1 for Allies, everything else at zero X1 &lt;- cbind(1, 0, 0, 1, 0, 0 , 0) ## X1 is a 1 X k matrix ## Use a loop to iterate through each set of betas for(i in 1:numsims){ ## for each set of betas, calculate p probs ## for a given set of betas, this gives us nrow(X1) predicted probabilities pestimate &lt;-plogis(X1 %*% qb.beta[i,]) ## Fundamental uncertainty ## not required for logit/probit, but we will show how ## rbinom generates 1000 0&#39;s or 1&#39;s based on the predicted probability ## We use rbinom bc of the bernoulli, other glm&#39;s will have other distributions ## then we take the mean to get our estimate of the predicted probability moutcome &lt;- mean(rbinom(numsims, 1, pestimate)) qestimatessimple[i] &lt;-moutcome ## repeat for set of simulated betas } ## Step 4: Similar to bootstrap distribution, find CI using the percentiles quantile(qestimatessimple, c(0.025, 0.975)) 2.5% 97.5% 0.00 0.02 For more information on loops in R, you can follow this tutorial we created for 2020 SICSS-Rutgers. Expand for details on rbinom. rbinom is the random generation function for the binomial distribution. If we supply it with number of trials (in the Bernoulli, this is 1), and a probability of success, it will generate our desired number of outcomes according to this distribution. For example, let’s say we wanted to generate a random set of 100 coin flips for a coin that is fair– where the probability of success is .5. We will get a sample of 0’s and 1’s. If we take the mean, it will be close to .5, and with enough coin flips, will converge on .5. rb.res &lt;- rbinom(100, 1, .5) rb.res [1] 0 1 0 0 0 0 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1 0 1 0 1 0 1 1 1 1 0 0 [38] 0 0 0 0 1 1 1 1 0 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 1 [75] 1 1 1 1 0 0 1 0 1 0 0 1 0 1 1 0 1 1 1 1 1 1 0 1 0 0 mean(rb.res) [1] 0.49 In logit/probit, this step is unneccessary because the \\(\\mathbb{E}(y_i) = \\pi_i\\). When we take the mean of our rbinom estimates, we are just going to recover the probability we supplied to it. However, in other cases, Jensen’s inequality may apply, which states that \\(\\mathbb{E}[g(X)] \\neq g(\\mathbb{E}[X])\\). For example, if we have an outcome that is distributed according to the exponential distribution: Here, the \\(\\theta\\) is \\(\\lambda\\) where \\(\\lambda = \\frac{1}{e^{X\\beta}}\\) but \\(\\mathbb{E}(y)= \\frac{1}{\\lambda}\\). Unfortunately, \\(\\mathbb{E}(\\frac{1}{\\hat \\lambda}) \\neq \\frac{1}{\\mathbb{E}(\\hat \\lambda)} = \\frac{1}{\\mathbb{E}(e^{X\\beta})}\\). For that example, the rexp() step in this case would be essential. If we’re not sure when Jensen’s inequality will apply, we can just keep the fundamental uncertainty step as part of the process. Find the point estimate and 95% CI for the average predicted probability of conflict when the dyad are allies and all other covariates are held at observed values. Here, the code is more complicated, because every time we generate a predicted probability (for any observed value), we need to go through the fundamental uncertainty step (when applicable). ## install.packages(&quot;mvtnorm&quot;) library(mvtnorm) ## Step 2: Sample 1000 new Bhs (we will use 50 for this example) ## This uses the multivariate normal distribution for resampling set.seed(1234) numsims &lt;- 50 qb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) ## This generates numsims X k coefficients matrix ## Step 3: Iterate through the estimates ## Create an empty vector to store 1000 quantities of interest qestimates &lt;- rep(NA, numsims) ## Here, our covariate matrix stays the same X1 &lt;- model.matrix(out.logit) X1[, &quot;Allies&quot;] &lt;- 1 ## Use a loop to for(i in 1:numsims){ ## for each set of betas, calculate p probs ## for a given set of betas, this gives us nrow(X1) predicted probabilities pestimates &lt;-plogis(X1 %*% qb.beta[i,]) ## Fundamental uncertainty ## not required for logit/probit, but we will show how ## generate empty vector for outcomes moutcomes &lt;- rep(NA, numsims) ## for each probability estimate, calculate the mean of simulated y&#39;s for(j in 1:length(pestimates)){ ## rbinom generates 1000 0&#39;s or 1&#39;s based on the predicted probability ## We use rbinom bc of the bernoulli, other glm&#39;s will have other distributions ## then we take the mean to get our estimate of the predicted probability for a given observation moutcomes[j] &lt;- mean(rbinom(numsims, 1, pestimates[j])) } ## take the mean of the predicted probability estimates across all observations qestimates[i] &lt;- mean(moutcomes) ## repeat for set of simulated betas } ## Step 4: Similar to bootstrap distribution, find CI using the percentiles quantile(qestimates, c(0.025, 0.975)) 2.5% 97.5% 0.00232977 0.00344796 Because the shortcut applies where we do not need to calculate fundamental uncertainty in the logit / probit case, we can simplify this to: ## install.packages(&quot;mvtnorm&quot;) library(mvtnorm) ## Step 2: Sample 1000 new Bhs (we will use 50 for this example) ## This uses the multivariate normal distribution for resampling set.seed(1234) numsims &lt;- 50 qb.beta &lt;- rmvnorm(numsims, coef(out.logit), vcov(out.logit)) ## This generates numsims X k coefficients matrix ## Step 3: Iterate through the estimates ## Create an empty vector to store 1000 quantities of interest qestimates &lt;- rep(NA, numsims) ## Here, our covariate matrix stays the same X1 &lt;- model.matrix(out.logit) X1[, &quot;Allies&quot;] &lt;- 1 for(i in 1:numsims){ pestimates &lt;-plogis(X1 %*% qb.beta[i,]) qestimates[i] &lt;- mean(pestimates) } ## Step 4: Similar to bootstrap distribution, find CI using the percentiles quantile(qestimates, c(0.025, 0.975)) 2.5% 97.5% 0.002314344 0.003419644 We can also plot the distribution of estimates library(ggplot2) ggplot(data.frame(x = qestimates), aes(x = qestimates)) + geom_histogram(aes(y=..density..)) + geom_density(color=&quot;red&quot;) `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. "],
["visualizing-results.html", "7.4 Visualizing Results", " 7.4 Visualizing Results Now that we have our point estimates of our quantities of interest and our confidence intervals, we can kick it up a notch by visualizing these results. Let’s plot our predicted probability when Allies = 1 with percentile Quasi-Bayesian CI’s and Bias-Corrected Bootstrap CI’s plot(x = 1:2, y= c(pe1, pe1), ylim = c(0, .004), xlim = c(0.5, 2.5), pch = 20, cex = 1.6, main = &quot;Average Predicted Probability of Dispute when Allies&quot;, cex.main = .7, cex.lab = .7, cex.axis = .7, ylab = &quot;Predicted Probability&quot;, xlab = &quot;&quot;, xaxt = &quot;n&quot;) # removes x-axis # add lines from c(x1, x2) on the x-axis and from c(y1, y2) on the y-axis # note, we don&#39;t want any horizontal movement, so we keep x1 and x2 the same c(1,1) lines(c(1,1), c(quantile(qestimates, c(0.025, 0.975))), lwd = 1.5) lines(c(2,2), c(quantile(bc, c(0.025, .975))), lwd = 1.5) # add text a text(c(1, 2), c(0.001, 0.001), c(&quot;Quasi-Bayesian&quot;, &quot;BC Bootstrap&quot;), cex = .7 ) Here is the same but in ggplot form. ## ggplot works best if you create a dataframe of the data you want to plot myg &lt;- data.frame(rbind(c(pe1, quantile(qestimates, c(0.025, 0.975))), c(pe1, quantile(bc, c(0.025, .975))))) colnames(myg) &lt;- c(&quot;pp&quot;, &quot;qb&quot;, &quot;bc&quot;) myg pp qb bc 1 0.002759902 0.002314344 0.003419644 2 0.002759902 0.002088520 0.003244449 ## now provide this dataframe to ggplot ggplot(myg, aes(x = 1:nrow(myg), y = pp))+ geom_point(stat = &quot;identity&quot;, size = 3) + geom_errorbar(data=myg, aes(x =1:nrow(myg), ymin = qb, ymax = bc), width = 0, size = 1.1) + xlim(.5, 2.5) + ylim(0, .005) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank(), plot.title = element_text(hjust = 0.5)) + ylab(&quot;Predicted Probability&quot;) + ggtitle(&quot;Average Predicted Probability of Dispute when Allies&quot;) + annotate(&quot;text&quot;, x = c(1,2), y = .004, label = c(&quot;Quasi-Bayesian&quot;, &quot;BC Bootstrap&quot;)) "],
["additional-r-shortcuts.html", "7.5 Additional R shortcuts", " 7.5 Additional R shortcuts There are a few R packages that help generate these quantities of interest AND estimate uncertainty. If you understand what is going on underneath the packages, then you should feel free to use them to avoid manually coding up each process. 7.5.1 Prediction We’ve already used the prediction package, but now let’s add uncertainty calculation to this. This package is useful for generating confidence intervals around single quantities of interest. The package below is better for constructing confidence intervals around the difference between quantities of interest. library(prediction) preout &lt;- prediction(out.logit, at = list(Allies = c(0, 1)), type = &quot;response&quot;, calculate_se = T) summary(preout) at(Allies) Prediction SE z p lower upper 0 0.003711 0.0002270 16.343 4.878e-60 0.003266 0.004156 1 0.002760 0.0003135 8.804 1.322e-18 0.002145 0.003374 I believe prediction relies on the delta method for uncertainty. The others below will have options for simulations or bootstrapped standard errors. 7.5.2 Margins Thomas Leeper developed a package called margins which is modeled after the Stata margins command. Here is some documentation for this package. We first fit a regression model like normal. out &lt;- glm(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, data=mids, family=binomial(link = &quot;logit&quot;)) Open the package and use the margins command. It is similar to prediction but we will specify the uncertainty with vce and tell it which variable we want the marginal effect, and what type of change in that variable we want to calculate the effect. ## install.packages(&quot;margins&quot;) library(margins) Warning: package &#39;margins&#39; was built under R version 4.0.2 ## Difference in Allies 0 vs. 1 holding other covariates at 0 ## Using Delta Method for uncertainty marg1 &lt;- margins(out, vce = &quot;delta&quot;, at = list(MajorPower=0, Contiguity=0, ForeignPolicy=0, BalanceOfPower=0, YearsSince=0), variables = &quot;Allies&quot;, change = c(0, 1), type=&quot;response&quot;) Warning in check_values(data, at): A &#39;at&#39; value for &#39;BalanceOfPower&#39; is outside observed data range (0.000173599997651763,1)! summary(marg1) factor MajorPower Contiguity ForeignPolicy BalanceOfPower YearsSince AME Allies 0.0000 0.0000 0.0000 0.0000 0.0000 -0.0010 SE z p lower upper 0.0004 -2.3910 0.0168 -0.0018 -0.0002 Let’s try a second example, shifting the uncertainty estimate and the X covariates. ## Difference in Allies 0 vs. 1 holding MajorPower at 1, other covariates at observed values ## Using simulations for uncertainty marg2 &lt;- margins(out, vce = &quot;simulation&quot;, at = list(MajorPower =1), variables = &quot;Allies&quot;, change = c(0, 1), type=&quot;response&quot;) summary(marg2) factor MajorPower AME SE z p lower upper Allies 1.0000 -0.0020 0.0010 -1.9576 0.0503 -0.0041 0.0000 ## Manual equivalent of point estimate X.marg1 &lt;- model.matrix(out) X.marg1[, &quot;Allies&quot;] &lt;-1 X.marg1[, &quot;MajorPower&quot;] &lt;- 1 X.marg0 &lt;- model.matrix(out) X.marg0[, &quot;Allies&quot;] &lt;-0 X.marg0[, &quot;MajorPower&quot;] &lt;- 1 BH &lt;- coef(out) ame &lt;- mean(plogis(X.marg1 %*% BH) - plogis(X.marg0 %*% BH)) ## Compare ame [1] -0.002039019 summary(marg2)$AME Allies -0.002039019 7.5.3 Zelig Zelig has a lot of built-in plotting functions. This can be a great package if you want to present results the way they have set up the package. They make it slightly harder to extract specific elements from the results. Documentation here. ## install.packages(&quot;Zelig&quot;) library(Zelig) z.out &lt;- zelig(Conflict ~ MajorPower + Contiguity + Allies + ForeignPolicy + BalanceOfPower + YearsSince, data=mids, model = &quot;logit&quot;, cite = F) ## Use function setx to designate values of X ## Let&#39;s set everything to 0 set.x &lt;- setx(z.out, MajorPower=0, Contiguity=0, Allies=0, ForeignPolicy=0, BalanceOfPower=0, YearsSince=0) s.out1 &lt;- sim(z.out, set.x, num = 100) # should do more like 1000 ## Expected value and 95% CI s.out1 sim x : ----- ev mean sd 50% 2.5% 97.5% [1,] 0.003755489 0.001072985 0.003499368 0.002113449 0.006454072 pv 0 1 [1,] 1 0 ## Change Allies to 1, and let&#39;s look at the difference between Allies vs. Non-Allies set.x1 &lt;- setx1(z.out, MajorPower=0, Contiguity=0, Allies=1, ForeignPolicy=0, BalanceOfPower=0, YearsSince=0) ## Let&#39;s look at the QOI that is the difference between these s.out2 &lt;- sim(z.out, set.x, set.x1, num = 50) ## Get mean and CI&#39;s from distribution mean(s.out2$get_qi(qi=&quot;fd&quot;, xvalue=&quot;x1&quot;)) [1] -0.0009859548 quantile(s.out2$get_qi(qi=&quot;fd&quot;, xvalue=&quot;x1&quot;), c(0.025, 0.975)) 2.5% 97.5% -0.0018093458 -0.0002380693 7.5.4 Using expand.grid When we use the predict function in R, we specify a “`newdata” dataframe to indicate for which values of \\(X\\) we want to estimate values of the outcome. When we do this, we are actually building a dataframe, like the below: df &lt;- data.frame(MajorPower = 0, Allies=1, Contiguity = 0) df MajorPower Allies Contiguity 1 0 1 0 In cases where we want to make predictions for multiple values of a given variable, we can feed a vector into the data.frame: df &lt;- data.frame(MajorPower = 0, Allies=c(0,1), Contiguity = 0) df MajorPower Allies Contiguity 1 0 0 0 2 0 1 0 However, this becomes more tedious when we want to estimate for combinations of different variables (e.g., MajorPower and Allies as 0 or 1). You have to tell R precisely which rows should include which values. This means indicating four separate values for each variables to get all possible combinations. df &lt;- data.frame(MajorPower = c(0, 0, 1,1), Allies=c(0,1, 0, 1), Contiguity = 0) df MajorPower Allies Contiguity 1 0 0 0 2 0 1 0 3 1 0 0 4 1 1 0 What expand.grid does is let you more quickly indicate that you want to estimate for all possibles combinations of the values of the variables you supply. E.g.: df &lt;- expand.grid(MajorPower = c(0, 1), Allies=c(0,1), Contiguity = 0) df MajorPower Allies Contiguity 1 0 0 0 2 1 0 0 3 0 1 0 4 1 1 0 This can be a useful shortcut when you need to look at combinations of variables that have many different values. "],
["week-4-tutorial.html", "7.6 Week 4 Tutorial", " 7.6 Week 4 Tutorial Let’s return to the example at the end of section 6, and calculate predicted probabilities, now with estimates of uncertainty. Recall the Banks and Hicks data include the following variables below. We add partyid as a variable for this analysis. abtrace1: 1= if the respondent thought the ad was about race. 0= otherwise condition2: 1= respondent in the implicit condition. 2= respondent in one of four explicit racism conditions. racresent: a 0 to 1 numeric variable measuring racial resentment oldfash: a 0 to 1 numeric variable measuring “old-fashioned racism” trumvote: 1= respondent has vote preference for Trump 0=otherwise partyid: A 1 to 7 numeric variables indicating partisanship from strong Democrat to strong Republican. Below 4 is a Democrat/Democratic leaner, above 4 is a Republican/Republican leaner. Let’s load the data again. ## install.packages(&quot;rio&quot;) library(rio) study &lt;- import(&quot;https://github.com/ktmccabe/teachingdata/blob/main/ssistudyrecode.dta?raw=true&quot;) Let’s set the experiment aside for now and focus on vote choice as the outcome: trumvote. Let’s suppose we were interested in understanding whether partisanship influences vote choice. What would be a simple regression model we could run? (e.g., what would be on the left, what would be on the right?) What type of model should it be? (e.g., OLS, glm, logit, probit, etc.?) Run this model below What do you conclude about the influence of party on vote choice? Try on your own, then expand for one solution. fit1 &lt;- glm(trumvote ~ partyid, data=study, family=binomial(link=&quot;probit&quot;)) library(texreg) texreg::knitreg(fit1) Statistical models Model 1 (Intercept) -2.06*** (0.13) partyid 0.43*** (0.03) AIC 1056.38 BIC 1066.23 Log Likelihood -526.19 Deviance 1052.38 Num. obs. 1019 p &lt; 0.001; p &lt; 0.01; p &lt; 0.05 We took a probit approach. What other approaches could we have taken? In a glm model, we have to make a leap from a probit/logit coefficient to vaguely answer our research question. What we want to do instead, is become more focused in our research questions, hypotheses, and estimations to generate precise quantities of interest that speak directly to our theoretical questions. Let’s try this again. Let’s be more focused in our question/hypothesis. As people shift from strong Democrats to strong Republicans, how does their probability of voting for Trump change? How can we transform our model from before into quantities that speak directly to this question? Computing probabilities. We only have one covariate, so this becomes an easier problem. newdata.party &lt;- data.frame(partyid = 1:7) ## Set type to be response results1 &lt;- predict(fit1, newdata = newdata.party, type = &quot;response&quot;) x 0.0512844 0.1147444 0.2204047 0.3669367 0.5362029 0.6990679 0.8295963 How can we best communicate this to our readers? Computing probabilities. plot(x=1:7, y=results1, main = &quot;Predicted probability by partisanship&quot;, type=&quot;b&quot;, xlab = &quot;Partisanship: Strong Dem to Strong Rep&quot;, ylab=&quot;Predicted Probability&quot;) library(ggplot2) ggres &lt;- data.frame(probs=results1, partisanship=1:7) ggplot(ggres, aes(x=partisanship, y=probs))+ geom_line()+ geom_point()+ xlab(&quot;Partisanship: Strong Dem to Strong Rep&quot;)+ ylab(&quot;Predicted Probability&quot;)+ ggtitle(&quot;Predicted probability by partisanship&quot;) How can we improve this even further? Calculate uncertainty! Again, because we only have one covariate, the process is a little more simple. Let’s use the predict function to calculate the standard errors for us on the link scale with se.fit = T and then construct our own confidence intervals. Try on your own, then expand for the solution. results1.link &lt;- predict(fit1, newdata=newdata.party, type=&quot;link&quot;, se.fit=T) results1.link $fit 1 2 3 4 5 6 7 -1.6325260 -1.2016765 -0.7708270 -0.3399775 0.0908720 0.5217215 0.9525710 $se.fit 1 2 3 4 5 6 7 0.10002223 0.07628414 0.05617531 0.04487352 0.04892006 0.06553079 0.08784628 $residual.scale [1] 1 Now for each value, we get the standard error estimate. We now have to convert these to the response scale. m.results &lt;- pnorm(results1.link$fit) results1.lb &lt;- pnorm(results1.link$fit - qnorm(.975)*results1.link$se.fit) results1.ub &lt;-pnorm(results1.link$fit + qnorm(.975)*results1.link$se.fit) ## Let&#39;s look at the results from the original point estimates and this approach cbind(results1, m.results, results1.lb, results1.ub) results1 m.results results1.lb results1.ub 1 0.05128436 0.05128436 0.03373233 0.07543205 2 0.11474445 0.11474445 0.08831718 0.14636254 3 0.22040474 0.22040474 0.18917824 0.25439421 4 0.36693674 0.36693674 0.33435178 0.40051009 5 0.53620285 0.53620285 0.49800149 0.57407307 6 0.69906787 0.69906787 0.65294495 0.74220540 7 0.82959626 0.82959626 0.78242093 0.86965177 Finally, let’s add it to our plot. plot(x=1:7, y=results1, main = &quot;Predicted probability by partisanship&quot;, type=&quot;b&quot;, xlab = &quot;Partisanship: Strong Dem to Strong Rep&quot;, ylab=&quot;Predicted Probability&quot;) points(x=1:7, y= results1.ub, type=&quot;l&quot;) points(x=1:7, y= results1.lb, type=&quot;l&quot;) library(ggplot2) ggres &lt;- data.frame(cbind(m.results, results1.lb, results1.ub), partisanship=1:7) ggplot(ggres, aes(x=partisanship, y=m.results))+ geom_line()+ geom_point()+ #geom_ribbon(aes(ymin=results1.lb, ymax=results1.ub), alpha=.5)+ geom_errorbar(aes(ymin=results1.lb, ymax=results1.ub), width=.02)+ xlab(&quot;Partisanship: Strong Dem to Strong Rep&quot;)+ ylab(&quot;Predicted Probability&quot;)+ ggtitle(&quot;Predicted probability by partisanship&quot;) The prediction package in R will give us a shortcut by calculating the confidence intervals for us. We can repeat the previous process inside this function. # install.packages(&quot;prediction&quot;) library(prediction) ## prediction can take a new dataframe or specific values of covariates pred.results &lt;- prediction(fit1, at=list(partyid = 1:7), type=&quot;response&quot;, calculate_se = T) summary(pred.results) at(partyid) Prediction SE z p lower upper 1 0.0512844 0.0105264 4.871990 1.1e-06 0.0306531 0.0719157 2 0.1147444 0.0147835 7.761644 0.0e+00 0.0857693 0.1437196 3 0.2204047 0.0166507 13.236974 0.0e+00 0.1877700 0.2530395 4 0.3669367 0.0168967 21.716505 0.0e+00 0.3338199 0.4000536 5 0.5362029 0.0194359 27.588317 0.0e+00 0.4981093 0.5742965 6 0.6990679 0.0228165 30.638670 0.0e+00 0.6543483 0.7437874 7 0.8295963 0.0222636 37.262407 0.0e+00 0.7859604 0.8732322 When we have covariates, we have a few more decisions to make about how to calculate quantities of interest and how to compute uncertainty. Let’s amend our model to include additional covariates. fit2 &lt;- glm(trumvote ~ partyid + racresent + oldfash, data=study, family=binomial(link=&quot;probit&quot;)) Now, we have the same research question, but we have covariates. We have to decide how we want to calculate the predicted probabilities of voting for Trump at different levels of partisanship. Where should we set racresent and oldfash when computing these values? Let’s suppose we want to hold them at observed values. This means we will calculate the average predicted probability of voting for Trump at each value of partisanship, holding the other covariates at observed values. We can do this in a few ways. Here, let’s do this manually: bh &lt;- coef(fit2) ## party id 1 X.1 &lt;- model.matrix(fit2) X.1[, &quot;partyid&quot;] &lt;- 1 p.1 &lt;- pnorm(X.1 %*% bh) p.1.mean &lt;- mean(p.1) ## party id 2 X.2 &lt;- model.matrix(fit2) X.2[, &quot;partyid&quot;] &lt;- 2 p.2 &lt;- pnorm(X.2 %*% bh) p.2.mean &lt;- mean(p.2) ## More efficient approach 1: p.means &lt;- rep(NA, 7) for(i in 1:7){ X &lt;- model.matrix(fit2) X[, &quot;partyid&quot;] &lt;- i p &lt;- pnorm(X %*% bh) p.means[i] &lt;- mean(p) } ## More efficient approach 2: myest &lt;- function(value){ X &lt;- model.matrix(fit2) X[, &quot;partyid&quot;] &lt;- value p &lt;- pnorm(X %*% bh) p.mean &lt;- mean(p) return(p.mean) } p.means &lt;- sapply(1:7, myest) Or, we can use prediction again. pred.results2 &lt;- prediction(fit2, at=list(partyid = 1:7), type=&quot;response&quot;) Let’s compare the output: cbind(p.means, summary(pred.results2)$Prediction) p.means [1,] 0.08296357 0.08296357 [2,] 0.14824083 0.14824083 [3,] 0.24098759 0.24098759 [4,] 0.35835987 0.35835987 [5,] 0.49072437 0.49072437 [6,] 0.62383575 0.62383575 [7,] 0.74331307 0.74331307 What if we want uncertainty? We can bootstrap or simulate confidence intervals around each quantity of interest, or use a function to do this for us. Bootstrap Details ## Step 1: sample new rows of the data and subset wrows &lt;- sample(x =1:nrow(study), size = nrow(study), replace = T) subdata &lt;- study[wrows, ] ## Step 2: run your regression model with the new data boot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, data=subdata, family=binomial(link=&quot;probit&quot;)) ## Step 3: generate average predicted probability Xboot &lt;- model.matrix(boot.probit) Xboot[, &quot;partyid&quot;] &lt;- 1 Bh &lt;- coef(boot.probit) p.boot &lt;- mean(pnorm(Xboot %*% Bh)) ## Step 4: wrap it in a function, make data generic myboot &lt;- function(df){ wrows &lt;- sample(x =1:nrow(df), size = nrow(df), replace = T) subdata &lt;- df[wrows, ] boot.probit &lt;-glm(trumvote ~ partyid + racresent + oldfash, data=subdata, family=binomial(link=&quot;probit&quot;)) Xboot &lt;- model.matrix(boot.probit) Xboot[, &quot;partyid&quot;] &lt;- 1 Bh &lt;- coef(boot.probit) p.boot &lt;- mean(pnorm(Xboot %*% Bh)) return(p.boot) } ## Step 5: Uncomment and replicate 1000 times #bestimates.party1 &lt;- replicate(1000, myboot(study)) ## Extract confidence interval #quantile(bestimates.party1, c(0.025, .975)) We would then repeat this for each partyid value. Alternatively, we could use prediction! "],
["putting-everything-together.html", "7.7 Putting everything together", " 7.7 Putting everything together Recall, we can take something that looks like column 5 in the table from Antoine Banks and Heather Hicks example from the previous section and move it into a figure, as the authors did. Let’s run the model from column 5. fit.probit5 &lt;- glm(abtrace1 ~ factor(condition2)*racresent + factor(condition2)*oldfash, data=study, family=binomial(link = &quot;probit&quot;)) Let’s generate predicted probabilities for thinking the ad is about race across levels of racial resentment in the sample, for people in the implicit and explicit conditions, holding all covariates at observed values. library(prediction) pr.imp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625), condition2=1), calculate_se = TRUE) ## Let&#39;s store the summary output this time ## And to make it easier to plot, we&#39;ll store as dataframe pr.imp.df &lt;- summary(pr.imp) pr.exp &lt;- prediction(fit.probit5, at= list(racresent = seq(0, 1,.0625), condition2=2), calculate_se = TRUE) pr.exp.df &lt;- summary(pr.exp) You can peek inside pr.imp.df to see the format of the output. Let’s now visualize! We will try to stay true to the authors’ visual choices here. ## Plot results plot(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$Prediction, type=&quot;l&quot;, ylim = c(0, 1), lty=2, ylab = &quot;Predicted Probability&quot;, xlab = &quot;Racial Resentment&quot;, main = &quot;Predicted Probability of Viewing the Ad as about Race&quot;, cex.main = .7) ## add explicit point values points(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$Prediction, type=&quot;l&quot;) ## add additional lines for the upper and lower confidence intervals points(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$lower, type=&quot;l&quot;, col=&quot;gray&quot;) points(x=pr.exp.df$`at(racresent)`, y=pr.exp.df$upper, type=&quot;l&quot;, col=&quot;gray&quot;) points(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$lower, type=&quot;l&quot;, lty=3) points(x=pr.imp.df$`at(racresent)`, y=pr.imp.df$upper, type=&quot;l&quot;, lty=3) ## Legend legend(&quot;bottomleft&quot;, lty= c(2,1, 3,1), c(&quot;Implicit&quot;, &quot;Explicit&quot;, &quot;Implicit 95% CI&quot;, &quot;Explicit 95% CI&quot;), cex=.7) Let’s combine the two dataframes. pr.comb &lt;- rbind(pr.imp.df, pr.exp.df) library(ggplot2) ggplot(pr.comb, aes(x=`at(racresent)`, y= Prediction, color=as.factor(`at(condition2)`)))+ geom_line()+ geom_ribbon(aes(ymin=lower, ymax=upper, fill=as.factor(`at(condition2)`)), alpha=.5)+ xlab(&quot;Racial Resentment&quot;)+ theme_bw()+ theme(legend.position = &quot;bottom&quot;) + scale_color_discrete(name=&quot;Condition&quot;, breaks=c(1,2), labels=c(&quot;Implicit&quot;, &quot;Explicit&quot;))+ scale_fill_discrete(name=&quot;Condition&quot;, breaks=c(1,2), labels=c(&quot;Implicit&quot;, &quot;Explicit&quot;)) We could instead show the difference between the conditions across levels of racial resentment. library(margins) marest &lt;- margins(fit.probit5, at= list(racresent = seq(0, 1,.0625)), variables=&quot;condition2&quot;, change = c(1,2), vce=&quot;delta&quot;, type=&quot;response&quot;) ## Store summary as dataframe marest.df &lt;- summary(marest) ## plot res ggplot(marest.df, aes(x=racresent, y=AME))+ geom_line()+ geom_errorbar(aes(ymin=lower, ymax=upper), alpha=.5, width=0)+ theme_bw()+ xlab(&quot;Racial Resentment&quot;)+ ggtitle(&quot;AME: Explicit - Implicit Condition on Pr(Ad About Race)&quot;)+ geom_hline(yintercept = 0, color=&quot;red&quot;) "],
["ordinal.html", "Section 8 Ordinal Outcomes", " Section 8 Ordinal Outcomes This section will cover ordinal logit and probit regression. These are models that work well for outcome variables that include a set of (more than 2) ordered discrete categories. Often, survey responses have this form of outcome (e.g., a likert scale from “strongly agree” to “strongly disagree”). We also might categorize behavioral responses in an ordinal way from “stay home” to “protest”, for example. You can review the following for additional external resources on this section. King, Gary. 1998. Unifying political methodology: The likelihood theory of statistical inference. University of Michigan Press. Chapter 5.4. Resources for ordinal models in R here and here A section from Gelman and Hill Chapter 6, posted to Canvas. "],
["ordinal-outcome-data.html", "8.1 Ordinal Outcome Data", " 8.1 Ordinal Outcome Data Here is a motivating example for the use of ordered data from Paluck and Green “Deference, Dissent, and Dispute Resolution: An Experimental Intervention Using Mass Media to Change Norms and Behavior in Rwanda” which was published in the American Political Science Review in 2009. doi:10.1017/S0003055409990128 Abstract. Deference and dissent strike a delicate balance in any polity. Insufficient deference to authority may incapacitate government, whereas too much may allow leaders to orchestrate mass violence. Although cross-national and cross-temporal variation in deference to authority and willingness to express dissent has long been studied in political science, rarely have scholars studied programs designed to change these aspects of political culture. This study, situated in post-genocide Rwanda, reports a qualitative and quantitative assessment of one such attempt, a radio program aimed at discouraging blind obedience and reliance on direction from authorities and promoting independent thought and collective action in problem solving. Over the course of one year, this radio program or a comparable program dealing with HIV was randomly presented to pairs of communities, including communities of genocide survivors, Twa people, and imprisoned genocidaires … Although the radio program had little effect on many kinds of beliefs and attitudes, it had a substantial impact on listeners’ willingness to express dissent and the ways they resolved communal problems. In a field experiment, the authors have randomly assigned participants in different research sites to listen to a radio program over the course of a year that varied in its message. As the authors note, “Because radios and batteries are relatively expensive for Rwandans, they usually listen to the radio in groups. Thus, we used a group-randomized design in which adults from a community listened together either to the treatment (reconciliation) program or to the control program (another entertainment-education radio soap opera about health and HIV).” The authors have 14 clusters without 40 individuals within each cluster. Treatment (treat): radio program with one of two messages, where 1=the treatment condition with a reconciliation message and 0=control, listening to a health message. Outcome (dissent): Willingness to Display Dissent: An ordered scale with four categories from 1 (“I should stay quiet”) to 4 (“I should dissent”) Let’s load the data and look at the treatment and outcome. library(rio) pg &lt;- import(&quot;https://github.com/ktmccabe/teachingdata/blob/main/paluckgreen.dta?raw=true&quot;) ## Let&#39;s treat the outcome as a factor pg$dissent &lt;- as.factor(pg$dissent) ## Let&#39;s visualize the outcome by group library(ggplot2) library(tidyverse) pg %&gt;% filter(is.na(dissent)==F) %&gt;% ggplot(aes(x=dissent, group=factor(treat), fill=factor(treat)))+ geom_bar(aes(y=..prop..),stat= &quot;count&quot;, position=&quot;dodge&quot;, color=&quot;black&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;)+ scale_fill_brewer(&quot;Condition&quot;, labels=c(&quot;Control&quot;, &quot;Treatment&quot;),palette=&quot;Paired&quot;)+ scale_x_discrete(labels= c(&quot;Should Stay Quiet&quot;, &quot;2&quot;, &quot;3&quot;, &quot;Should Dissent&quot;)) We can see variation in the outcome, where some people are at the “stay quiet” end of the scale, while others are at the opposite end. We might have a few questions about the outcome: What is the probability of being in a particular category given a set of \\(\\mathbf{x_i&#39;}\\) values? Does the treatment influence likelihood of expressing dissent? Does the treatment significantly affect the probability of responding in a particular category? What model should they use to help answer these questions? One approach would be to use OLS. They could treat 1 to 4 scale as continuous from Should stay quiet to Should dissent If they do this, the interpretation of the regression coefficients would be: Going from Control to Treatment (0 to 1), is associated with \\(\\hat \\beta\\) movement on this scale. What could be problematic here? Might go below or above scale points Distance between scale points might not be an equal interval Doesn’t answer the “probability” question, just describes movement up and down the scale. A second approach could be to collapse the scale to be dichotomous and use logit/probit or a linear probability model. For example, they could treat the outcome to 0 = (lean toward stay quiet/stay quiet) vs. 1 (lean toward dissent/dissent) Here, after converting the outcomes in probability, the interpretation would be Going from Control to Treatment (0 to 1), is associated with an average difference in predicted probability of dissent (\\(Y_i = 1\\)) What could be problematic here? We lose information. A third approach–and the focus of this section– would be to use an ordinal logistic or probit regression. This is appropriate when our goal, at least in part, is to estimate the probability of being in a specific category, and these categories have a natural ordering to them. 8.1.1 Ordinal Model With ordered data, we have an outcome variable \\(Y_i\\) that can fall into different, ordered categories: \\(Y_i \\in \\{C_1, C_2, \\ldots, C_J \\}\\) with some probability. The image above shows a distribution where the area under the curve sums to 1, with the area divided into 4 categories, separated by three cutpoints. The area represents probability mass. For example, the area to the left of z1 represents the \\(Pr(Y_i^\\ast \\leq z1)\\). In our ordered model, we assume that there is a latent (unobserved) \\(Y^\\ast_i = X_i\\beta + \\epsilon\\) This means we can still have a single model \\(X_i \\beta\\), which determines what outcome level is achieved (this requires an assumption). where \\(\\epsilon\\) is either assumed to be normally distributed (probit) or distributed logistically (logit), and corresponds to the fuzziness of the cutpoints (\\(\\zeta_j\\)), which define in which category an outcome is observed to fall. We observe this category in our outcome: \\(Y_i\\). For example, we observe if someone said “should stay quiet” or “should dissent” vs. one of the two middle categories. \\[\\begin{gather*} Y_i=\\begin{cases} C_1 \\textrm{ if } Y^\\ast_i \\le \\zeta_1\\\\ C_2 \\textrm{ if } \\zeta_1 &lt; Y^\\ast_i \\le \\zeta_2\\\\ C_3 \\textrm{ if } \\zeta_2 &lt; Y^\\ast_i \\le \\zeta_3\\\\ \\ldots\\\\ C_J\\textrm{ if } \\zeta_{J-1} &lt; Y^\\ast_i \\\\ \\end{cases}\\\\ \\end{gather*}\\] The \\(\\zeta_j\\) are called “cutpoints” Need cutpoints that are distinct, but the distance between cutpoints does not have to be the same. This can be particularly useful when we have scales that have a natural ordering, but the distance between scale points might not have the same meaning or be the same (e.g., “Agree”,“Disagree”, “Revolt”). This is different from an interval variable, where we assume the difference between scale points carries the same meaning (e.g., credit score, cups of flour in a recipe). Note: There is no intercept in linear prediction model in this case. Instead of the intercept, we have the specific cutpoints. Rule of thumb: Estimation with more than 3-5 categories unstable 8.1.2 Interpretation Here is an example of what the ordered probit model output can look like from the authors’ paper. You can see coefficients similar to the models we’ve been working with before but instead of an intercept, we have the different cutpoints, in this case, three cutpoints for \\(J-1\\) categories. We can interpret the coefficients as a one-unit increase in \\(x\\) has a \\(\\hat \\beta\\) increase/decrease in the linear predictor scale of \\(Y^\\ast\\) (in log-odds for logit or probit z-score standard deviations). This gives us an initial sense (based on sign and significance) of how an independent variable positively or negatively affects the position of \\(Y*\\). However, it does not give us any information about specific categories. Thus, alas, this is unsatisfying for a couple of reasons. \\(Y_i^\\ast\\) is an unobserved variable (not the categories themselves) The scale is harder to interpret than probability Therefore, we will generally want to convert our estimates into probabilities (our quantities of interest) One wrinkle here is we now have \\(J\\) predicted probabilities to estimate, one for each category. A second wrinkle here is any change in the probability of being in the \\(jth\\) category of \\(Y\\) also affects the probabilities of being in the \\(\\neq jth\\) categories because the probabilities of being in each category have to sum together to 1. (E.g., Increasing the probability that someone said “should dissent” affects the probability they said \"should stay quiet.) "],
["likelihood-framework.html", "8.2 Likelihood Framework", " 8.2 Likelihood Framework In the binary case, we wanted to estimate \\(Pr(Y_i = 1 | X)\\). Our goal in an ordinal model is to estimate the probability that \\(Y_i\\) is in a particular \\(j\\) category \\(C_j\\). \\(Pr(Y_i = C_j | X)\\) To do so, we are going to use the cumulative distribution functions to estimate the probability that \\(Y_i\\) is below a particular cutpoint \\(\\zeta_j\\) or between two cutpoints \\(\\zeta_j\\) and \\(\\zeta_{j+1}\\). Finding predicted probabilities for a given \\(j\\) category can be written as follows: \\(Pr(Y_i|X_i) = \\mathbf 1(Y_i=C_j) \\{ \\Pr(Y^\\ast \\le \\zeta_{j})- \\Pr(Y^\\ast \\le \\zeta_{j-1})\\}\\) We can spell this out more explicitly for each \\(j\\) category: \\(Pr(Y_i = C_{J} | X) = 1 - Pr(Y^\\ast\\leq \\zeta_{J - 1} | X_i)\\) \\(Pr(Y_i = C_{3} | X) = Pr(Y^\\ast \\leq \\zeta_{3} | X_i) - Pr(Y^\\ast \\leq \\zeta_{2} | X_i)\\) \\(Pr(Y_i = C_{2} | X) = Pr(Y^\\ast \\leq \\zeta_{2} | X_i) - Pr(Y^\\ast \\leq \\zeta_{1} | X_i)\\) \\(Pr(Y_i = C_{1} | X) = Pr(Y^\\ast \\leq \\zeta_{1} | X_i)\\) Just as in the binary case, we use our \\(\\Phi()\\) pnorm or \\(\\frac{exp^{X\\beta}}{1 + exp^{X\\beta}}\\) plogis functions to get our probabilities from the linear predictors. However, in this ordered case, we also have to include the estimate for the cutpoint \\(\\zeta_j\\) when performing this operation. You can kind of think of this as having a separate intercept for each category instead of just one intercept in the binary case. For example, in the ordinal logit case, the linear predictor is in the scale of log of the proportional-odds. We can write our regression as: \\(\\log \\frac{P(Y \\leq j | X)}{P(Y &gt; j | X)} = (\\zeta_j - \\eta)\\) where \\(\\eta = x_1\\beta_1 + x_2\\beta_2 + ... + x_k\\beta_k\\) To get probability we apply the plogis function \\(logit^{-1}(\\zeta_j - \\eta)\\) Same for probit, but we use pnorm: \\(probit^{-1}(\\zeta_j - \\eta)\\) 8.2.1 Likelihood The likelihood of all observations, assuming independence is: \\(\\mathcal L(\\beta, \\zeta | Y) = \\prod_{i=1}^{N} Pr(Y_i = C_j)\\) for a given category. To incorporate all \\(J\\) categories, we can write: \\(\\mathcal L(\\beta, \\zeta | Y) = \\prod_{i=1}^{N} \\prod_{j=1}^{J} \\mathbf 1(Y_i=C_j) \\{ \\Pr(Y^\\ast \\le \\zeta_{j}) - \\Pr(Y^\\ast \\le \\zeta_{j-1})\\}\\) where \\(\\mathbf 1(Y_i=C_j)\\) is an indicator for whether or not a given \\(Y_i\\) is observed in the \\(jth\\) category. Note that here instead of estimating just \\(\\beta\\), we now also estimate the cutpoints \\(\\zeta\\). The log likelihood then just changes this to the sum: \\[\\begin{align*} \\mathcal l(\\beta, \\zeta | Y) &amp;= \\sum_{i=1}^{N} \\sum_{j=1}^{J}\\mathbf 1(Y_i=C_j) \\{ \\log( \\Pr(Y^\\ast \\le \\zeta_{j}) - \\Pr(Y^\\ast \\le \\zeta_{j-1}))\\}\\\\ &amp;= \\sum_{i=1}^{N} \\sum_{j=1}^{J} \\mathbf 1(Y_i=C_j) \\{\\log(\\Phi(\\zeta_j - \\mathbf x_i&#39;\\beta) - \\Phi(\\zeta_{j-1} - \\mathbf x_i&#39;\\beta))\\} \\end{align*}\\] In addition to assuming independence of our observations, we assume each category has a nonzero probability of being observed and that the cutpoints are monotonically increasing: \\(\\zeta_j &lt; \\zeta_{j+1}\\). "],
["fitting-ordinal-models-in-r.html", "8.3 Fitting Ordinal Models in R", " 8.3 Fitting Ordinal Models in R To fit an ordinal logit or probit model in R, we can use the MASS package and polr function. You may need to install this in your RStudio the first time you use it. install.packages(&quot;MASS&quot;) When we are ready to run the model, we then open the package and use the function polr. The first inputs are very similar to lm and glm. However, we add an argument specifying the method = which can be “logistic” (the default) or “probit.” We also specify Hess=T to make sure we get the uncertainty estimates with the results. By the way, why would that argument be called Hess? library(MASS) fit &lt;- polr(as.factor(dissent) ~ treat, data= pg, Hess = T, method = &quot;probit&quot;) summary(fit) Call: polr(formula = as.factor(dissent) ~ treat, data = pg, Hess = T, method = &quot;probit&quot;) Coefficients: Value Std. Error t value treat 0.2645 0.1021 2.589 Intercepts: Value Std. Error t value 1|2 -0.5102 0.0761 -6.7017 2|3 -0.0576 0.0740 -0.7784 3|4 0.2697 0.0742 3.6344 Residual Deviance: 1253.844 AIC: 1261.844 (116 observations deleted due to missingness) To get p-values, we can use the AER package. library(AER) coeftest(fit) z test of coefficients: Estimate Std. Error z value Pr(&gt;|z|) treat 0.264455 0.102143 2.5891 0.0096238 ** 1|2 -0.510237 0.076135 -6.7017 2.06e-11 *** 2|3 -0.057570 0.073958 -0.7784 0.4363235 3|4 0.269665 0.074198 3.6344 0.0002786 *** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Or calculate manually round(2*pnorm(abs(summary(fit)$coefficients[,3]), lower.tail = F), digits=6) treat 1|2 2|3 3|4 0.009624 0.000000 0.436323 0.000279 For details on how to fit this manually in R using optim, expand. # Grab X and Y, listwise deletion pgsub &lt;- na.omit(cbind(pg$dissent, pg$treat)) Y &lt;- pgsub[,1] X &lt;- pgsub[, 2] ## Log likelihood in R for this example llikor &lt;- function(par, Y, X){ k &lt;- 1 # X columns j &lt;- 4 # categories beta &lt;- par[1:k] zeta &lt;- par[(k+1):length(par)] X &lt;-cbind(X) beta &lt;- cbind(beta) ## linear predictor eta &lt;- X %*% beta ## indicator variables y1 &lt;- ifelse(Y==1, 1, 0) y2 &lt;- ifelse(Y==2, 1, 0) y3 &lt;- ifelse(Y==3, 1, 0) y4 &lt;- ifelse(Y==4, 1, 0) ## likelihood for each category l1 &lt;- y1*log(pnorm(zeta[1] - eta)) l2 &lt;- y2*log(pnorm(zeta[2] - eta) - pnorm(zeta[1] - eta)) l3 &lt;- y3*log(pnorm(zeta[3] - eta) - pnorm(zeta[2] - eta)) l4 &lt;- y4*log(1 - pnorm(zeta[3] - eta)) ## sum together llik &lt;- sum(c(l1, l2, l3, l4)) return(llik) } ## Optimize opt.out &lt;- optim(par=c(.2, 1, 2, 3), fn=llikor, X=X, Y=Y, method=&quot;BFGS&quot;, control=list(fnscale=-1), hessian = T) ## Coefficients opt.out$par[1] [1] 0.2644911 coef(fit) treat 0.2644546 ## Cutpoints opt.out$par[2:4] [1] -0.51019872 -0.05757855 0.26969063 fit$zeta 1|2 2|3 3|4 -0.5102370 -0.0575699 0.2696646 ## Standard errors sqrt(diag(solve(- opt.out$hessian))) [1] 0.10214310 0.07613473 0.07395763 0.07419776 sqrt(diag(vcov(fit))) treat 1|2 2|3 3|4 0.10214309 0.07613503 0.07395770 0.07419775 8.3.1 Quantities of Interest In discussing the results of Table 3 from the ordered probit analysis in the paper, the authors note, “A shift of .26 probits implies, for example, that a health group respondent with a 30% chance of strongly agreeing to dissent would move to a 40% chance if assigned to the reconciliation program group. This is a large, but not implausibly large, shift in opinion.” Similar to the binary case, we can convert our estimates to probability, but here we will do it for specific categories. Let’s first conduct an estimate of the difference in probability of being in the top category of dissent for those in the treatment - control conditions. ## probability of being in top category cutpoint3 &lt;- fit$zeta[3] X &lt;- 1 # in treatment b &lt;- coef(fit) # coefficient pp1 &lt;- 1 - pnorm(cutpoint3 - X*b) X &lt;- 0 # in control pp0 &lt;- 1 - pnorm(cutpoint3 - X*b) ## difference pp1 - pp0 3|4 0.1042124 We recover that there is an 10 percentage point difference in probability predicted for those in the treatment vs. control. If we were interested in other categories, we could similarly compare probabilities within those. For example, what about category 3? cutpoint3 &lt;- fit$zeta[3] cutpoint2 &lt;- fit$zeta[2] X &lt;- 1 # in treatment b &lt;- coef(fit) # coefficient pp1 &lt;- pnorm(cutpoint3 - X*b) - pnorm(cutpoint2 - X*b) X &lt;- 0 # in control pp0 &lt;- pnorm(cutpoint3 - X*b) - pnorm(cutpoint2 - X*b) pp1 - pp0 3|4 -0.000883825 We can use our R shortcuts for all categories. Here, to make sure we are in probabilities, we can use type=\"probs\". ## predict prs &lt;- predict(fit, newdata = data.frame(treat = c(0,1)), type = &quot;probs&quot;) ## Prediction library(prediction) ## specify which category outp &lt;- prediction(fit, at = list(treat = c(0,1)), category = 1) summary(outp)$Prediction [1] 0.3049427 0.2192609 How can we visualize the results? Given our independent variable of interest, treat just has two categories, we could make a plot similar to the barplot at the beginning of the section. When our independent variable of interest has several categories, things can get a bit messier. We will do an example in a weekly tutorial. How can we get uncertainty? Like before, we can use bootstrap or simulation, or ask Zelig or margins to do it for us. How can we incorporate covariates? Like before, we would just have a matrix of \\(X\\) covariates instead of a single column for the treatment. Like before, we could also calculate probabilities holding those covariates at observed values or setting them to designated values of interest. How could we fit a logit version? We just switch the method = \"logistic\", and then we should be careful to also use plogis() in place of pnorm for the probability calculation. Note: ordered logits also have the benefit of having the odds ratio interpretation when we exp(coef(fit)) exponentiate the coefficients. See this post’s section on “Interpreting the odds ratio” halfway down the page for more information. Again, it is more common in political science to see probabilities than odds ratios, but different disciplines prefer different quantities of interest. "],
["assumptions.html", "8.4 Assumptions", " 8.4 Assumptions A key assumption for the ordinal models is Parallel lines/Proportional Odds: We only have one set of \\(k \\times 1\\) \\(\\hat \\beta\\), not a separate set of coefficients for each ordinal category. This means that the relationship between the lowest versus all higher categories of the response variable are assumed to be the same as those that describe the relationship between the next lowest category and all higher categories, etc. For each \\(X\\) term included in the model, the coefficient ‘slopes’ are the same regardless of the threshold. If not, we would need a separate set of coefficients describing each pair of outcomes (e.g., Slopes for being in Cat 1 vs. Cat 2; Cat 2 vs. Cat 3, etc.) Even though we have different cutpoint values across categories, a one-unit change, going from control to treatment, the effects are parallel across response categories. For example, if theoretically, being a woman vs. a man has a positive effect on moving between Categories 3 and 4 in a particular model, but you believe it would have the opposite effect for moving from Category 1 to 2, this would suggest the ordinal model is not appropriate. What to do if assumption is violated? Ignore, Do Binary, Do multinomial (discussed in the next session), use a model that has been developed for relaxing this assumption (e.g., see clm function in R). One test for this that has been developed for the ordered logit case is the Brant test. fit.l &lt;- polr(as.factor(dissent) ~ treat, data= pg, Hess = T, method = &quot;probit&quot;) ## One way to test this #install.packages(&quot;brant&quot;) library(brant) brant(fit.l,by.var=F) -------------------------------------------- Test for X2 df probability -------------------------------------------- Omnibus 3.93 2 0.14 treat 3.93 2 0.14 -------------------------------------------- H0: Parallel Regression Assumption holds ## Second way- compare fit of ordinal and multinom models #install.packages(&quot;nnet&quot;) library(nnet) mlm &lt;- multinom(as.factor(dissent) ~ treat, data=pg) # weights: 12 (6 variable) initial value 686.215709 iter 10 value 625.130759 final value 625.130585 converged M1 &lt;- logLik(fit.l) M2 &lt;- logLik(mlm) G &lt;- -2*(M1[1] - M2[1]) pchisq(G, 6 - 4,lower.tail = FALSE) [1] 0.1667305 In both cases, our p-value is large enough that we cannot reject the null hypothesis, meaning that we are “okay” sticking with the assumption in this case. "],
["ordinal-practice-problems.html", "8.5 Ordinal Practice Problems", " 8.5 Ordinal Practice Problems Here are a few practice problems for ordered models. Try to replicate column 2 from Table 3 by adding factor(site_pr) to the regression model. Note that the standard errors will not exactly match. We will discuss why in a follow-up section. Calculate the average predicted probability of leaning toward staying quiet (category 2) for the treatment and control group, holding covariates at observed values. Note: If you do this manually, you need to remove the intercept column that is automatically added to the model.matrix Try on your own, and then expand for the solution. fit.c2 &lt;- polr(as.factor(dissent) ~ treat + factor(site_pr), data=pg, method=&quot;probit&quot;, Hess =T) coef(fit.c2) treat factor(site_pr)2 factor(site_pr)3 factor(site_pr)4 0.28484306 -0.17480464 -0.25513562 -0.17731799 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 -0.30547159 -0.11384681 0.04659582 ## Option 1 library(prediction) p1 &lt;- prediction(fit.c2, at=list(treat=c(0,1)), category=2) summary(p1)$Prediction [1] 0.1726136 0.1537839 ## Option 2 X &lt;- model.matrix(fit.c2)[,-1] X[, &quot;treat&quot;]&lt;-1 bh &lt;- coef(fit.c2) eta &lt;- X %*% bh cutpoint2 &lt;- fit.c2$zeta[2] cutpoint1 &lt;- fit.c2$zeta[1] mean(pnorm(cutpoint2 - eta) - pnorm(cutpoint1 - eta)) [1] 0.1537839 X[, &quot;treat&quot;] &lt;-0 bh &lt;- coef(fit.c2) eta &lt;- X %*% bh cutpoint2 &lt;- fit.c2$zeta[2] cutpoint1 &lt;- fit.c2$zeta[1] mean(pnorm(cutpoint2 - eta) - pnorm(cutpoint1 - eta)) [1] 0.1726136 8.5.1 A note on robust standard errors In writing down our likelihood equations, we assume our observations are independent. However, that is not always the case. For example, in Paluck and Green’s article, the experimental design is based on clustered pairs, and the treatment is randomly assigned to one unit of a pair. Ideally, we want to account for this non-independence in our model. In linear models (e.g., using lm), this is often accomplished post-estimation by adjusting the standard errors. We might adjust them for clustering if we have a case like the authors’ or for suspected heteroskedasticity (maybe the errors are not constant, but instead, are larger for higher values of a given X variable). Professor Jeffrey Wooldridge, an expert on econometrics, is a proponent of robust standard errors, given the stringent assumption of homoskedasticity. There are many types of these “robust” standard errors, and we may encounter more later in the course. Generally, what each of these robust standard errors does is alter the nature of the variance covariance matrix from which we extract the standard errors. Recall in OLS, we have an expression for the variance of the coefficients that looks like the below. By assuming constant error variance, we can simplify it. \\[\\begin{align*} \\mathbf{V}(\\widehat{\\beta}) &amp;= (X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\\\ &amp;= \\underbrace{(X^T X)^{-1}X^T \\sigma^2I_n X (X^T X)^{-1}}_\\text{Assume homoskedasticity}\\\\ &amp;= \\underbrace{\\sigma^2(X^T X)^{-1} X^T X (X^T X)^{-1}}_\\text{Because it is a constant, we can move it out in front of the matrix multiplication, and then simplify the terms.} \\\\ &amp;= \\sigma^2(X^T X)^{-1} \\end{align*}\\] After we plug in \\(\\hat \\sigma^2\\) for \\(\\sigma^2\\), we arrive at our variance-covariance matrix vcov:\\(\\hat \\sigma^2(X^T X)^{-1}=\\frac{1}{N-K} \\mathbf{e&#39;e}(X^T X)^{-1}\\) When we don’t have constant error variance, we have to pause here: \\((X^T X)^{-1}X^T \\mathbf{V}( \\epsilon) X (X^T X)^{-1}\\) and assume something different about the structure of the error terms in \\(\\mathbf{V}( \\epsilon)\\) and end up with a different final expression for vcov. The robust estimators differ in how they specify an estimate for \\(\\mathbf{V}( \\epsilon) = \\Sigma\\) and whether they take into account the degrees of freedom in the model (number of observations, number of variables, number of clusters, etc.) For example, let’s fit the authors’ model as a linear model using MacKinnon and White HC1 robust standard errors. Those specify the following inside portion for an estimate of \\(\\mathbf{V}( \\epsilon)\\).This is what Stata uses as a default “robust” standard error: \\(\\Sigma = \\frac{n}{n-k} \\text{diag}(\\hat \\epsilon_i^2)\\) This means we have a slightly different term for each observation instead of a constant estimate \\(\\hat \\sigma^2\\) for all observations. fit.lin &lt;- lm(as.numeric(dissent) ~ treat + factor(site_pr), data=pg) ## Option 1: Adjusting for heteroskedasticity library(sandwich) newvcov &lt;- vcovHC(fit.lin, type=&quot;HC1&quot;) sqrt(diag(newvcov)) (Intercept) treat factor(site_pr)2 factor(site_pr)3 0.1661596 0.1145570 0.2128812 0.2158906 factor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 0.2164289 0.2176110 0.2175578 0.2085747 ## Option 2: Adjusting for heteroskedasticity library(estimatr) fit.lin2 &lt;- lm_robust(as.numeric(dissent) ~ treat + factor(site_pr), data=pg, se_type = &quot;stata&quot;) sqrt(diag(vcov(fit.lin2))) (Intercept) treat factor(site_pr)2 factor(site_pr)3 0.1661596 0.1145570 0.2128812 0.2158906 factor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 0.2164289 0.2176110 0.2175578 0.2085747 Another adjustment takes into account the grouped or “clustered” nature of the data. Here, we have a separate estimate for each group in the data. This can be implemented in R by using vcovCL or the clusters argument in lm_robust. ## Option 1: Adjusting for clustering library(sandwich) newvcov &lt;- vcovCL(fit.lin, type=&quot;HC1&quot;, cadjust=T, cluster = pg$sitcode) sqrt(diag(newvcov)) (Intercept) treat factor(site_pr)2 factor(site_pr)3 0.07685775 0.07066623 0.12388030 0.09271341 factor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 0.07174477 0.12894245 0.20605985 0.06962861 ## Option 2: Adjusting for clustering library(estimatr) fit.lin2 &lt;- lm_robust(as.numeric(dissent) ~ treat + factor(site_pr), data=pg, se_type = &quot;stata&quot;, clusters = sitcode) sqrt(diag(vcov(fit.lin2))) (Intercept) treat factor(site_pr)2 factor(site_pr)3 0.07685775 0.07066623 0.12388030 0.09271341 factor(site_pr)4 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 0.07174477 0.12894245 0.20605985 0.06962861 This is all great, but the authors don’t have a linear model, they have an ordered probit model. It turns out that people have developed similar robust standard error estimators to adjust the standard errors from these non-linear models. For more details, see slides from Molly Roberts on this point. These take a slightly different form, but the intuition is the same. What we are altering is the structure of the variance-covariance matrix (which is a function of the Hessian in the likelihood case), making adjustments across all observations or clusters of observations. We can also implement these in R using HC0 standard errors. ## Adjusting for clustered standard errors fit.c2 &lt;- polr(as.factor(dissent) ~ treat + factor(site_pr), data=pg, method=&quot;probit&quot;, Hess =T) clval &lt;- vcovCL(fit.c2, type=&quot;HC0&quot;, cluster = pg$sitcode) You can compare these standard errors to column 2 from Table 3 in the paper. sqrt(diag(clval)) treat factor(site_pr)2 factor(site_pr)3 factor(site_pr)4 0.06212521 0.11636326 0.07559847 0.06387804 factor(site_pr)5 factor(site_pr)6 factor(site_pr)7 1|2 0.10879657 0.16765260 0.06391195 0.07144512 2|3 3|4 0.07494704 0.08409084 The coeftest function in R, when applied to a new vcov will reproduce the full regression output. library(lmtest) coeftest(fit.c2, vcov=clval) z test of coefficients: Estimate Std. Error z value Pr(&gt;|z|) treat 0.284843 0.062125 4.5850 4.54e-06 *** factor(site_pr)2 -0.174805 0.116363 -1.5022 0.1330371 factor(site_pr)3 -0.255136 0.075598 -3.3749 0.0007385 *** factor(site_pr)4 -0.177318 0.063878 -2.7759 0.0055052 ** factor(site_pr)5 -0.305472 0.108797 -2.8077 0.0049892 ** factor(site_pr)6 -0.113847 0.167653 -0.6791 0.4970975 factor(site_pr)7 0.046596 0.063912 0.7291 0.4659633 1|2 -0.646720 0.071445 -9.0520 &lt; 2.2e-16 *** 2|3 -0.190723 0.074947 -2.5448 0.0109348 * 3|4 0.138053 0.084091 1.6417 0.1006487 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 I got 99 problems, and standard errors are just one However, robust standard errors cannot correct underlying model misspecification. Recall that the initial likelihood equation is what assumes independence– this is the equation we use for everything, not only for computing standard errors but also for estimating the coefficients themselves. If we truly believe we have dependence among our observations, we might consider using an entirely different likelihood function– one that incorporates this dependence, instead of just adjusting the standard errors after the fact. Nonetheless, we may still think our model is good enough, even if not “correct.” So long as you recognize that the robust standard errors don’t correct for this underlying problem, some are still proponents for using robust standard errors in these cases. For example, Wooldridge recommends reporting robust standard errors, and writes, "],
["week-6-tutorial.html", "8.6 Week 6 Tutorial", " 8.6 Week 6 Tutorial For this example, we will replicate a portion of the article“How Empathic Concern Fuels Partisan Polarization” by Elizabeth N. Simas, Scott Clifford, and Justin H. Kirkland.published in 2020 in the American Political Science Review. Replication files are available here Abstract. Over the past two decades, there has been a marked increase in partisan social polarization, leaving scholars in search of solutions to partisan conflict. The psychology of intergroup relations identifies empathy as one of the key mechanisms that reduces intergroup conflict, and some have suggested that a lack of empathy has contributed to partisan polarization. Yet, empathy may not always live up to this promise. We argue that, in practice, the experience of empathy is biased toward one’s ingroup and can actually exacerbate political polarization. First, using a large, national sample, we demonstrate that higher levels of dispositional empathic concern are associated with higher levels of affective polarization. Second, using an experimental design, we show that individuals high in empathic concern show greater partisan bias in evaluating contentious political events. Taken together, our results suggest that, contrary to popular views, higher levels of dispositional empathy actually facilitate partisan polarization. We are going to replicate Study 1’s analysis testing Hypotheses 1 and 2. Here, the authors conduct an original survey fielded by YouGov during May 2016 with 1000 respondents. Let’s load the data. How many observations do we have? library(foreign) emp &lt;- read.dta(&quot;https://github.com/ktmccabe/teachingdata/blob/main/week6.dta?raw=true&quot;) The authors’ first two hypotheses are: Empathic concern should predict more positive affect for copartisans, relative to outpartisans (H1). Empathic concern should increase negative affect for outpartisans (H2). Outcome Measure: “To examine this type of partisan favoritism, we utilize responses to two questions asking respondents to rate the Democratic and Republican Parties on a seven-point scale ranging from”very favorable\" to “very unfavorable.” We then subtract respondents’ ratings of the opposite party from their ratings of their own party to create an ordinal measure that ranges from six (highest inparty rating, lowest outparty rating) to −6 (lowest inparty rating and highest outparty rating)\" affectpol: an ordinal measure that ranges from six (highest inparty rating, lowest outparty rating) to −6 (lowest inparty rating and highest outparty rating)\" outfav: rating of opposing party on a seven-point scale ranging from “very favorable” to “very unfavorable.” Let’s take a look at these variables. Are they coded as the authors describe? What class are they currently? table(emp$affectpol) -6 -4 -3 -2 -1 0 1 2 3 4 5 6 1 1 4 5 11 106 80 80 103 148 137 116 class(emp$affectpol) [1] &quot;numeric&quot; table(emp$outfav) 1 2 3 4 5 6 7 431 163 73 85 16 16 8 class(emp$outfav) [1] &quot;numeric&quot; Independent Variables. empconc: Mean of empathetic concern items from the Interpersonal Reactivity Index (IRI) Additional variables to measure other dimensions of empathy: empdist personal distress, emppers perspective taking, empfant fantasy Additional controls for strength of party identification pidext, ideological extremity ideoext, news interest news, dummy variable for party membership dem, and demographics: educ, age, male, white, inc3miss (income) What type of model could they use to test H1 and H2? They choose to run an ordinal logistic regression. Let’s do as they do. To run an ordinal model in R, we need to make sure our outcome is ordinal! (meaning a factor variable) emp$outfav &lt;- as.factor(emp$outfav) table(emp$outfav) 1 2 3 4 5 6 7 431 163 73 85 16 16 8 class(emp$outfav) [1] &quot;factor&quot; Go ahead and replicate the first regression in the table with all of the controls, making sure to treat income as a factor variable but education as a numeric variable. Note: your coefficients will not exactly match because the authors weight their data using survey weights. library(MASS) fit.emp &lt;- polr(outfav ~ empconc + empdist + emppers + empfant + + pidext + ideoext + news + dem + as.numeric(educ) + age + male + white + factor(inc3miss), data=emp, Hess=T, method=&quot;logistic&quot;) summary(fit.emp) Call: polr(formula = outfav ~ empconc + empdist + emppers + empfant + +pidext + ideoext + news + dem + as.numeric(educ) + age + male + white + factor(inc3miss), data = emp, Hess = T, method = &quot;logistic&quot;) Coefficients: Value Std. Error t value empconc -0.805771 0.492455 -1.63623 empdist 0.321857 0.413656 0.77808 emppers 0.433013 0.528121 0.81991 empfant -0.020821 0.408769 -0.05093 pidext -0.246698 0.095193 -2.59155 ideoext -0.556725 0.072652 -7.66288 news -0.557133 0.094192 -5.91487 dem 0.002573 0.156964 0.01639 as.numeric(educ) 0.071461 0.054630 1.30809 age -0.004130 0.004784 -0.86345 male -0.310108 0.160583 -1.93114 white -0.015491 0.176718 -0.08766 factor(inc3miss)2 -0.049340 0.196223 -0.25145 factor(inc3miss)3 -0.127770 0.200626 -0.63686 factor(inc3miss)4 -0.148389 0.242841 -0.61105 Intercepts: Value Std. Error t value 1|2 -3.4355 0.6385 -5.3808 2|3 -2.3032 0.6308 -3.6513 3|4 -1.5674 0.6259 -2.5042 4|5 -0.2119 0.6325 -0.3351 5|6 0.3713 0.6467 0.5741 6|7 1.5460 0.7177 2.1541 Residual Deviance: 1828.862 AIC: 1870.862 (245 observations deleted due to missingness) Let’s take a look out how the weights affect the result by using the survey package. There are many options in establishing an svydesign. Ours is a relatively simple case where all we have is a vector of weights. In other cases, samples might include information about the sampling units or strata. Once we establish an svydesign object, we now need to use svy commands for our operations, such as svymean or svyglm or svyolr ## install.packages(&quot;survey&quot;, dependencies =T) library(survey) empd &lt;- svydesign(ids=~1, weights = emp$weight_group, data=emp) fit.empw2 &lt;- svyolr(outfav ~ empconc + empdist + emppers + empfant + + pidext + ideoext + news + dem + as.numeric(educ) + age + male + white + factor(inc3miss), design=empd, method=&quot;logistic&quot;) summary(fit.empw2) Call: svyolr(outfav ~ empconc + empdist + emppers + empfant + +pidext + ideoext + news + dem + as.numeric(educ) + age + male + white + factor(inc3miss), design = empd, method = &quot;logistic&quot;) Coefficients: Value Std. Error t value empconc -1.4141333759 0.585070456 -2.4170309 empdist 0.7759996312 0.561768870 1.3813504 emppers 1.1700466598 0.672754809 1.7391874 empfant 0.9733692488 0.475484526 2.0471103 pidext -0.3281938237 0.124714241 -2.6315665 ideoext -0.4970353573 0.106530714 -4.6656531 news -0.5759629296 0.130609880 -4.4097960 dem -0.0456812052 0.195236242 -0.2339791 as.numeric(educ) 0.0284167135 0.065624317 0.4330211 age -0.0007305433 0.005493873 -0.1329742 male -0.1759866726 0.212493495 -0.8281979 white 0.0366164598 0.215144017 0.1701951 factor(inc3miss)2 -0.0470362687 0.226737718 -0.2074479 factor(inc3miss)3 -0.0794217660 0.246811912 -0.3217907 factor(inc3miss)4 -0.0588030650 0.330225820 -0.1780693 Intercepts: Value Std. Error t value 1|2 -2.9118 0.9199 -3.1652 2|3 -1.7749 0.9029 -1.9658 3|4 -1.0853 0.9058 -1.1982 4|5 0.3080 0.8948 0.3442 5|6 0.9101 0.9006 1.0106 6|7 2.8740 0.9296 3.0918 (245 observations deleted due to missingness) The weights seem to make a difference! Now we are closely matching what the authors report. The use of survey weights represents yet another point of researcher discretion. Let’s use the weighted results and proceed to make them easier to interpret. Recall, H2 was: Empathic concern should increase negative affect for outpartisans (H2). We want to show how negative affect toward the outparty changes across levels of empathic concern. How should we visualize this? Could calculate probabilities of being in each of the outfav seven categories across different levels of empathetic concern. Could calculate probabilities of being in theoretically interesting outfav categories across different levels of empathetic concern. Note: in each case, we need to decide where to set our covariate values and potentially also calculate uncertainty estimates. What do they do? (focus on the right side for the out-party measure) Let’s estimate the probability of being in the lowest category for empathy values from 0 to 1 by .2 intervals. Let’s set all covariates at their means. We could do this in predict or manually. We will do it manually for now. ## Set covariates to particular values X &lt;- model.matrix(fit.empw2) X &lt;- X[, -1] #remove intercept ## Crude way to take means ## why might this be bad to do in practice? X.means &lt;- colMeans(X) X.means[&quot;empconc&quot;] &lt;- 0 # Find Xb and zeta # this piece [1:length(X.means)] makes sure we omit the zetas eta &lt;- X.means %*% coef(fit.empw2)[1:length(X.means)] zeta &lt;- fit.empw2$zeta ## Find Pr(lowest category) emp0 &lt;- plogis(zeta[1] - eta) emp0 [,1] [1,] 0.2666338 ## Repeat for each value of empconc of interest X.means[&quot;empconc&quot;] &lt;- .2 # Find Xb and zeta eta &lt;- X.means %*% coef(fit.empw2)[1:length(X.means)] zeta &lt;- fit.empw2$zeta ## Find Pr(lowest category) plogis(zeta[1] - eta) [,1] [1,] 0.3254268 emp2 &lt;- plogis(zeta[1] - eta) emp2 [,1] [1,] 0.3254268 ## Or put it in a function to be faster findpr &lt;- function(val){ X.means[&quot;empconc&quot;] &lt;- val # Find Xb and zeta eta &lt;- X.means %*% coef(fit.empw2)[1:length(X.means)] zeta &lt;- fit.empw2$zeta ## Find Pr(lowest category) pr &lt;- plogis(zeta[1] - eta) return(pr) } ## Does it work? Test findpr(0) [,1] [1,] 0.2666338 ## Repeat for all values of empathy emp.prs &lt;- sapply(seq(0, 1, .2), findpr) emp.prs [1] 0.2666338 0.3254268 0.3902847 0.4592682 0.5298489 0.5992556 We can visualize these estimates similar to the authors. plot(x=seq(0, 1, .2), y=emp.prs, ylim = c(.1, .7), type=&quot;l&quot;, xlab = &quot;Empathic Concern&quot;, ylab = &quot;&quot;, main = &quot;Outparty Favorability \\n Pr(Outparty Very Unfavorable)&quot;, cex.main=.8) We could add lines for all categories. We’ll just add it to the function for now. findprall &lt;- function(val){ X.means[&quot;empconc&quot;] &lt;- val # Find Xb and zeta eta &lt;- X.means %*% coef(fit.empw2)[1:length(X.means)] zeta &lt;- fit.empw2$zeta ## Find Pr(7th lowest category) pr7 &lt;- 1 - plogis(zeta[6] - eta) ## Find Pr(6th lowest category) pr6 &lt;- plogis(zeta[6] - eta) - plogis(zeta[5] - eta) ## Find Pr(5th lowest category) pr5 &lt;- plogis(zeta[5] - eta) - plogis(zeta[4] - eta) ## Find Pr(4th lowest category) pr4 &lt;- plogis(zeta[4] - eta) - plogis(zeta[3] - eta) ## Find Pr(3rd lowest category) pr3 &lt;- plogis(zeta[3] - eta) - plogis(zeta[2] - eta) ## Find Pr(2nd lowest category) pr2 &lt;- plogis(zeta[2] - eta) - plogis(zeta[1] - eta) ## Find Pr(lowest category) pr1 &lt;- plogis(zeta[1] - eta) return(c(pr1, pr2, pr3, pr4, pr5, pr6, pr7)) } ## Repeat for all values of empathy emp.prsall &lt;- sapply(seq(0, 1, .2), findprall) We can add these lines to the plot. Yikes! A bit messy. You can see why they focus on the first category only. plot(x=seq(0, 1, .2), y=emp.prsall[1,], ylim = c(0, 1), type=&quot;l&quot;, xlab = &quot;Empathic Concern&quot;, ylab = &quot;&quot;, main = &quot;Outparty Favorability \\n Pr(Outparty Very Unfavorable)&quot;, cex.main=.8) points(x=seq(0, 1, .2), y=emp.prsall[2,], type=&quot;l&quot;, lty=2) points(x=seq(0, 1, .2), y=emp.prsall[3,], type=&quot;l&quot;, lty=3) points(x=seq(0, 1, .2), y=emp.prsall[4,], type=&quot;l&quot;, lty=4) points(x=seq(0, 1, .2), y=emp.prsall[5,], type=&quot;l&quot;, lty=5) points(x=seq(0, 1, .2), y=emp.prsall[6,], type=&quot;l&quot;, lty=6) points(x=seq(0, 1, .2), y=emp.prsall[7,], type=&quot;l&quot;, lty=7) legend(&quot;topleft&quot;, lty=1:7, c(&quot;Very Unfav&quot;, &quot;2&quot;, &quot;3&quot;, &quot;4&quot;, &quot;5&quot;, &quot;6&quot;, &quot;Very Fav&quot;), cex=.6) A last step would be to calculate uncertainty. Just like before, we could use simulation or the bootstrap method. "],
["ml.html", "Section 9 Multinomial Outcomes", " Section 9 Multinomial Outcomes This section will describe cases where we have unordered categorical outcome variables. The following supplemental resources may be useful here. Rodriguez Chapter 6.1-6.2.4 (see pdf on Canvas) Ledolter Chapter 11, pgs. 132-134 (see pdf on Canvas) Multinomial R resources via UCLA \\(Y_i\\) can take any of \\(J\\) categories like ordinal logit. Usually, we use this for more than 2 categories, or else it would collapse to a simple binary outcome. The key here is these are not ordered! For example, a nominal set of categories might be vote choice between different candidates, Instead, it could be choice of different investment or insurance plans, Perhaps it is a choice of locations to target in an attack, Or maybe it is a type of campaign strategy– direct mail vs. online ads, etc. "],
["overview-of-nominal-data.html", "9.1 Overview of Nominal Data", " 9.1 Overview of Nominal Data Our goal in these models is generally to predict the probability of being in a particular category \\(C_j\\). The model we will use to estimate this is the multinomial logit. This is a generalization of binary and ordered logit. Coefficients defined relative to baseline outcome category \\(J\\): \\(\\log \\frac{\\pi_j}{\\pi_J} = \\eta_j\\) where \\(\\eta_j = \\alpha_j + X_1\\beta_{1j} + X_2\\beta_{2j} + ... X_k\\beta_{kj}\\) Note that \\(\\pi_i\\) (our probability) is now indexed by \\(j\\) or \\(J\\) This means we have more than just one set of estimates for \\(\\hat \\beta\\), depending on the category comparison we make (Recall that in ordinal logit we had to assume that one set of coefficients was sufficient. Here, we have \\(J-1\\) sets of coefficients.) \\(\\beta_J = 0\\) by design for identification (\\(J\\) represents the baseline category); \\(\\sum_{j=1}^{J} \\pi_j = 1\\), The probabilities of being in each category, together, must sum to 1. \\(Y_i = C_j\\) according to \\(Y_{ij}^* = max(Y_{i1}^*, Y_{i2}^*, ..., Y_{ij}^*)\\). The outcome belongs to the category that has the highest \\(Y_i*\\). The probability of \\(Y_i\\) being in a particular category is: \\[\\begin{align*} \\pi_{ij} = Pr(Y_i = C_j | x_i) &amp;= \\frac{\\exp(\\mathbf x_i^T\\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)} \\end{align*}\\] 9.1.1 Multinomial Likelihood Here, similar to the ordinal log likelihood, we need to sum over all observations and all outcome categories to represent the joint probability: \\(\\mathcal L(\\beta, | Y) = \\prod_i^N \\prod_{j=1}^{J} \\mathbf 1(Y_i=C_j){\\pi_{i, j}}\\) \\(\\mathcal l(\\beta, | Y) = \\sum_i^N \\sum_{j=1}^{J} \\mathbf 1(Y_i=C_j){\\log \\pi_{i, j}}\\) Like we have done previously in likelihood, where \\(\\pi\\) is a function of \\(X\\) and \\(\\beta\\). "],
["motivating-example.html", "9.2 Motivating Example", " 9.2 Motivating Example To work through the model we will use data from the article “Empowering Women? Gender Quotas and Women’s Political Careers” by Yann Kerevel, in The Journal of Politics here Women’s representation in executive office continues to lag behind that of their female peers in legislatures, and several studies find women face an executive glass ceiling. One way to shatter this executive glass ceiling is through the adoption of legislative gender quotas. However, scholars know little about how legislative quotas affect women’s access to executive office. Previous research has been unable to determine whether once elected through quotas, women advance to executive office at similar rates to men. Using data on the future career paths of nearly 2,000 Mexican legislators, I find women face a glass ceiling in winning nomination to executive office. Using career data before and after quota implementation, and exploiting lax enforcement in the district component of the mixed electoral system, I find quotas have done little to increase the advancement of women into executive office, although they have increased opportunities for women in other legislative positions. On pg. 1169, Kerevel writes, “If a glass ceiling exists, women may find it harder to advance to executive office compared to men. Female legislators may still be able to develop successful political careers in similar roles, such as seeking reelection, winning nomination to other legislative offices, or receiving some type of political appointment. However, women may be less likely than men to secure executive nominations to elected positions or appointments to important cabinet posts. The introduction of gender quotas is unlikely to shatter this glass ceiling given the numerous ways women are marginalized once elected and the general lack of executive quotas.” The first hypothesis following this claim is: H1. Glass ceiling- Women legislators will be nominated to future executive office at lower rates than men. We will focus on this first hypothesis, but if you are interested, the author also discusses specific hypotheses related to the potential negative and positive effects of gender quotas in the paper. Data The data include information on the future career moves of nearly 2000 Mexican federal deputies who served between 1997-2009. Outcome: genderimmedambcat3, first future career move ballot access for state legislature/city council, for Senate, for mayor/governor; appointment to cabinet or party leadership; bureaucratic appointment; other office Explanatory variables: female, party_cat, leg_exp_dum, exec_exp_dum, leadership Let’s load the data and take a look at the outcome variable. library(foreign) ker &lt;- read.dta(&quot;https://github.com/ktmccabe/teachingdata/blob/main/kerevel.dta?raw=true&quot;) table(ker$genderimmedambcat3) other deputy/regidor ballot access 680 241 senate ballot access mayor/gov ballot access 130 243 cabinet/party leader bur appt 137 329 The outcome categories focus on nominations to these offices (i.e., ballot access), as the author finds that gender has less to do with electoral success. 9.2.1 Assumption and Considerations Let’s focus on the author’s research question. How could we evaluate this relationship? The UCLA site has a good discussion of these tradeoffs. One option would be to collapse this to a more simple problem of multiple separate binary logit models comparing two outcome categories at a time. One possible issue is there is no constraint such that multiple pairwise logits won’t generate probabilities that when summed together exceed 1. Another is that each pairwise comparison might involve a slightly different sample. In contrast, the multinomial logit uses information from all \\(N\\) observations when jointly estimating the likelihood. A second option would be to collapse the outcome categories down to two levels only (E.g., 1=Mayor/governor or Bureaucrat vs. 0=Otherwise) Just like we said collapsing a scale in ordinal models loses information, the same thing would happen here. This only makes sense to do in a case where a collapseed scale is theoretically interesting. Assess the scale– maybe the categories actually are ordered! Think about whether the categories actually could be considered ordered or even on an interval scale. If that is the case, then you might be able to return to a linear or ordinal model. Instead, if we want to keep the nominal categories as they are, and we believe they are not ordered, we can move forward with the multinomial model. If we do so, we will want to make sure we have enough data in each outcome category to feel comfortable estimating the predicted probability of being in that category. We also want to avoid situations where there is no variation in our independent variables for a given outcome category (e.g., maybe only men were nominated for mayor/governor) If we run into those situations, we might need more data or think about alternative modelling strategies. 9.2.2 Key Assumption: Independence of Irrelevant Alternatives The IIA assumption requires that our probability of choosing a particular outcome over another (A over B) does not depend on the choice set, in particular, the presence or absence of some choice C. Classic case: bus example Suppose you have the transportation choice set below, where the numbers are the probabilities of choosing a particular form of transportation. Choice set: {train, red bus, blue bus} = {.5, .25, .25} Choice set: {train, red bus} = {.5, .5}–Violates IIA Choice set: {train, red bus} = {2/3, 1/3}–Does not violate IIA The is because we assume the independence of the error terms \\(\\epsilon_i\\) across choices. In a multinomial probit model, this can be relaxed. However, multinomial probit is very computationally intensive, often not used. "],
["running-multinomial-logit-in-r.html", "9.3 Running multinomial logit in R", " 9.3 Running multinomial logit in R The multinomial specification will estimate coefficients that are relative to a baseline level of the outcomes. We should be thoughtful about what we choose as the baseline so that the coefficients are useful to us. Here, the author chooses to use “other” as the baseline. In R, we can easily change the baseline level of a factor variable. So first, we should check the class of the outcome variable and convert it into a factor variable if needed. class(ker$genderimmedambcat3) [1] &quot;factor&quot; We can then adjust the baseline level using the relevel command as has the ref argument for specifying a reference category. ## Sets a level as the base category for the factor ker$genderimmedambcat3 &lt;- relevel(ker$genderimmedambcat3, ref = &quot;other&quot;) Let’s run a simple regression model with just female as a covariate to see how the regression output differs from other models we have used thus far. ## install.packages(&quot;nnet&quot;) library(nnet) fit0 &lt;- multinom(genderimmedambcat3 ~ factor(female), data = ker) # weights: 18 (10 variable) initial value 3153.496666 iter 10 value 2849.378831 final value 2837.879460 converged You can see that there is now a set of coefficients– Intercept and for female, for each outcome category-baseline comparison. We also do not automatically have the p-values for the output. We can find the p-values the same way we have done before by calculating z-scores through the division of the coefficients over the standard errors. Once we have the z-scores we can use pnorm the same way we did in the ordinal section. z &lt;- summary(fit0)$coefficients/summary(fit0)$standard.errors p &lt;- as.matrix(2*(pnorm(abs(z), lower.tail = F))) p (Intercept) factor(female)1 deputy/regidor ballot access 8.040554e-39 0.10711432 senate ballot access 1.096025e-57 0.03803764 mayor/gov ballot access 1.119624e-33 0.06783023 cabinet/party leader 2.110040e-54 0.65008684 bur appt 2.383587e-26 0.01908118 As you can see, the output here for the coefficients is already messy, and we only have one covariate!! Just imagine how messy it can get with several covariates. Often, because of this, researchers move to present results visually instead. The practice problems will include a chance to replicate one of the author’s visuals from the JOP paper. 9.3.1 Multinomial Quantities of Interest Like the previous models, we can calculate predicted probabilities at specific values of our \\(X\\) covariates. What differs here is just the specific form of this function. Predicted probabilities of the outcome being in a particular outcome category \\(C_j\\): \\(Pr(Y_i = C_j | X) = \\frac{\\exp(X\\hat \\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(X \\hat \\beta_j)}\\) The numerator is very similar to the numerator for when we have a binary logistic regression The denominator sums up \\(\\exp(X \\hat \\beta_j)\\) where \\(\\hat \\beta_j\\) represents the set of coefficients for each outcome category except for the baseline (i.e., \\(J- 1\\) means 1 less than the total number of outcome categories). Recall that in the baseline category \\(J\\), \\(\\hat \\beta_J\\) is forced to 0 to help with the estimation or “identification” of the other coefficients relative to that category. Well why bring that up now? Well, \\(exp(X \\hat \\beta_J) = exp(0) = 1\\). When we estimate probabilities in the baseline category, the numerator will just be 1. Let’s take an example of finding the predicted probability of \\(C_j\\) = Senate ballot access when \\(x_i\\) is set to be female = 1. ## Create the model matrix Xfemale &lt;- cbind(1, 1) # 1 for intercept, 1 for female ## Extract coefficients and give them informative labels Bsenate &lt;- coef(fit0)[2,] # 2nd row of coefficients Bdep &lt;- coef(fit0)[1,] Bmayor &lt;- coef(fit0)[3,] Bcabinet &lt;- coef(fit0)[4,] Bbur &lt;- coef(fit0)[5,] ## Probability of senate ballot access for female exp(Xfemale %*% Bsenate)/ (1 + exp(Xfemale %*% Bsenate) + exp(Xfemale %*% Bdep) + exp(Xfemale %*% Bmayor) + exp(Xfemale %*% Bcabinet) + exp(Xfemale %*% Bbur)) [,1] [1,] 0.09846309 Let’s do the same for female = 0. We just need to change X. Xnfemale &lt;- cbind(1, 0) exp(Xnfemale %*% Bsenate)/ (1 + exp(Xnfemale %*% Bsenate) + exp(Xnfemale %*% Bdep) + exp(Xnfemale %*% Bmayor) + exp(Xnfemale %*% Bcabinet) + exp(Xnfemale %*% Bbur)) [,1] [1,] 0.06829273 We might also be interested in the probability of \\(C_j\\) = other. The syntax here will look slightly different because “other” was the baseline category. The denominator stays the same, but the numerator is just 1. ## Manual- probability of other for female (the base category) 1/ (1 + exp(Xfemale %*% Bsenate) + exp(Xfemale %*% Bdep) + exp(Xfemale %*% Bmayor) + exp(Xfemale %*% Bcabinet) + exp(Xfemale %*% Bbur)) [,1] [1,] 0.3538449 Just like in the other models, we can rely on outside packages, too. For some models, these packages are not going to have full capabilities– they might not be able to calculate standard errors, for example. These are “living packages” so you can always check the documentation and update the packages to see if new capabilities have been added. library(prediction) p.senate &lt;- prediction(fit0, at = list(female = 1)) mean(p.senate$`Pr(senate ballot access)`) [1] 0.09846309 mean(p.senate$`Pr(other)`) [1] 0.3538449 We can use margins to calculate the difference in predicted probabilities, for example, between female=1 and female=0. We should specify the category for which we want this comparison. It appears we need to set vce = booststrap for this to work. library(margins) m.senate &lt;- margins(fit0, variables = &quot;female&quot;, category = &quot;senate ballot access&quot;, vce=&quot;bootstrap&quot;, change=c(0,1)) summary(m.senate) Of course, we could also do this manually, and run the bootstrap ourselves! "],
["practice-problems-for-multinomial.html", "9.4 Practice Problems for Multinomial", " 9.4 Practice Problems for Multinomial We will try to replicate a portion of the analysis from the paper. Note that different multinomial functions in R and in Stata (which the author used) might rely on slightly different optimization and estimation algorithms, which in smaller samples, might lead to slightly different results. This is one place where your results might not exactly match the authors’, but they should be close. Let’s try to replicate Figure 1a in the paper (which corresponds to Table 1a in the appendix), which shows the average marginal effect of being female vs. male on the first future career move, for each outcome category in the data. In addition to the female covariate, the author includes covariates for party_cat (party identification), leg_exp_dum and exec_exp_dum (legislative and executive experience), and leadership (chamber leadership experience) which are each treated as factor variables in the regression. We should set party_cat to have the baseline of PRI to match the author. ker$party_cat &lt;- relevel(as.factor(ker$party_cat), ref=&quot;PRI&quot;) The author sets covariates to observed levels when estimating the marginal effects. Based on the marginal effects, how would you evaluate the author’s hypothesis on the effect of gender on future career moves to executive office? Try on your own and then expand for the solution. fit2 &lt;- multinom(genderimmedambcat3 ~ factor(female) + party_cat + factor(leg_exp_dum) + factor(exec_exp_dum) + factor(leadership), data=ker) library(margins) marg.effect.execnom &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;mayor/gov ballot access&quot;) marg.effect.deputy &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;deputy/regidor ballot access&quot;) marg.effect.senator &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;senate ballot access&quot;) marg.effect.execappt &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;cabinet/party leader&quot;) marg.effect.bureau &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;bur appt&quot;) marg.effect.other &lt;- margins(fit2, variables=&quot;female&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;other&quot;) Let’s make a visual close to the authors using ggplot. Recall, ggplot is easiest to work with when the data you want to plot are in a data.frame. So we are going to bind together the summary output and specify which row corresponds to which outcome. mcomb &lt;- data.frame(rbind(summary(marg.effect.execnom), summary(marg.effect.deputy), summary(marg.effect.senator), summary(marg.effect.execappt), summary(marg.effect.bureau), summary(marg.effect.other))) mcomb$outcome &lt;- c(&quot;executive nom&quot;, &quot;deputy&quot;, &quot;senator&quot;, &quot;executive appt&quot;, &quot;bureaucracy&quot;, &quot;other&quot;) mcomb$outcome &lt;- factor(mcomb$outcome, levels = c(&quot;deputy&quot;, &quot;senator&quot;, &quot;executive nom&quot;, &quot;executive appt&quot;, &quot;bureaucracy&quot;, &quot;other&quot;)) We can now use geom_point and geom_errorbar to plot the AME point estimates and bootstrap confidence intervals. library(ggplot2) ggplot(mcomb, aes(x=outcome, y=AME))+ geom_point()+ geom_errorbar(aes(ymin=lower, ymax=upper), width=.05)+ theme_bw()+ ylim(-.4, .4)+ geom_hline(yintercept=0)+ ylab(&quot;Change in probability&quot;)+ xlab(&quot;&quot;) ggsave(&quot;images/kerplot.png&quot;, device=&quot;png&quot;, width=6, height=4) Based on this analysis, women have a significantly lower probability of being nominated for a future mayoral or gubernatorial position, which aligns with the author’s hypothesis. "],
["week-7-tutorial.html", "9.5 Week 7 Tutorial", " 9.5 Week 7 Tutorial For this exercise, we will use data from Amy Lerman, Meredith Sadin, and Samuel Trachtman’s 2017 article in the American Political Science Review, “Policy Uptake as Political Behavior: Evidence from the Affordable Care Act.” Abstract. Partisanship is a primary predictor of attitudes toward public policy. However, we do not yet know whether party similarly plays a role in shaping public policy behavior, such as whether to apply for government benefits or take advantage of public services. While existing research has identified numerous factors that increase policy uptake, the role of politics has been almost entirely overlooked. In this paper, we examine the case of the Affordable Care Act to assess whether policy uptake is not only about information and incentives; but also about politics. Using longitudinal data, we find that Republicans have been less likely than Democrats to enroll in an insurance plan through state or federal exchanges, all else equal. Employing a large-scale field experiment, we then show that de-emphasizing the role of government (and highlighting the market’s role) can close this partisan gap. In a portion of their analysis, they use survey data to assess the relationship between partisanship and insurance uptake through the ACA marketplaces. The researchers’ hypothesis is: First, we expect that partisanship will be a strong predictor of policy behavior. In the case of the ACA, we anticipate that Republicans—who on average are much less supportive of the health insurance reform and are generally more resistant to government intervention in the private market—will be less likely than Democrats to take advantage of health insurance options provided by the ACA. Key variables include ins: insurance status, 1=“uninsured”, 3= “marketplace”, 4= “private” republican: 1= Republican, 0= Democrat age2: numeric variable for age ed: education level of respondent racethn: race/ethnicity of respondent income2: categorical income of respondent sex: sex of respondent state : respondent’s state of residence (coded as a FIPs code) empl2: respondent’s employment status date: date of the survey poll Let’s load the data and look at the outcome ins. library(foreign) library(nnet) # install this package lst &lt;- read.dta(&quot;https://github.com/ktmccabe/teachingdata/blob/main/lst.dta?raw=true&quot;) table(lst$ins) 1 3 4 2100 724 904 Let’s assess the research question. Our goal is to understand the relationship between party identification and uptake of ACA marketplace insurance. What are the possible ways we could model this relationship given our outcome data? Let’s suppose we decided to go with the multinomial logistic regression. What class() should our outcome variable be? What would make sense to use as a baseline category? Go ahead and recode the variable as necessary. Even if your variable is a factor, I would recommend giving it informative labels instead of numbers so that it is easier to interpret the regression outcome. class(lst$ins) [1] &quot;numeric&quot; lst$ins &lt;- as.factor(ifelse(lst$ins == 1, &quot;uninsured&quot;, ifelse(lst$ins == 3, &quot;marketplace&quot;, ifelse(lst$ins == 4, &quot;private&quot;, NA)))) lst$ins &lt;- relevel(lst$ins, ref = &quot;marketplace&quot;) Let’s conduct a multinomial logistic regression of the following form: \\(\\log \\frac{ \\pi_{j}}{ \\pi_{J}} = \\alpha_{ij} + Republican_i\\beta_{kj} + age2_i \\beta_{j} + ed_i\\beta_{kj} + racethn_i \\beta_{kj} + income2_i\\beta_{kj} + sex_i\\beta_{kj} + empl2_i\\beta_{kj}\\) where all covariates are treated as factor variables. Run the model. fit &lt;- multinom(as.factor(ins) ~ republican + age2 + factor(ed) + factor(racethn) + factor(income2) + as.factor(sex) + as.factor(empl2), data = lst) # weights: 60 (38 variable) initial value 4095.626612 iter 10 value 3475.264179 iter 20 value 3122.816006 iter 30 value 3067.172420 iter 40 value 3059.543787 final value 3059.470322 converged Let’s consider our assumptions. What is a key assumption of the multinomial logistic regression model? How could it potentially be violated in the authors’ case? Expand for one example. The key assumption is the IIA assumption. The authors consider potential violations to this assumption in footnote 4. “Consistent estimation using the multinomial logistic model relies on the Independence of Irrelevant Alternatives (IIA) assumption, which requires that the choice of one of the available options does not depend on whether some alternative option is present. While this assumption is hard to test (Allison 2012), there is some evidence that it could be violated in this case, with the presence of the”uninsured\" option affecting the distribution of choices across private and marketplace insurance. Thus, as a robustness check, we also estimate a model in which we first analyze the decision to insure (for the study population), and second, conditional on having insurance, analyze the private versus marketplace choice.\" Let’s assume we are okay with our assumptions. Let’s now try to use the model to evaluate the research hypothesis. First, we should get comfortable extracting coefficients from multinomial output. Extract the coefficients on the Republican covariate and their standard errors Calculate z-scores and p-values Assess the statistical significance Provide an initial evaluation of the authors’ research hypothesis Note that coef(fit) is now a matrix of output. We want to extract the republican column. repcoef &lt;- coef(fit)[, &quot;republican&quot;] repse &lt;- summary(fit)$standard.errors[, &quot;republican&quot;] ## Calculate z-scores rep.zs &lt;- repcoef/repse rep.ps &lt;- 2*(pnorm(abs(rep.zs), lower.tail = F)) round(cbind(repcoef, rep.zs, rep.ps), digits=3) repcoef rep.zs rep.ps private 0.942 8.368 0 uninsured 0.966 9.044 0 Let’s now transform these into quantities of interest that closely evaluate the research question. What quantity of interest should we estimate? Compute the quantity of interest Compute uncertainty Consider ways to visualize the results Quantity of Interest calculation. Let’s calculate the average difference in predicted probability of signing up for marketplace insurance for Republicans vs. Democrats, holding covariates at observed values. Recall from above that the probability of \\(Y_i\\) being in a particular category is: \\[\\begin{align*} Pr(Y_i = C_j |X) &amp;= \\frac{\\exp(\\mathbf x_i^T\\beta_j)}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)} \\end{align*}\\] Recall that marketplace insurance is the baseline category. So this means our formula is \\[\\begin{align*} Pr(Y_i = Marketplace | X) &amp;= \\frac{1}{1 + \\sum_{j=1}^{J-1} \\exp(\\mathbf x_i^T\\beta_j)} \\end{align*}\\] ## Point estimates Xrep &lt;- model.matrix(fit) Xrep[, &quot;republican&quot;] &lt;- 1 Xdem &lt;- model.matrix(fit) Xdem[, &quot;republican&quot;] &lt;- 0 ## Extract all coefficients B &lt;- t(coef(fit)) Bprivate &lt;- coef(fit)[1, ] Buninsured &lt;- coef(fit)[2, ] ## Approach one repmarket.p &lt;- mean(1 / (1 + exp(Xrep %*% Bprivate) + exp(Xrep %*% Buninsured))) demmarket.p &lt;- mean(1 / (1 + exp(Xdem %*% Bprivate) + exp(Xdem %*% Buninsured))) diffmarket.p &lt;- repmarket.p- demmarket.p ## Approach two (easier when you have a lot of outcome categories) repmarket.p &lt;- mean(1 / (1 + rowSums(exp(Xrep %*% B)))) demmarket.p &lt;- mean(1 / (1 + rowSums(exp(Xdem %*% B)))) diffmarket.p &lt;- repmarket.p - demmarket.p diffmarket.p [1] -0.1313263 ## Approach three library(margins) marg.effect.market &lt;- margins(fit, variables=&quot;republican&quot;, change=c(0, 1), vce= &quot;bootstrap&quot;, category=&quot;marketplace&quot;) summary(marg.effect.market) Uncertainty calculation. We can use the simulation or bootstrap approach like before to calculate uncertainty. We will use the bootstrap because the syntax for the simulation approach is more complicated because our coefficients are in a matrix. The package Zelig has this capability. Simulation can be more faster given the multinomial model takes a moment to run. ## Bootstrap myboot &lt;- function(df){ wrows &lt;- sample(1:nrow(df), size=nrow(df), replace = T) subdata &lt;- df[wrows, ] fit.boot &lt;- multinom(as.factor(ins) ~ republican + age2 + factor(ed) + factor(racethn) + factor(income2) + as.factor(sex) + as.factor(empl2), data = subdata) ## Point estimates Xrep &lt;- model.matrix(fit.boot) Xrep[, &quot;republican&quot;] &lt;- 1 Xdem &lt;- model.matrix(fit.boot) Xdem[, &quot;republican&quot;] &lt;- 0 ## Extract all coefficients B &lt;- t(coef(fit.boot)) Bprivate &lt;- coef(fit.boot)[1, ] Buninsured &lt;- coef(fit.boot)[2, ] ## Approach one repmarket.p &lt;- mean(1 / (1 + exp(Xrep %*% Bprivate) + exp(Xrep %*% Buninsured))) demmarket.p &lt;- mean(1 / (1 + exp(Xdem %*% Bprivate) + exp(Xdem %*% Buninsured))) diffmarket.p.boot &lt;- repmarket.p- demmarket.p return(cbind(repmarket.p, demmarket.p, diffmarket.p.boot)) } myboot.ests &lt;- do.call(&quot;rbind&quot;, replicate(1000, myboot(lst), simplify = F)) ## Confidence intervals around the difference cis.diff &lt;- quantile(myboot.ests[, &quot;diffmarket.p.boot&quot;], c(0.025, 0.975)) cis.diff ## Visualize the two distributions png(&quot;images/boostrappid.png&quot;, res=300, width=5, height=4, units=&quot;in&quot;) plot(density(myboot.ests[, &quot;repmarket.p&quot;]), col=&quot;red&quot;, main = &quot;Bootstrap Distribution for Republican and Democrats \\n Marketplace Uptake Probability&quot;, cex.main = .7, xlim = c(0.05, .3), xlab=&quot;Estimated Probability of Uptake through Marketplace&quot;) points(density(myboot.ests[, &quot;demmarket.p&quot;]), col=&quot;blue&quot;, type=&quot;l&quot;) dev.off() "],
["count.html", "Section 10 Count data", " Section 10 Count data In this section, we will talk about models that are appropriate for estimating outcomes that are structured as “count data”– non-negative integer data. The key characteristics of count data are: \\(Y\\) is not continuous and \\(Y_i \\geq 0\\) Typically the count can be small for many observations (not always) Describes number of occurrences in a specified time period (the observation period) or space (e.g., per state) (e.g., vetoes during a term in office) The following supplemental resources may be useful: King, Gary. 1998. Unifying political methodology: The likelihood theory of statistical inference. University of Michigan Press. Chapters: 5.5-5.10 Gelman and Hill Chapter 6. (on Canvas) Resources for R related to count data R help for negative binomial from UCLA R help for poisson "],
["overview-of-count-data.html", "10.1 Overview of Count Data", " 10.1 Overview of Count Data Many of our dependent variables in social science may be considered counts: The number of arrests or traffic stops The number of bills passed The number of terrorist attacks The number of tweets The number of judges a president nominates per year Each of these variables shares the features that they are discrete and range from 0 to some positive number. Example: Your outcome data might look like this, where each \\(i\\) observation represents a count of some kind: Y &lt;- rpois(n=30, lambda = 2) Y [1] 2 2 1 4 3 2 0 4 5 0 2 2 2 2 2 2 0 1 3 0 2 5 1 3 2 0 0 3 4 1 A common way one might approach modeling data of this type is to use OLS to estimate a linear model. After all, the data seem quasi-continuous! Sometimes, this might be just fine, but let’s think about some situations where this could go awry. Oftentimes count data are heavily right-skewed and very sparse. For example, suppose we were interested in the number of times a social media user makes a comment on a discussion forum. It is very common for a large number of people to make close to zero posts, while a small share of users might make a larger number of posts. Particularly in small samples, OLS can struggle with heavily skewed data because the error variance is likely not going to be homogenous, the distribution of errors is not going to be normal6, and the linearity assumption could very well be suspect (violations of the usual assumptions). When continuous data are heavily right-skewed (e.g., sometimes income is), it is often recommended to \\(\\log\\) transform the \\(y\\) variable before fitting the regression with a linear model. With count data, we can pursue other options. Moreover, if you have a lot of counts that are 0, this transformation is problematic anyway because \\(\\log(0) = -Inf\\). The transformation won’t really work, as standard statistical software will often treat those values as missing. Below is an example of this type of skew, sparsity, and clustering toward 0. Nonsensical values? With OLS, there is also no guarantee that smaller estimated \\(\\hat y\\) values from the regression line will stay non-negative even though we know that the actual count outcomes are always going to be non-negative. There is also no guarantee that larger \\(\\hat y\\) values will stay within the possible upper-range of \\(y\\) values. When data are heavily skewed, the regression line, which represents the “conditional mean” \\(\\mathbf E(Y |X=x)\\) given some values of \\(X\\) might be a poor estimate given that generally we know that means can be poor estimates of highly skewed data (e.g., picture how estimates of income given a certain level of education would change if Bill Gates and Mark Zuckerberg are in your sample vs. if they are not). For more information on dealing with skewed data and non-normal errors in linear regression, see Chapter 12.1 posted on Canvas from the Fox textbook. Count data will always prevent the errors from being normality distributed, which can be problematic for estimates in small samples. In large samples, the uncertainty estimates will still approximate the correct values.↩︎ "],
["poisson-model.html", "10.2 Poisson Model", " 10.2 Poisson Model So we are left unsatisfied with a linear model for our count data. What do we do? Fortunately, there are multiple probability distributions that may be appropriate for the data generating process that generates counts, which we can use maximum likelihood to estimate. Since we have been in Bernoulli world for a while, let’s refresh on the steps we consider when approaching a potential maximum likelihood problem. What is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\). Define the likelihood for a single observation Define the likelihood for all observations Find the log-likelihood Maximize the function with respect to (wrt) \\(\\theta\\) The first probability distribution we will consider is the Poisson. This is a discrete distribution (so we are in pmf instead of pdf territory). Let’s go step-by-step. What is the data generating process? Based on this, describe the probability distribution for \\(Y_i\\). \\(Y_i \\sim Pois(\\lambda)\\) \\(\\Pr(Y=Y_i|\\lambda)= \\frac{exp(-\\lambda) \\lambda^{Y_i}}{Y_i!}\\), which describes the probability that the outcome takes a particular value, given \\(\\lambda\\). We assume no two events occur at the same time. We also assume the probability of an event occurring at a particular time is not a function of the previous events. Events happen independently. Sometimes the occurrence of an event makes the occurrence of a subsequent event less or more likely. This would be a violation and suggestive of overdispersion, which we will return to later. Gary King uses the example of text messaging. Receiving a text message now probably makes it more likely you will receive additional text messages (i.e., group chats) Recall that in a normal distribution, we needed two parameters \\(\\mu\\) and \\(\\sigma^2\\), to describe the shape of a normally distributed variable– the mean and variance of a normally distributed outcome. In the Poisson case, the shape of our distribution is defined by the single parameter \\(\\lambda\\). A special feature of the Poisson probability distribution is that \\(\\lambda\\) is both the parameter for the mean \\(\\mathbf E(Y) = \\lambda\\) and the variance \\(\\mathbf V(Y) = \\lambda\\). Try to remember this detail, because it will come up later when we assess if a Poisson process is actually a good approximation of the data generating process. It’s often not the case that a mean is the same as the variance. It can actually be a quite bad approximation of the variance. Let’s look at an example of Poisson data to prove to ourselves that if data are Poisson the mean and variance are equivalent: ## We use rpois() to generate some count data according to the Poisson distribution. ## Let&#39;s specify lambda = 4 ## This means the mean of the distribution will be 4 and the variance will be 4 ## In any given sample, it might be slightly off from this Y &lt;- rpois(n=100000, lambda = 4) mean(Y) [1] 4.00034 var(Y) [1] 4.00326 The pmf above describes the probability that Y takes a particular outcome. ## For example given a Y ~ Pois(lambda = 4), let&#39;s look at the probability of getting particular counts using dpois, the pmf function for poisson counts &lt;- 0:16 probs &lt;- dpois(counts, lambda = 4) names(probs) &lt;- counts round(probs, digits=3) 0 1 2 3 4 5 6 7 8 9 10 11 12 0.018 0.073 0.147 0.195 0.195 0.156 0.104 0.060 0.030 0.013 0.005 0.002 0.001 13 14 15 16 0.000 0.000 0.000 0.000 ## The probability is higher the closer we are to 4, the mean of the distribution ## Let&#39;s check our formula from above for the probability Y = 2 lambda &lt;- 4 yi &lt;- 2 dpois(yi, lambda=4) [1] 0.1465251 ## formula from above exp(-lambda)*lambda^yi/factorial(yi) [1] 0.1465251 The expected count here is 4, the mean of the distribution. The probability of any given count is specified according to the pmf above. Adding covariates In regression, we will consider \\(\\lambda\\) (the expected count) to be a function of \\(\\mathbf x_i&#39;\\beta\\), and we will try to estimate our outcome \\(\\mathbf E(Y_i | \\mathbf x_i)\\) given values of \\(\\mathbf x_i\\). However, just like in the logit/probit case, our parameter \\(\\lambda \\neq \\mathbf x_i&#39;\\beta\\). Instead, it is a nonlinear function of \\(\\mathbf x_i&#39;\\beta\\). Here, we just have a different link function. Specifically, \\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\) \\(\\log \\lambda_i = \\eta_i = \\mathbf x_i^T\\beta\\) \\(\\lambda_i = \\mathbf E[Y_i | \\mathbf x_i] = \\exp(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_{k}x_{ik})\\) is the expected number of events per a unit of time or space Analogy: Recall, in the Bernoulli case, we had just the parameter \\(\\pi\\), which described the expected probability of success given there is just one trial. Recall, in logistic regression \\(\\pi_i = \\frac{exp(\\mathbf x_i^T\\beta)}{1 + exp(\\mathbf x_i^T\\beta)}\\). Here, the transformation is just a different link function. OK, if we are using the existing functions in R, we can essentially stop here and proceed to glm (yes, we get to use our glm friend again). But, let’s look at the likelihood to finish out the process. Define the likelihood for a single observation This is just that pmf from above. For now, we will just write \\(\\lambda\\), but we know eventually we will need to substitute it with our expression \\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\). \\[\\begin{align*} \\mathcal L(\\beta |Y_i)=\\Pr(Y=Y_i|\\lambda) \\end{align*}\\] Define the likelihood for all observations Here, we need to multiply across all observations. To do this, we are assuming independence. \\[\\begin{align*} \\mathcal L(\\beta |Y)&amp;=\\mathcal L(\\beta|Y_1)\\times\\mathcal L(\\beta|Y_2)\\times \\ldots \\times \\mathcal L(\\beta|Y_{n})\\\\ \\mathcal L(\\beta|Y)&amp;=\\prod_{i=1}^N\\mathcal L(\\beta|Y_i)\\\\ &amp;= \\prod_{i = 1}^{N}\\frac{1}{Y_i!}\\lambda_i^{Y_i}\\exp(-\\lambda_i) \\end{align*}\\] Find the log-likelihood We’ve seen this party trick before. Taking the \\(\\log\\) gives us the sums: \\[\\begin{align*} \\mathcal l(\\beta|Y)&amp;=\\sum_{i=1}^N\\mathcal \\log(\\mathcal L(\\beta|Y_i))\\\\ &amp;= \\sum_{i = 1}^{n}\\log(\\frac{1}{Y_i!}\\lambda_i^{Y_i}\\exp(-\\lambda_i))\\\\ &amp;= \\sum_{i = 1}^{n}\\log (\\frac{1}{Y_i!}) + Y_i\\log(\\lambda_i) - \\lambda_i\\\\ &amp;= \\sum_{i = 1}^{n}Y_i\\mathbf x_i^\\top\\beta - \\exp(\\mathbf x_i^\\top\\beta) - \\log(Y_i!) \\end{align*}\\] Maximize the function with respect to (wrt) \\(\\theta\\) Oof, this is where we take the derivative to find the \\(S(\\theta)\\). Fortunately, the last term does not have a \\(\\beta\\), so it falls out Recall, the derivative of \\(e^{z}= e^{z}\\) \\[\\begin{align*} \\frac{\\delta}{\\delta \\beta} \\ell(\\beta | Y, X) &amp;= \\frac{\\delta}{\\delta \\beta} \\sum_{i = 1}^{n}Y_i\\mathbf x_i^\\top\\beta - \\exp(\\mathbf x_i^\\top\\beta) - \\log(Y_i!)\\\\ &amp;= \\sum_{i = 1}^{n} (Y_i - \\exp(\\mathbf x_i^\\top\\beta))\\mathbf x_i^\\top\\\\ &amp;= X^TY - X^T\\exp(X\\beta) \\text{ which is a $k \\times 1$} \\end{align*}\\] This will not yield a closed form solution for \\(\\hat \\beta\\) when setting it to zero. Instead, we have to use numerical methods to estimate the parameters (e.g., think optim). The good thing is that now that we have taken the first derivative, we can take the second derivative to find the Hessian, which will allow us to estimate uncertainty. \\[\\begin{align*} &amp;= - \\sum_{i = 1}^{n} \\mathbf x_i\\mathbf x_i&#39;\\exp(\\mathbf x_i^\\top\\beta)\\\\ &amp;= - X^TVX \\text{ where } V = n \\times n \\text{ diagonal matrix of } \\exp(X\\beta) \\end{align*}\\] Note this is the \\(k \\times k\\) matrix! For the variance estimates of our coefficients, we want the \\((\\mathbf E(-H))^{-1} = (X^TVX)^{-1}\\) That is our vcov(fit) in the Poisson case OK, let’s start translating this math into R. "],
["motivating-example-for-count-data.html", "10.3 Motivating Example for Count Data", " 10.3 Motivating Example for Count Data We will use the following article for our motivating example: “Legislative Capacity and Executive Unilateralism” by Alex Bolton and Sharece Thrower, which was published in the American Journal of Political Science in 2015. Abstract. This article develops a theory of presidential unilateralism in which both ideological divergence with Congress and legislative capacity influence the president’s use of executive orders. We argue that when Congress is less capable of constraining the executive, the president will issue more executive orders during periods of divided government. Conversely, in periods of high legislative capacity, the president is less likely to issue executive orders when faced with an opposed Congress. Based on an examination of institutional changes, we identify years prior to the mid‐1940s as characterized by low congressional capacity and the subsequent period as characterized by high capacity. Testing the theory between 1905 and 2013, we find strong support for these predictions and demonstrate that legislative capacity conditions the role of ideological disagreement in shaping presidential action. Overall, this article deepens our current understanding of the dynamics of separation‐of‐powers politics and the limits of executive power. The primary research question: Is the president constrained by an ideologically opposed Congress? The authors explore how the number of executive orders made per year varies acording to whether the government is divided or unified. Outcome: allnoncerm_eo, all non-ceremonial executive orders in a year Key Explanatory variable: divided, whether or not there was divided government, where the president and the majority party in either the House or Senate are different parties Other explanatory variables include dummy variables for the president, an indicator if it is war time, measures related to the economy, and whether it is close to the beginning or end of an administration Let’s load the data and look at the outcome variable. library(foreign) bolton &lt;- read.dta(&quot;https://github.com/ktmccabe/teachingdata/blob/main/bt.dta?raw=true&quot;) table(bolton$allnoncerm_eo) 20 26 30 31 34 35 37 38 39 40 41 42 43 45 48 49 50 52 53 54 1 2 1 2 2 2 3 1 1 1 3 2 1 4 2 3 1 2 2 2 55 56 57 61 63 64 65 66 68 69 70 71 72 75 76 78 85 92 96 97 1 1 3 1 1 2 1 2 2 1 2 1 1 1 1 1 1 1 1 1 98 112 116 117 120 146 164 172 188 200 212 219 225 232 241 247 250 253 265 267 2 1 1 1 1 1 2 1 1 1 1 1 2 1 1 3 1 1 1 1 273 286 287 303 305 307 309 315 319 328 338 339 345 358 382 393 438 471 473 501 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Often for count variables, it can be useful to visualize them in a histogram. Here is a ggplot version. library(ggplot2) ggplot(bolton, aes(allnoncerm_eo))+ geom_histogram(binwidth = 5)+ theme_minimal() The authors distinguish time pre- and post-1945 based on different levels of Congressional capacity. We can look at how the outcome changed over time and note how there were far more executive orders in the earlier period. plot(x=bolton$year, y=bolton$allnoncerm_eo, pch =20, main = &quot;Executive Orders by Year&quot;, cex.main = .8, ylim = c(0, 500)) abline(v=1945, lty =2) # vertical line at 1945 10.3.1 Fitting Poisson in R We will investigate the relationship between divided government and executive orders in the first time period. The authors’ hypothesize, \"During periods of low legislative capacity (prior to the mid-1940s), the president issues more executive orders under divided government. To fit a Poisson model in R, we use the glm function. However, now we have a different family= \"poisson\" and link = \"log\". We don’t actually have to explicitly write the link because R will use this link by default. Let’s fit a regression of our outcome on the key explanatory variables, along with other controls the authors use. Note that because I want the early period, I have to subset the data. I can do this outside, prior to the regression. Or, I can subset in the data argument, as is done in the below code: fit &lt;- glm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + + tr+ taft + wilson + harding + coolidge + hoover, family = &quot;poisson&quot;, data = subset(bolton, year &lt; 1945)) 10.3.2 Interpreting regression output The summary output for Poisson is much nicer than the multinomial output we were working with previously. Let’s extract just the divided coefficient output from the summary. summary(fit)$coefficients[2,] Estimate Std. Error z value Pr(&gt;|z|) 4.435893e-01 4.195090e-02 1.057401e+01 3.933025e-26 How should we interpret this coefficient? Recall \\(\\log \\hat \\lambda = \\mathbf x_i&#39;\\hat \\beta\\) For every one-unit change in \\(x\\), we estimate an average \\(\\hat \\beta\\) change in the \\(\\log\\) of the expected executive orders, holding the other covariates constant. Note that usually counts are measured given a particular time or space interval. For this reason sometimes these are considered “rates” (i.e., the number of executive orders per year). "],
["poisson-quantities-of-interest.html", "10.4 Poisson Quantities of Interest", " 10.4 Poisson Quantities of Interest 10.4.1 Expected Counts Our primary quantity of interest is the expected count, in this case the expected number of executive orders, given certain values of the covariates \\(\\mathbf E(Y | X) = \\lambda = exp(\\mathbf x_i&#39; \\beta)\\) This means to get our quantities of interest, we exponentiate \\(exp(\\mathbf x_i&#39; \\hat \\beta)\\) after setting specific values for \\(X\\), such as at the means of the covariates or at the observed values. Like other glm models, we can also use predict to do this for us by setting type = response or prediction. For example, let’s find the averaged executive orders expected for unified vs. divided government, holding other covariates at their observed values. We will do this manually and using prediction. X &lt;- model.matrix(fit) X[, &quot;divided&quot;] &lt;- 0 B &lt;- coef(fit) eta &lt;- X %*% B expcount &lt;- exp(eta) avg.exp.count.0 &lt;- mean(expcount) avg.exp.count.0 [1] 265.7753 X &lt;- model.matrix(fit) X[, &quot;divided&quot;] &lt;- 1 B &lt;- coef(fit) eta &lt;- X %*% B expcount &lt;- exp(eta) avg.exp.count.1 &lt;- mean(expcount) avg.exp.count.1 [1] 414.1552 avg.exp.counts &lt;- prediction(fit, at=list(divided=c(0, 1)), type = &quot;response&quot;) summary(avg.exp.counts) at(divided) Prediction SE z p lower upper 0 265.8 2.862 92.85 0.000e+00 260.2 271.4 1 414.2 15.759 26.28 3.224e-152 383.3 445.0 We can also find the differences in expected counts by subtracting the above estimates from each other, or computing this directly through margins: avg.exp.count.1 - avg.exp.count.0 [1] 148.3798 avg.diff &lt;- margins(fit, variables=&quot;divided&quot;, change = c(0, 1), vce = &quot;delta&quot;) summary(avg.diff) factor AME SE z p lower upper divided 148.3798 16.6768 8.8974 0.0000 115.6938 181.0658 Just like in logit and probit, we have the same options for calculating uncertainty: Delta Method, Simulation, and Bootstrap. 10.4.2 Sidenote: Multiplicative coefficient interpretation For Poisson, changes in \\(x\\) will have a multiplicative change in \\(y\\): Recall a math rule for exponents to follow the below: \\(z^{a+b} = z^a * z^b\\) \\[\\begin{align*} \\mathbf E(Y | X) &amp;= e^{\\alpha + x_1 \\beta_1 + x_2 \\beta_2}\\\\ &amp;= e^{\\alpha} * e^{x_1 \\beta_1} *e^{x_2 \\beta_2} \\end{align*}\\] For example, compare \\(x_1\\) to \\(x_1 + 1\\) \\[\\begin{align*} \\mathbf E(Y | X) &amp;= e^{\\alpha + (x_1 + 1) \\beta_1 + x_2 \\beta_2}\\\\ &amp;= e^{\\alpha} * e^{x_1 \\beta_1}* e^{\\beta_1} *e^{x_2 \\beta_2} \\end{align*}\\] We’ve now multiplied the outcome by \\(e^{\\beta_1}\\). Here’s an example using a bivariate model fit.biv &lt;- glm(allnoncerm_eo ~ divided , family = &quot;poisson&quot;, data = subset(bolton, year &lt; 1945)) ## Let&#39;s calculate yhat using predict for divided = 0 or 1 yhats &lt;- predict(fit.biv, data.frame(divided = c(0, 1)), type=&quot;response&quot;) yhats 1 2 280.6471 282.8333 ## Manual yhatx0 &lt;- exp(coef(fit.biv)[1] + coef(fit.biv)[2]*0) yhatx1 &lt;- exp(coef(fit.biv)[1] + coef(fit.biv)[2]*1) yhatx1 (Intercept) 282.8333 ## Multiplicative interpretation yhatx0*(exp(coef(fit.biv)[&quot;divided&quot;])) (Intercept) 282.8333 10.4.3 Incidence Rate Ratios Similar to logistic regression where we could exponentiate the coefficients to generate estimated odds ratios, in poisson, we can exponentiate the coefficients to get “incidence rate ratios.” When we say a one-unit change in the independent variable \\(x\\), this is like saying \\(\\hat\\beta = \\log \\hat \\lambda_{x + 1} - \\log \\hat \\lambda_{x} = \\log \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}}\\) If we exponentiate, the \\(\\log\\) cancels to 1: \\(exp(\\hat \\beta) = exp(\\log \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}})\\) \\(exp(\\hat \\beta) = \\frac{\\hat \\lambda_{x + 1}}{\\hat \\lambda_{x}}\\) This quantity represents a ratio of the expected counts or “rates” For a one-unit change in \\(x\\), the expected count is estimated to change by a factor of \\(exp(\\hat \\beta)\\) This can be converted to a percent change interpretion by taking \\((IRR - 1) \\times 100\\) For example, the incidence rate ratio for executive orders in a year going from unified to divided government is: exp(coef(fit)[&quot;divided&quot;]) divided 1.55829 We can see how this works out using the quantities calculated above. Multiplying the expected count when divided = 0 by this ratio gives us the expected count when divided = 1. exp(coef(fit)[&quot;divided&quot;])*avg.exp.count.0 divided 414.1552 avg.exp.count.1 [1] 414.1552 Note how this is a percent change interpretation where ((avg.exp.count.1 - avg.exp.count.0)/ avg.exp.count.0)* 100 [1] 55.82903 (exp(coef(fit)[&quot;divided&quot;])-1)*100 divided 55.82903 For a one-unit change going from unified to divided government, we see a 55.8% increase in executive orders. 10.4.4 Where Poisson is poisonous: Recall the detail when we specified the pmf, that the mean and variance at the same. When we have covariates \\(X\\), this means we assume the conditional mean and variance are the same: \\(\\lambda = \\mathbf E[Y_i |x_i] = Var(Y_i | x_i)\\); the mean and variance is \\(\\lambda\\) in the distribution. Often, our data are “overdispersed” or “underdispersed”, violating this assumption We can investigate this in our example. First, let’s look at the raw mean and variance of the outcome. ## whoa! very different, ## but the good news is we care about the *conditional* mean and variance ## not these raw values mean(subset(bolton$allnoncerm_eo, bolton$year &lt; 1945)) [1] 280.975 var(subset(bolton$allnoncerm_eo, bolton$year &lt; 1945)) [1] 9011.256 We can conduct a test of overdispersion in our model using dispersiontest. If we have a significant result and a dispersion constant \\(&gt;\\) 0, this would suggest overdispersion. library(AER) dispersiontest(fit) Overdispersion test data: fit z = 3.6675, p-value = 0.0001225 alternative hypothesis: true dispersion is greater than 1 sample estimates: dispersion 11.02527 To check for overdisperson, we can also look at the standardized residuals of the model. ## By hand prvals &lt;- predict(fit, type = &quot;response&quot;) # predicted values res &lt;- subset(bolton$allnoncerm_eo, bolton$year &lt; 1945) - prvals # residual y - predicted values sres &lt;- res/sqrt(prvals) # standardized residual ## automatic in R sres &lt;- residuals(fit,type=&quot;pearson&quot;) # automatic We can graphically look at the standardized residuals by levels of the predicted values from our regression. Here, we don’t want residuals that exceed +/- 2. ## don&#39;t want values above 2 or below -2 plot(prvals, sres, ylab = &quot;standardized residuals&quot;, xlab = &quot;predicted values&quot;) abline(h = c(-2, 0, 2), lty = c(2, 1, 2)) We are in danger!! This model suffers from overdispersion. We have two options Check our model. Do we think it is misspecified in terms of the covariates? Is it suffering from omitted variable bias? We can change the specification and re-run the test. Use a different model. Because this assumption is often violated, we tend to use one of the following for count data Overdispersed poisson (quasipoisson) Negative binomial regression More on this in the next section "],
["quasipoisson-and-negative-binomial-models.html", "10.5 Quasipoisson and Negative Binomial Models", " 10.5 Quasipoisson and Negative Binomial Models The quaipoisson model relaxes the assumption that the mean and variance have to be equivalent. It is the same as the Poisson but multiplies the standard errors by \\(\\sqrt{d}\\), where \\(d\\) is the dispersion parameter. These models are fit in R almost exactly the same way as poisson. We just switch family = \"quasipoisson\" Note in the summary output, it lists the dispersion parameter. fitq &lt;- glm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + + tr+ taft + wilson + harding + coolidge + hoover, family = &quot;quasipoisson&quot;, data = subset(bolton, year &lt; 1945)) summary(fitq) Call: glm(formula = allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + +tr + taft + wilson + harding + coolidge + hoover, family = &quot;quasipoisson&quot;, data = subset(bolton, year &lt; 1945)) Deviance Residuals: Min 1Q Median 3Q Max -8.2027 -2.4224 -0.1515 2.1532 8.9032 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 8.0360573 0.9848054 8.160 1.22e-08 *** divided 0.4435893 0.1727719 2.567 0.01634 * inflation 0.0005599 0.0110836 0.051 0.96010 spending_percent_gdp -0.0286140 0.0082874 -3.453 0.00191 ** war 0.4745189 0.1936737 2.450 0.02133 * lame_duck 0.3692336 0.2769252 1.333 0.19399 administration_change -0.0321975 0.1844677 -0.175 0.86279 trend -0.0619311 0.0316454 -1.957 0.06116 . tr -2.5190839 0.9562064 -2.634 0.01401 * taft -2.4104113 0.8805222 -2.737 0.01102 * wilson -1.4750129 0.6866621 -2.148 0.04120 * harding -1.4205088 0.4700492 -3.022 0.00558 ** coolidge -1.1070363 0.3792082 -2.919 0.00715 ** hoover -0.9985352 0.3099077 -3.222 0.00341 ** --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 16.96148) Null deviance: 1281.92 on 39 degrees of freedom Residual deviance: 439.87 on 26 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 4 The dispersion parameter is estimated using those standardized residuals from the Poisson model. sres &lt;- residuals(fit, type=&quot;pearson&quot;) chisq &lt;- sum(sres^2) d &lt;- chisq/fit$df.residual d [1] 16.96146 Let’s compare the Poisson and Quasipoisson model coefficients and standard errors. round(summary(fit)$coefficients[2,], digits=4) Estimate Std. Error z value Pr(&gt;|z|) 0.4436 0.0420 10.5740 0.0000 round(summary(fitq)$coefficients[2,], digits=4) Estimate Std. Error t value Pr(&gt;|t|) 0.4436 0.1728 2.5675 0.0163 We can retrieve the standard error from the quasipoisson by multiplication. ## Multiply the Poisson standard error by sqrt(d) round(summary(fit)$coefficients[2,2] * sqrt(d), digits=4) [1] 0.1728 Note that the standard error among the Quasipoisson model is much bigger, accounting for the larger variance from overdispersion. 10.5.1 Negative Binomial Models Another model commonly used for count data is the negative binomial model. This is the model Bolton and Thrower use. This has a more complicated likelihood function, but, like the quasipoisson, it has a larger (generally, more correct) variance term. Analogous to the normal distribution, the negative binomial has both a mean and variance term parameter. \\(\\Pr(Y=y) = \\frac{\\Gamma(r+y)}{\\Gamma(r) \\Gamma(y+1)} (\\frac{r}{r+\\lambda})^r (\\frac{\\lambda}{r+\\lambda})^y\\) Models dispersion through term \\(r\\) \\(\\mathbf E(Y_i|X) = \\lambda_i = e^{\\mathbf x_i^\\top \\beta}\\) \\(\\mathbf Var(Y_i|X) = \\lambda_i + \\lambda_i^2/r\\) (note, this is no longer just \\(\\lambda_i\\)) Note that while the probability distribution looks much uglier the mapping of \\(\\mathbf x_i&#39; \\beta\\) is the same. We will still have a \\(\\log\\) link and exponeniate to get our quantities of interest. In R, we use the glm.nb function from the MASS package to fit negative binomial models. Here, we do not need to specify a family, but we do specify the link = \"log\". library(MASS) fitn &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + + tr+ taft + wilson + harding + coolidge + hoover, link=&quot;log&quot;, data = subset(bolton, year &lt; 1945)) summary(fitn)$coefficients[2,] Estimate Std. Error z value Pr(&gt;|z|) 0.442135921 0.141831942 3.117322607 0.001825017 Compare this to column 1 in the model below. We have now replicated the coefficients in column 1 of Table 1 in the authors’ paper. Our standard errors do not match exactly because the authors use clustered standard errors by president. Moreover, given a relatively small sample the two programs R (and Stata, which the authors use) might generate slightly different estimates. "],
["count-data-practice-problems.html", "10.6 Count data practice problems", " 10.6 Count data practice problems Let’s reproduce column 2 from Table 1 in the article and related estimates. Fit the following negative binomial model for year \\(&gt;\\) 1944. Note: the numbers won’t exactly match the authors but should be close \\[ \\begin{aligned} \\log ({ E( \\operatorname{allnoncerm_eo} ) }) &amp;= \\alpha + \\beta_{1}(\\operatorname{divided})\\ + \\\\ &amp;\\quad \\beta_{2}(\\operatorname{inflation}) + \\beta_{3}(\\operatorname{spending\\_percent\\_gdp})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{war}) + \\beta_{5}(\\operatorname{lame\\_duck})\\ + \\\\ &amp;\\quad \\beta_{6}(\\operatorname{administration\\_change}) + \\beta_{7}(\\operatorname{trend})\\ + \\\\ &amp;\\quad \\beta_{8}(\\operatorname{truman}) + \\beta_{9}(\\operatorname{ike})\\ + \\\\ &amp;\\quad \\beta_{10}(\\operatorname{jfk}) + \\beta_{11}(\\operatorname{lbj})\\ + \\\\ &amp;\\quad \\beta_{12}(\\operatorname{nixon}) + \\beta_{13}(\\operatorname{ford})\\ + \\\\ &amp;\\quad \\beta_{14}(\\operatorname{carter}) + \\beta_{15}(\\operatorname{reagan})\\ + \\\\ &amp;\\quad \\beta_{16}(\\operatorname{bush41}) + \\beta_{17}(\\operatorname{clinton})\\ + \\\\ &amp;\\quad \\beta_{18}(\\operatorname{bush43}) \\end{aligned} \\] Conduct a linear model using OLS and a Quasipoisson for comparison For each, calculate the average number of executive orders expected for divided government. How do these compare across models? Try on your own, then expand for the solution. fit.lm &lt;- lm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + truman + ike + jfk + lbj + nixon + ford + carter + reagan + bush41 + clinton + bush43, data = subset(bolton, year &gt; 1944)) fit.nb &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + truman + ike + jfk + lbj + nixon + ford + carter + reagan + bush41 + clinton + bush43, link=&quot;log&quot;, data = subset(bolton, year &gt; 1944)) fit.qp &lt;- glm(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + truman + ike + jfk + lbj + nixon + ford + carter + reagan + bush41 + clinton + bush43, family=&quot;quasipoisson&quot;, data = subset(bolton, year &gt; 1944)) ## Manual X &lt;- model.matrix(fit.lm) X[, &quot;divided&quot;] &lt;- 1 B &lt;- coef(fit.lm) exp.eo.lm &lt;- mean(X %*% B) X &lt;- model.matrix(fit.nb) X[, &quot;divided&quot;] &lt;- 1 B &lt;- coef(fit.nb) exp.eo.nb &lt;- mean(exp(X %*% B)) X &lt;- model.matrix(fit.qp) X[, &quot;divided&quot;] &lt;- 1 B &lt;- coef(fit.qp) exp.eo.qp &lt;- mean(exp(X %*% B)) exp.eo.lm [1] 56.86582 exp.eo.nb [1] 55.52576 exp.eo.qp [1] 55.41505 ## Prediction exp.eo.lm &lt;- prediction(fit.lm, at = list(divided=1)) exp.eo.nb &lt;- prediction(fit.nb, at = list(divided=1), type=&quot;response&quot;) exp.eo.qp &lt;- prediction(fit.qp, at = list(divided=1), type = &quot;response&quot;) summary(exp.eo.lm) at(divided) Prediction SE z p lower upper 1 56.87 1.939 29.33 4.77e-189 53.07 60.67 summary(exp.eo.nb) at(divided) Prediction SE z p lower upper 1 55.53 1.799 30.86 4.027e-209 52 59.05 summary(exp.eo.qp) at(divided) Prediction SE z p lower upper 1 55.42 2.117 26.18 4.395e-151 51.27 59.56 We are going to build further on this example to try to reproduce Figure 2 in the authors’ paper. This is a plot that shows the percentage change in executive orders from divided to unified government with 95% confidence intervals. These estimates are generated using the negative binomial regression models presented in Table 1. The good news is if we have followed the course notes and practice problems to this point, we have already fit both of these models. To find the percent change, we calculate the incidence rate ratios, which represent: For a one-unit change in \\(x\\), the expected count changes by a factor of \\(exp(\\hat \\beta_j)\\). This can be converted to percent change by \\((IRR - 1) \\times 100\\). For example, if the incidence rate ratio was 1.4, then for a one-unit change in \\(x\\), we would see a \\(40\\%\\) change (increase) in the expected count. To find the confidence intervals, we can use confint(fit), exponentiate these, and then follow the same formula for the lower bound and upper bound. Our confidence intervals will be bigger than the authors because we used a different type of standard error. Try on your own, then expand for the solution. fitn &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + + tr+ taft + wilson + harding + coolidge + hoover, link=&quot;log&quot;, data = subset(bolton, year &lt; 1945)) fit.nb &lt;- glm.nb(allnoncerm_eo ~ divided + inflation + spending_percent_gdp + war + lame_duck + administration_change + trend + truman + ike + jfk + lbj + nixon + ford + carter + reagan + bush41 + clinton + bush43, link=&quot;log&quot;, data = subset(bolton, year &gt; 1944)) ## Incidence Rate Ratios amd irr1 &lt;- exp(coef(fitn)[&quot;divided&quot;]) irr2 &lt;- exp(coef(fit.nb)[&quot;divided&quot;]) ci.irr1 &lt;- exp(confint(fitn)[&quot;divided&quot;,]) Waiting for profiling to be done... ci.irr2 &lt;- exp(confint(fit.nb)[&quot;divided&quot;,]) Waiting for profiling to be done... ## Percent change pc1 &lt;- (irr1-1)* 100 ci.pc1 &lt;- (ci.irr1 - 1)*100 pc2 &lt;- (irr2-1)* 100 ci.pc2 &lt;- (ci.irr2 - 1)*100 ## Prepare data for plotting df &lt;-data.frame(pc = c(pc1, pc2), lower=c(ci.pc1[1], ci.pc2[1]), upper = c(ci.pc1[2], ci.pc2[2])) df$period &lt;- c(&quot;Regime 1 \\n 1905-1944&quot;, &quot;Regime 2 \\n 1945-2013&quot;) ggplot(df, aes(y=pc,x=period))+ geom_bar(stat=&quot;identity&quot;)+ geom_errorbar(aes(ymin=lower, ymax=upper), width=.05)+ ggtitle(&quot;Effect of Divided Government on EO Usage&quot;)+ ylab(&quot;Percentage Change Divided Relative to Unified&quot;)+ xlab(&quot;&quot;)+ theme_minimal() "],
["week-8-tutorial.html", "10.7 Week 8 Tutorial", " 10.7 Week 8 Tutorial For this week’s tutorial, we will use data from the article, “Less than you think: Prevalence and predictors of fake news dissemination on Facebook” published in Science Advances by Andrew Guess, Jonathan Nagler, and Joshua Tucker available here. Abstract.So-called “fake news” has renewed concerns about the prevalence and effects of misinformation in political campaigns. Given the potential for widespread dissemination of this material, we examine the individual-level characteristics associated with sharing false articles during the 2016 U.S. presidential campaign. To do so, we uniquely link an original survey with respondents’ sharing activity as recorded in Facebook profile data. First and foremost, we find that sharing this content was a relatively rare activity. Conservatives were more likely to share articles from fake news domains, which in 2016 were largely pro-Trump in orientation, than liberals or moderates. We also find a strong age effect, which persists after controlling for partisanship and ideology: On average, users over 65 shared nearly seven times as many articles from fake news domains as the youngest age group. The authors look at the demographics predictors of disseminating fake news using survey data and behavioral data on respondents’ Facebook sharing history. The key outcome variable is num_fake_shares, representing the number of articles an individual shared from a list of fake news domains created by Buzzfeed. Key independent variables include party,ideology (factor), age (factor), female, black, educ, faminc, num_posts Let’s load the data and look at the outcome. library(rio) fake &lt;- import(&quot;https://github.com/ktmccabe/teachingdata/blob/main/sciencerep.RData?raw=true&quot;) table(fake$num_fake_shares) 0 1 2 3 4 5 6 7 8 9 10 12 13 25 50 1090 63 12 8 5 1 1 2 1 2 2 1 1 1 1 It looks like our data are a count. Let’s try to visualize this distribution in a histogram. How would you describe this distribution? library(ggplot2) ggplot(fake, aes(num_fake_shares))+ geom_histogram(bins=50)+ theme_minimal()+ ggtitle(&quot;Distribution of Fake News Shares&quot;) + ylab(&quot;Number of respondents&quot;) + xlab(&quot;Number of fake news stories shared&quot;) To filter the histogram by party like Fig. 1 in the paper, expand. library(ggplot2) fakesub &lt;- subset(fake, party %in% c(&quot;Democrat&quot;, &quot;Republican&quot;, &quot;Independent&quot;)) ggplot(fakesub, aes(num_fake_shares, fill=party))+ geom_histogram(bins=50)+ theme_minimal()+ ggtitle(&quot;Distribution of Fake News Shares&quot;) + ylab(&quot;Number of respondents&quot;) + scale_fill_manual(&quot;&quot;, values = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgray&quot;)) + xlab(&quot;Number of fake news stories shared&quot;) + theme(legend.position = &quot;bottom&quot;, legend.title = element_blank()) Warning: Removed 1499 rows containing non-finite values (stat_bin). Let’s say we were interested in exploring the relationship between age and the number of fake news posts. It can always be useful to conduct more descriptive or exploratory analysis before hitting the heavy machinery. For example, the authors look at the average number of fake news posts shared by age group in Figure 2. We can do the same. Ours will look a little different because it’s unweighted: ## Base r version meansbyage &lt;- tapply(fake$num_fake_shares, fake$age, mean, na.rm=T) barplot(meansbyage, main = &quot;Age and Fake News Shares&quot;, ylab= &quot;Mean number of fake news stories shared&quot;, xlab=&quot;Age group&quot;, ylim=c(0,.6), col=&quot;black&quot;) ## ggplot version library(tidyverse) fake %&gt;% filter(!is.na(age)) %&gt;% group_by(age) %&gt;% summarise(mean_fakes = mean(num_fake_shares, na.rm = TRUE)) %&gt;% ggplot(aes(x=age, y=mean_fakes)) + geom_bar(stat = &quot;identity&quot;) + ggtitle(&quot;Age and Fake News Shares&quot;) + ylab(&quot;Mean number of fake news stories shared&quot;) + xlab(&quot;Age group&quot;) + ylim(0, .6)+ theme_minimal() `summarise()` ungrouping output (override with `.groups` argument) Let’s move to a regression approach to study this relationship. How could we model this relationship? What are the pros and cons of each approach? Would OLS work? Would a binary logit or probit work? What other approaches could work? Let’s replicate the first column of Table 2 in the authors’ analysis. What type of model do the authors use? Why might this be an appropriate model in their case? How should the coefficients be interpreted? What would be a quantity of interest? Recall that for our primary count models, the regression equation is of the form: \\(\\log \\lambda_i = \\eta_i = \\mathbf x_i^T\\beta\\) \\(\\lambda_i = exp(\\mathbf x_i^T\\beta)\\) \\(\\lambda_i = \\mathbf E[Y_i | \\mathbf x_i] = \\exp(\\beta_0 + \\beta_1x_{i1} + ... + \\beta_{k}x_{ik})\\) is the expected number of events per a unit of time or space Let’s fit the model using glm. Be careful to specify the right family. What is the dispersion parameter? What conceptual quantity does this represent? Expand for details. We will fit a quasipoisson following the authors. Note: they apply survey weights through the weights function. You could also use the survey package for this. For now, we will follow the approach of the authors. model1.qp &lt;- glm(num_fake_shares ~ ideology + age + female + black + educ + faminc, weights = weight_svypop_w3, data = fake, family = &quot;quasipoisson&quot;) summary(model1.qp) Call: glm(formula = num_fake_shares ~ ideology + age + female + black + educ + faminc, family = &quot;quasipoisson&quot;, data = fake, weights = weight_svypop_w3) Deviance Residuals: Min 1Q Median 3Q Max -3.4510 -0.5414 -0.3406 -0.1929 12.8243 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -3.416028 1.379091 -2.477 0.01341 * ideologyVery liberal 0.486833 1.238165 0.393 0.69426 ideologyLiberal -1.127456 1.438798 -0.784 0.43345 ideologyModerate 0.332692 1.186454 0.280 0.77922 ideologyConservative 2.186850 1.154932 1.893 0.05857 . ideologyV. conserv. 2.366213 1.158278 2.043 0.04132 * age30-44 0.771712 0.811327 0.951 0.34174 age45-65 1.136384 0.764972 1.486 0.13771 ageOver 65 2.052391 0.766352 2.678 0.00752 ** female -0.113780 0.216606 -0.525 0.59950 black -0.879538 0.753713 -1.167 0.24351 educ -0.084881 0.081317 -1.044 0.29681 faminc -0.007145 0.008419 -0.849 0.39627 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 3.304861) Null deviance: 1676.7 on 1040 degrees of freedom Residual deviance: 1172.0 on 1028 degrees of freedom (2459 observations deleted due to missingness) AIC: NA Number of Fisher Scoring iterations: 7 What was the form of the regression model? library(equatiomatic) equatiomatic::extract_eq(model1.qp, wrap=T, terms_per_line = 2) \\[ \\begin{aligned} \\log ({ E( \\operatorname{num_fake_shares} ) }) &amp;= \\alpha + \\beta_{1}(\\operatorname{ideology}_{\\operatorname{Very\\ liberal}})\\ + \\\\ &amp;\\quad \\beta_{2}(\\operatorname{ideology}_{\\operatorname{Liberal}}) + \\beta_{3}(\\operatorname{ideology}_{\\operatorname{Moderate}})\\ + \\\\ &amp;\\quad \\beta_{4}(\\operatorname{ideology}_{\\operatorname{Conservative}}) + \\beta_{5}(\\operatorname{ideology}_{\\operatorname{V.\\ conserv.}})\\ + \\\\ &amp;\\quad \\beta_{6}(\\operatorname{age}_{\\operatorname{30-44}}) + \\beta_{7}(\\operatorname{age}_{\\operatorname{45-65}})\\ + \\\\ &amp;\\quad \\beta_{8}(\\operatorname{age}_{\\operatorname{Over\\ 65}}) + \\beta_{9}(\\operatorname{female})\\ + \\\\ &amp;\\quad \\beta_{10}(\\operatorname{black}) + \\beta_{11}(\\operatorname{educ})\\ + \\\\ &amp;\\quad \\beta_{12}(\\operatorname{faminc}) \\end{aligned} \\] We could have fit a regular poisson model. In that model, the dispersion paramter is taken to be 1, such that the conditional mean and variance are equivalent. In contrast, the quasipoisson accounts for the possibility that the variance is greater. Let’s see how much of an issue this is. model1.p &lt;- glm(num_fake_shares ~ ideology + age + female + black + educ + faminc, weights = weight_svypop_w3, data = fake, family = &quot;poisson&quot;) We can conduct a test of overdispersion in our model using dispersiontest. If we have a significant result and a dispersion constant \\(&gt;\\) 0, this would suggest overdispersion. What should we conclude? What are the implications? library(AER) dispersiontest(model1.p) Overdispersion test data: model1.p z = 2.9334, p-value = 0.001676 alternative hypothesis: true dispersion is greater than 1 sample estimates: dispersion 3.027398 Note: The authors address different modeling choices in the paper. “We aggregated all shares to the individual respondent level so that our dependent variables are counts (i.e., number of fake news stories shared). To account for this feature of the data, as well as the highly skewed distribution of the counts, we primarily used Poisson or quasi-Poisson regressions to model the determinants of Facebook sharing behavior. We conducted dispersion tests on the count data and used quasi-Poisson models if the null hypothesis of no dispersion is rejected. Below, we included negative binomial and Ordinary Least Squares (OLS) regressions to show that our results are generally not sensitive to model choice. All models applied weights from YouGov to adjust for selection into the sample. We specifically used sample-matching weights produced for the third wave of the survey, which was closest to the Facebook encouragement sent to respondents (27). (Results also do not appear to be sensitive to the use of weights.)” Let’s generate some quantities of interest. What is the incidence rate ratio for sharing fake news of being Age 65+ vs. the reference category 18-29? What is the average number of fake news posts expected to be shared by age group? Expand for details. Incidence rate ratios ## Incidence rate ratios are exp(Bj) exp(coef(model1.qp)[&quot;ageOver 65&quot;]) ageOver 65 7.786494 How should we interpret this? Average Fake News Posts Shared by Age Group library(prediction) ## Note: levels(fake$age) is a shortcut for putting in the vector of all possible age levels. Only works for factor variables. count.byage &lt;- prediction(model1.qp, at=list(age = levels(fake$age)), type=&quot;response&quot;) summary(count.byage) at(age) Prediction SE z p lower upper 18-29 0.06575 0.04894 1.344 1.791e-01 -0.03017 0.1617 30-44 0.14225 0.04644 3.063 2.193e-03 0.05122 0.2333 45-65 0.20484 0.03468 5.907 3.493e-09 0.13687 0.2728 Over 65 0.51196 0.08653 5.917 3.285e-09 0.34237 0.6816 ## 65+ X &lt;- model.matrix(model1.qp) X[, &quot;age30-44&quot;] &lt;- 0 X[, &quot;age45-65&quot;] &lt;- 0 X[, &quot;ageOver 65&quot;] &lt;- 1 B &lt;- coef(model1.qp) eta &lt;- X %*% B avg.fake.65 &lt;- mean(exp(eta)) ## 45-65 X &lt;- model.matrix(model1.qp) X[, &quot;age30-44&quot;] &lt;- 0 X[, &quot;age45-65&quot;] &lt;- 1 X[, &quot;ageOver 65&quot;] &lt;- 0 B &lt;- coef(model1.qp) eta &lt;- X %*% B avg.fake.45 &lt;- mean(exp(eta)) ## 30-44 X &lt;- model.matrix(model1.qp) X[, &quot;age30-44&quot;] &lt;- 1 X[, &quot;age45-65&quot;] &lt;- 0 X[, &quot;ageOver 65&quot;] &lt;- 0 B &lt;- coef(model1.qp) eta &lt;- X %*% B avg.fake.30 &lt;- mean(exp(eta)) ## 18-29 X &lt;- model.matrix(model1.qp) X[, &quot;age30-44&quot;] &lt;- 0 X[, &quot;age45-65&quot;] &lt;- 0 X[, &quot;ageOver 65&quot;] &lt;- 0 B &lt;- coef(model1.qp) eta &lt;- X %*% B avg.fake.18 &lt;- mean(exp(eta)) ## Gather results c(avg.fake.18, avg.fake.30, avg.fake.45, avg.fake.65) [1] 0.06574956 0.14224696 0.20484149 0.51195856 How could we calculate uncertainty? "],
["additional-considerations.html", "10.8 Additional Considerations", " 10.8 Additional Considerations 10.8.1 Offset We said that counts are often considered rates per interval. Sometimes when we have a count, we know the greatest possible value that an observation could take or we think that will be influenced by a particular variable, which varies by observation. This is called the exposure. For example maybe we are looking at the number of traffic accidents per year per population size. In contrast, in the example from before, counts of executive orders were just measured as a span of one year. We are going to use an example with an exposure from Gelman and Hill and the study of stop-and-frisk as a policy. This is example is based on the tutorial by Clay Ford here. The data include an outcome desribing the number of stops in a given area. The regression model looks at the relationship between race/ethnicity factor(eth) and the number of stops made in a precinct. Run the few lines below to load and prepare the data according to Gelman and Hill’s instructions: ## Stop and frisk. Does race/ethnicity influence number of stops? ## Prepare noisy data dat &lt;- read.table( &quot;http://www.stat.columbia.edu/~gelman/arm/examples/police/frisk_with_noise.dat&quot;, header=TRUE, skip=6) stops &lt;- aggregate(cbind(stops, past.arrests) ~ eth + precinct, data=dat, sum) The unit of analysis after we run this is the number of stops in a given precinct for a particular racial/ethnicity group. head(stops) eth precinct stops past.arrests 1 1 1 202 980 2 2 1 102 295 3 3 1 81 381 4 1 2 132 753 5 2 2 144 557 6 3 2 71 431 It is possible that the count of the number of stops may be influenced by the number of past arrests of a particular unit of analysis. We might want to measure a count per number of past arrests for an ethnicity group in a precinct instead of just a count per ethnicity group in a precinct. This will be our exposure and the \\(\\log\\) past.arrests will be our offset. When we have an offset, our model changes to: \\(\\frac{\\mathbf E(Y_i | X)}{N_i} = \\exp(\\mathbf x_i&#39;\\beta)\\) or alternatively \\(\\mathbf E(Y_i | X) = \\exp(\\mathbf x_i&#39;\\beta + \\log N_i)\\). The exposure enters the right side of the equation as what is referred to as an “offset”– the \\(\\log\\) of the exposure. We will not get a regression coeffcient for the offset because it is fixed to 1. However, we will still need to incorporate the offset when we estimate our outcomes. Let’s use a quasipoisson model this time. We enter the offset through an explicit argument offset. (Otherwise it will be treated like any regression variable, and its coefficient won’t be fixed to 1.) ## No natural limit for stops BUT might be influenced by past arrests sf.1 &lt;- glm(stops ~ factor(eth), data = stops, family=&quot;quasipoisson&quot;, offset = log(past.arrests)) summary(sf.1) Call: glm(formula = stops ~ factor(eth), family = &quot;quasipoisson&quot;, data = stops, offset = log(past.arrests)) Deviance Residuals: Min 1Q Median 3Q Max -47.327 -7.740 -0.182 10.241 39.140 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) -0.58809 0.05646 -10.416 &lt;2e-16 *** factor(eth)2 0.07021 0.09042 0.777 0.438 factor(eth)3 -0.16158 0.12767 -1.266 0.207 --- Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 (Dispersion parameter for quasipoisson family taken to be 222.5586) Null deviance: 46120 on 224 degrees of freedom Residual deviance: 45437 on 222 degrees of freedom AIC: NA Number of Fisher Scoring iterations: 5 When we calculate our quantities of interest, predict will automatically incorporate the offset. When we calculate them manually, we need to explicitly enter the offset. ## Predicted counts with offsets exp(XB + log(N)) sf.count &lt;- predict(sf.1, type = &quot;response&quot;) X &lt;- model.matrix(sf.1) B &lt;- coef(sf.1) sf.count.man &lt;- exp(X %*% B + log(stops$past.arrests)) Note that \\(e^{X\\hat\\beta + \\log N} = e^{X\\hat\\beta} * e^ {\\log N} = N*e^{X\\hat\\beta}\\). This means we could equivalently write: sf.count.man2 &lt;- exp(X %*% B)*stops$past.arrests cbind(sf.count, sf.count.man, sf.count.man2)[1:6,] sf.count 1 544.2816 544.2816 544.2816 2 175.7562 175.7562 175.7562 3 180.0316 180.0316 180.0316 4 418.2082 418.2082 418.2082 5 331.8515 331.8515 331.8515 6 203.6578 203.6578 203.6578 "],
["how-to-think-about-zero-counts.html", "10.9 How to think about Zero Counts", " 10.9 How to think about Zero Counts Sometimes our outcome data have an excessive number of zeroes. For example, perhaps there are a lot of people that never post on social media at all, and then there are a smaller number of those that do, and they may post in any positive number of times. For these, we might think there are two decisions People that post vs. do not post. This sounds like a binary model. How many times to post? Ok this sounds more like Poisson or Negative Binomial A large number of zeroes is not necessarily something that the Poisson and Negative binomial distributions would predict with high probability. For that reason, we might want to use a modeling strategy that accounts for zero excess. We will discuss two: Hurdle models and Zero-inflated poisson/negative binomial. 10.9.1 Hurdle Models Hurdle models may be useful when there are possibly sequential steps in achieving a positive count. The above example could motivate a hurdle model. First, someone decides if they want to post, and then if they want to post, they may post any positive \\(&gt;0\\) number of times. There is a probability that governs the likelihood of not posting (\\(Pr(Y_i = 0) = \\pi\\)) And then there is a count model restricted to \\(&gt;0\\) (“zero truncated”) describing the number of posts. This post from the University of Virginia explains how to fit hurdle models in R. 10.9.2 Zero Inflated Poisson/Negative binomial When you have excess zeroes, the intuitively named zero-inflated poisson or negative binomial model may also be appropriate. These are “mixture models” because there is a mixture of two distributions: the Bernoulli and Poisson/Negative Binomial. Here we think that there are two types of zeroes in the data. This is only appropriate to the extent that there are some observations that are truly ineligible from having a positive count– that have zero probability of having a count \\(&gt;\\) 0. For example, in the UCLA R tutorial linked to below, they study the number of fish a particular camping group caught at the park. Well some people might not have gone fishing! This would be a case where some of the zeroes may reflect a separate process (the decision to fish) However, even among those that decide to go fishing, some people may still catch zero fish. Just like in a typical Poisson or Negative Binomial process, it is still possible to have a 0 count. Here, we just think there may be two processes explaining the zeroes, and only using a standard count model does not help explain that first process. We fit two models – a logistic regression model and a count model. These tutorials from UCLA here and here describe how one would fit these models in R. I also recommend reading this blog post from Paul Allison in what to consider when choosing between count models. He argues often it may make just as or even more sense to stick with the overdispersed poisson or negative binomial unless you have a good reason to believe that there are people with zero-probability of having a positive count. For those in IR, you might also be interested in this article about the use of zero-inflated models for forecasting civil conflict. This article also provides an overview of different options for modeling count data. "]
]
