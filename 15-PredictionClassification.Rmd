# Prediction and Classification {#predict}

This section will provide a brief introduction to tools related to the goals of prediction and classification. 

In the social sciences, we often have one of a few goals in mind:

  - Describe
  - Explain, evaluate, and recommend $\rightarrow$ Causality
  - Discover and Predict
  
Most of the tools we have been working on thus far have focused on first describing our data and then conducting hypothesis tests through different types of regression, in order to assess a deductive hypothesis, *explaining* the relationship between two variables.

Here, we are going to turn to the third goal above.

Some supplemental resources for this section include the following R packages:

  - `caret` [package](http://topepo.github.io/caret/index.html) and [presentation](https://www.r-project.org/conferences/useR-2013/Tutorials/kuhn/user_caret_2up.pdf}{presentation)
  - `RTextTools` (may or may not be updated) [here](https://cran.r-project.org/web/packages/RTextTools/index.html) and [here](https://journal.r-project.org/archive/2013-1/collingwood-jurka-boydstun-etal.pdf}{summary)

## Overview of Prediction and Classification

When we predict something, we are estimating some unknown using information we do know and trying to do so as accurately and precisely as possible.

  - Often prediction involves classification--  predicting a categorical outcome (e.g., prediction of who wins vs. who loses)

Some social science examples of this might include

![](images/instagramhate.png)

  - Trying to detect hate speech online
  - Trying to flag "fake news" and other misinformation
  - Trying to forecast the results of an election
  - Trying to classify a large amount of text into subject or topic categories for analysis
  

### How to predict or classify

Goal: Estimate/guess some unknown using information we have -- and do so as accurately and precisely as possible.

  1. Choose an approach
      - Using an observed (known) measure as a direct proxy to predict an outcome (e.g., a dictionary)
      - Using one or more observed (known) measures in a (often, regression) model to predict an outcome
      - Using a model to automatically select the measures to use for predicting an outcome
  2. Assess accuracy and precision in-sample and out-of-sample using some sample "training" data where you do know the right answer ("ground truth").
      - Prediction error: $Truth - Prediction$ 
      - Bias: Average prediction error: $\text{mean}(Truth - Prediction)$
          + A prediction is `unbiased' if the bias is zero (if the prediction is on average true)
      - In regression: R-squared or Root-mean squared error 
          + RMSE is like `absolute' error-- the average magnitude of the prediction error 
      - For classification: Confusion Matrix 
          + A cross-tab of predictions you got correct vs. predictions you got wrong (misclassified)
          + Gives you true positives and true negatives vs. false positives and false negatives
  3. Repeat steps 1 and 2 until you are confident in your method for predicting or classifying.
  4. Apply to completely unknown data. 
  

## In-sample vs. Out-of-Sample

Problem: Models that fit our existing ("in-sample") data might not be the best for predicting out-of-sample data. Issues with variability and overfitting.

  - There may be idiosyncratic features of the data for which we do know the ground truth (e.g., outliers, different levels of variability) that could lead to poor predictions. This can lead to overfitting-- predicting your particular dataset reaaaaaallly well, but any other data, more poorly.
  - Our training data could be unrepresentative of the population in some way
  
For example, many AI tools suffer from gender and/or racial biases due to unrepresentative training data. If most of the data used to train a speech detection or facial recognition tool was trained on white faces/ white voices, it may have poor accuracy for other groups.

![](images/speecherrors.png)

Solutions:

  - Choose your training data wisely!
  - Search for systematic biases along key variables (instead of aggregate accuracy, also look for accuracy measures by subgroups)
  - Use out-of-sample predictions/classification tests to help avoid overfitting
      + Split data into train vs. test and/or do this repeatedly with
      + Cross-validation


## Cross-Validation

Cross-Validation is an approach to address overfitting issues

  - Take data for which you know the answer -- we call this "training data"
  - Randomly subset out a portion of the training data. This will become our "test" data.
  - Develop a model based on  the training data. 
  - Test the accuracy of the model on the test data (out-of-sample data that was not used to train the model).
  - Repeat process for different portions of the data.

Goal: See how well our model will generalize to new data (data the model hasn't seen).


### k-fold cross-validation

Divide your data into folds (how many $k$ depends on how much data you have)

  - Fit your model to $k-1$ folds
  - See how well your model predicts the data in the $k$th fold.
  - Can repeat, leaving out a different fold each time

### Leave-one-out cross-validation

Best for smaller data

  - Fit your model to all but one observation in your data
  - See how well your model predicts the left-out observation
  - Can repeat, continuing to leave out one observation each time


## Examples of Prediction and Classification

You may be wondering why we are covering these topics in a maximum likelihood course. Well, one of the most fundamental types of classification relies on logistic regression. We are just going to reframe the tools we have already learned for a different purpose.

Let's use an example with the `donation` data from [12.7](https://ktmccabe.github.io/mlebook/week-12-exercise.html)

```{r}
library(rio)
don <- import("https://github.com/ktmccabe/teachingdata/blob/main/donation.dta?raw=true")
```

Recall that a key outcome variable here was `donation` indicating if a particular member of the donorate donated to a given senator (=1) or not (=0)

```{r}
table(don$donation)
```

Can we predict whether someone will donate to a Senator?

Outcome: `donation` (1 or 0), binary variable = binary classifier

  1. Choose an approach: Build a model (e.g., logistic regression)
  2. Train the model
  3. Assess accuracy 
      - Within sample and test model using cross-validation out-of-sample
  4. If we were very satisfied with our model, we could apply the model to data for which we do not know the answer in the future. 
      - If you were a campaign, could you predict who will donate?
  
Let's go!

```{r}
## build a model (choose "features" you think will be good for prediction)

## you will want to remove missing data first
donsub <- don[, c("donation", "NetWorth", "Edsum")]
donsub <- na.omit(donsub)

fit <- glm(donation ~ NetWorth + Edsum, 
           family = binomial(link = "logit"), data = donsub)
```

In-Sample Accuracy Assessments
```{r}
## generate probability of donation for each observation
donsub$prob <- predict(fit, type = "response")

## set a prediction threshold
donsub$pred <- ifelse(donsub$prob > mean(donsub$donation), 1, 0)

## accuracy- proportion where prediction matches reality
mean(donsub$pred == donsub$donation)

## confusion matrix
table(truth = donsub$donation, predicted = donsub$pred)
```
      
There are different measures for accuracy that focus on particular types of errors:

```{r}
## where did we miss
table(actual = donsub$donation, pred = donsub$pred)
truepos <- table(actual = donsub$donation, pred = donsub$pred)[2,2]
falsepos <- table(actual = donsub$donation, pred = donsub$pred)[1,2]
trueneg <- table(actual = donsub$donation, pred = donsub$pred)[1,1]
falseneg <- table(actual = donsub$donation, pred = donsub$pred)[2,2]

## precision
precision <- truepos/(truepos + falsepos)

## specificity
specificity <- trueneg / (trueneg + falsepos)

## false positive rate
falsepos <- falsepos/(trueneg+ falsepos)

## recall aka sensitivity
recall <- truepos/(truepos + falseneg)

## f-score, combination of precision/recall
F1 <- (2 * precision * recall) / (precision + recall)
```
See [this post](https://towardsdatascience.com/accuracy-recall-precision-f-score-specificity-which-to-optimize-on-867d3f11124) for more details and guidance on which one to choose.


Another common way to assess accuracy is through an ROC curve

ROC curves plot the true positive rate/precision (y) vs. 1 - false positive (x) rates. Want the curve to be away from the diagonal, increasing the area under the curive (AUC).
```{r, message=F, warning =F}
# install.packages("pROC")
library(pROC)
ROC <- roc(response = donsub$donation,
                  predictor = donsub$pred)
plot(ROC, print.thres = "best")
auc(ROC)
```
For more information, see [here](http://fouryears.eu/2011/10/12/roc-area-under-the-curve-explained/).


Out-of-Sample Tests: Cross-Validation

```{r, message=F, warning=F}
## split data into k folds
library(cvTools)
set.seed(1234)
folds <- cvFolds(nrow(donsub), K = 10)
```

Let's run through the process for one fold
```{r}
## leave one fold out != i
fit <- glm(donation ~ NetWorth + Edsum, family = binomial(link = "logit"), 
           data = donsub[folds$subsets[folds$which != 1],])

## out of sample prediction on left out fold == i
testdata <- donsub[folds$subsets[folds$which == 1],]
testdata$prob <- predict(fit, newdata = testdata, 
                       type = "response")

## choose some criteria to assess accuracy
## we will just assess basic matches, but you may want a different one
## set a prediction threshold
testdata$pred <- ifelse(testdata$prob > .5, 1, 0)
acc <- mean(testdata$pred == testdata$donation)
acc
```

Let's make a loop to repeat this process for each fold.

```{r}
## container vector for assessment criteria
accs <- rep(NA, 10)

## set to length of container
for(i in 1:10){
  
  ## leave one fold out != i
  fit <- glm(donation ~ NetWorth + Edsum, family = binomial(link = "logit"), 
           data = donsub[folds$subsets[folds$which != i],])
  
  ## out of sample prediction on left out fold == i
  testdata <- donsub[folds$subsets[folds$which == i],]
  testdata$prob <- predict(fit, newdata = testdata, 
                       type = "response")

  ## choose some criteria to assess accuracy
  ## set a prediction threshold
  testdata$pred <- ifelse(testdata$prob > .5, 1, 0)
  accs[i] <- mean(testdata$pred == testdata$donation)
}
accs

mean(accs)
```

We've done it! If we are satisfied with this level of accuracy, we could stop here and apply our model with any new data. If we are not satisfied, we would start by building a new model and repeat the process.


***Continuous Outcome***

Let's now try an example with a continuous outcome: How much will someone donate to a Senator? One way to judge accuracy of a linear model is the root mean squared error

One basic approach would be to use a linear regression model. Other than that, it's the same process as before, but we will use a different assessment for accuracy.
```{r}
fit <- lm(total_donation ~ NetWorth + Edsum, data = don)

## Root mean squared error
rmse <- sqrt(sum(residuals(fit)^2)/fit$df.residual)
rmse
```

## Extending Machine Learning

How do I know which variables matter? Examples of more complex machine learning methods.

  - Random forests
  - Gradient boosting 
  - LASSO
  - SVM

Tradeoffs: Machine Learning as "black box"- see [here](https://towardsdatascience.com/the-black-box-metaphor-in-machine-learning-4e57a3a1d2b0)


Here is a rough example using `caret` with one of these methods along with cross-validation. Be sure to look at the [documentation](https://cran.r-project.org/web/packages/caret/vignettes/caret.html) before using this in your own work.
```{r, message=F, warning = F}
library(caret)

## Establish type of training
fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5)

## Train model. Note the . means include all variables. Let's subset first
library(tidyverse)
don2 <- don %>% dplyr::select(donation, Edsum, same_state, sameparty, NetWorth, peragsen)
don2$donation <-as.factor(ifelse(don2$donation == 1, "Donated", "Not Donated"))
don2 <- na.omit(don2)
mod_fit <- train(donation ~ ., data = don2, 
                 method = 'gbm',
                 verbose=F,
                 trControl = fitControl)
mod_fit
```


There are nearly countless ways to use machine learning. See [here](http://topepo.github.io/caret/train-models-by-tag.html).


